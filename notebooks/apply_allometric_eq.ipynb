{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fsspec\n",
    "import time\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from carbonplan_trace.v1.glas_preprocess import preprocess\n",
    "import carbonplan_trace.v1.glas_allometric_eq as allo\n",
    "import carbonplan_trace.v1.utils as utils\n",
    "from carbonplan_trace.v1.glas_height_metrics import get_all_height_metrics\n",
    "from carbonplan_trace.utils import zarr_is_complete\n",
    "\n",
    "# from gcsfs import GCSFileSystem\n",
    "# fs = GCSFileSystem(cache_timeout=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from s3fs import S3FileSystem\n",
    "from carbonplan_trace.v1.landsat_preprocess import access_credentials\n",
    "\n",
    "access_key_id, secret_access_key = access_credentials()\n",
    "fs = S3FileSystem(key=access_key_id, secret=secret_access_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## local or remote cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_type = \"remote\"  # 'local'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask.distributed import Client\n",
    "from dask_gateway import Gateway\n",
    "\n",
    "if cluster_type == \"remote\":\n",
    "    gateway = Gateway()\n",
    "    options = gateway.cluster_options()\n",
    "    options.worker_cores = 1\n",
    "    options.worker_memory = 120\n",
    "    options.image = \"carbonplan/trace-python-notebook:latest\"\n",
    "    cluster = gateway.new_cluster(cluster_options=options)\n",
    "    cluster.adapt(minimum=1, maximum=10)\n",
    "    client = cluster.get_client()\n",
    "elif cluster_type == \"local\":\n",
    "    client = Client(n_workers=2, threads_per_worker=1)\n",
    "else:\n",
    "    print(\"only cluster type of remote of local are supported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting example waveforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from carbonplan_trace.v1.glas_height_metrics import plot_shot\n",
    "# from carbonplan_trace.v1.glas_preprocess import get_modeled_waveform\n",
    "\n",
    "# lat_tag = '00N'\n",
    "# lon_tag = '010E'\n",
    "# preprocessed_path = f\"gs://carbonplan-climatetrace/intermediates/preprocessed_lidar/{lat_tag}_{lon_tag}.zarr\"\n",
    "# preprocessed = (\n",
    "#     utils.open_zarr_file(preprocessed_path)\n",
    "#     .stack(unique_index=(\"record_index\", \"shot_number\"))\n",
    "#     .dropna(dim=\"unique_index\", subset=[\"lat\"])\n",
    "# )\n",
    "# preprocessed['modeled_wf'] = get_modeled_waveform(preprocessed)\n",
    "# record = preprocessed.isel(unique_index=0).load()\n",
    "# plot_shot(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import random\n",
    "# for _ in range(10):\n",
    "#     ind = random.randint(0, preprocessed.dims['unique_index'])\n",
    "#     record = preprocessed.isel(unique_index=ind).load()\n",
    "#     plot_shot(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To run more tiles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "\n",
    "\n",
    "@dask.delayed\n",
    "def process_one_tile(bounding_box, skip_existing, access_key_id, secret_access_key):\n",
    "    min_lat, max_lat, min_lon, max_lon = bounding_box\n",
    "    lat_tag, lon_tag = utils.get_lat_lon_tags_from_bounding_box(max_lat, min_lon)\n",
    "    biomass_path = f\"s3://carbonplan-climatetrace/v1/biomass/{lat_tag}_{lon_tag}.zarr\"\n",
    "    preprocessed_path = (\n",
    "        f\"s3://carbonplan-climatetrace/v1/preprocessed_lidar/{lat_tag}_{lon_tag}.zarr\"\n",
    "    )\n",
    "    with dask.config.set(scheduler=\"single-threaded\"):\n",
    "        from s3fs import S3FileSystem\n",
    "\n",
    "        fs = S3FileSystem(key=access_key_id, secret=secret_access_key)\n",
    "\n",
    "        if skip_existing and fs.exists(biomass_path + \"/.zmetadata\"):\n",
    "            return (\"skipped\", biomass_path)\n",
    "\n",
    "        try:\n",
    "            assert fs.exists(preprocessed_path + \"/.zmetadata\")\n",
    "            mapper = fs.get_mapper(preprocessed_path)\n",
    "            preprocessed = (\n",
    "                xr.open_zarr(mapper)\n",
    "                .stack(unique_index=(\"record_index\", \"shot_number\"))\n",
    "                .dropna(dim=\"unique_index\", subset=[\"lat\"])\n",
    "            )\n",
    "            # filtering of null values stored as the maximum number for the datatype\n",
    "            preprocessed = preprocessed.where(\n",
    "                (preprocessed.rec_wf < 1e35).all(dim=\"rec_bin\"), drop=True\n",
    "            )\n",
    "            assert preprocessed.dims[\"unique_index\"] > 0\n",
    "        except:\n",
    "            # read in data, this step takes about 5 mins\n",
    "            data01 = utils.open_glah01_data()\n",
    "            data14 = utils.open_glah14_data()\n",
    "\n",
    "            # subset data to the bounding box\n",
    "            sub14 = utils.subset_data_for_bounding_box(data14, min_lat, max_lat, min_lon, max_lon)\n",
    "            sub01 = data01.where(data01.record_index.isin(sub14.record_index), drop=True)\n",
    "            combined = sub14.merge(sub01, join=\"inner\")\n",
    "\n",
    "            if len(combined.record_index) == 0:\n",
    "                return (\"no data in lidar\", biomass_path)\n",
    "\n",
    "            # preprocess data and persist\n",
    "            preprocessed = preprocess(combined, min_lat, max_lat, min_lon, max_lon)\n",
    "            del combined, sub14, sub01\n",
    "\n",
    "            if len(preprocessed.record_index) == 0:\n",
    "                return (\"no data after preprocess\", biomass_path)\n",
    "\n",
    "            preprocessed[\"datetime\"] = preprocessed.datetime.astype(\"datetime64[ns]\")\n",
    "\n",
    "            preprocessed = preprocessed.unstack(\"unique_index\")\n",
    "            preprocessed = preprocessed.chunk({\"record_index\": 500, \"shot_number\": 40})\n",
    "\n",
    "            mapper = fs.get_mapper(preprocessed_path)\n",
    "            mapper.clear()\n",
    "            for v in list(preprocessed.keys()):\n",
    "                if \"chunks\" in preprocessed[v].encoding:\n",
    "                    del preprocessed[v].encoding[\"chunks\"]\n",
    "            preprocessed.to_zarr(mapper, mode=\"w\", consolidated=True)\n",
    "\n",
    "        # calculate biomass\n",
    "        with_biomass = allo.apply_allometric_equation(\n",
    "            preprocessed, min_lat, max_lat, min_lon, max_lon\n",
    "        )\n",
    "\n",
    "        # saving output\n",
    "        height_metrics = [\n",
    "            \"VH\",\n",
    "            \"h25_Neigh\",\n",
    "            \"h50_Neigh\",\n",
    "            \"h75_Neigh\",\n",
    "            \"h90_Neigh\",\n",
    "            \"QMCH\",\n",
    "            \"MeanH\",\n",
    "            \"f_slope\",\n",
    "            \"senergy\",\n",
    "        ]\n",
    "\n",
    "        with_biomass = get_all_height_metrics(with_biomass, height_metrics).compute()\n",
    "        variables = [\n",
    "            \"lat\",\n",
    "            \"lon\",\n",
    "            \"time\",\n",
    "            \"biomass\",\n",
    "            \"allometric_eq\",\n",
    "            \"glas_elev\",\n",
    "            \"ecoregion\",\n",
    "            \"eosd\",\n",
    "            \"nlcd\",\n",
    "            \"igbp\",\n",
    "            \"treecover2000_mean\",\n",
    "            \"burned\",\n",
    "        ]\n",
    "\n",
    "        with_biomass = with_biomass.unstack(\"unique_index\")[variables + height_metrics]\n",
    "        with_biomass = with_biomass.chunk({\"record_index\": 500, \"shot_number\": 40})\n",
    "        mapper = fs.get_mapper(biomass_path)\n",
    "        for v in list(with_biomass.keys()):\n",
    "            if \"chunks\" in with_biomass[v].encoding:\n",
    "                del with_biomass[v].encoding[\"chunks\"]\n",
    "        with_biomass.to_zarr(mapper, mode=\"w\", consolidated=True)\n",
    "\n",
    "        return (\"processed\", biomass_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run all tiles that doesn't exist in output yet\n",
    "\n",
    "\n",
    "def get_list_of_mask_tiles(include=\"\"):\n",
    "    \"\"\"\n",
    "    Ecoregions mask is stored in 10 degree tiles, grab the filepaths\n",
    "    \"\"\"\n",
    "    no_data_tiles = [\"40N_070W\", \"30N_170W\", \"20N_120W\", \"00N_070E\"]\n",
    "\n",
    "    fs = S3FileSystem()\n",
    "    mask_folder = \"s3://carbonplan-climatetrace/intermediate/ecoregions_mask/\"\n",
    "    # fs.ls includes the parent folder itself, skip that link\n",
    "    mask_paths = [tp for tp in fs.ls(mask_folder) if not tp.endswith(\"/\") and include in tp]\n",
    "\n",
    "    all_lat_lon_tags = [utils.get_lat_lon_tags_from_tile_path(tp) for tp in mask_paths]\n",
    "\n",
    "    lat_lon_tags = []\n",
    "    for lat, lon in all_lat_lon_tags:\n",
    "        fn = f\"{lat}_{lon}\"\n",
    "        output_path = f\"s3://carbonplan-climatetrace/v1/biomass/{lat}_{lon}.zarr/.zmetadata\"\n",
    "        if not fs.exists(output_path) and not fn in no_data_tiles:\n",
    "            lat_lon_tags.append((lat, lon))\n",
    "\n",
    "    return lat_lon_tags\n",
    "\n",
    "\n",
    "lat_lon_tags = get_list_of_mask_tiles()\n",
    "# this should be in the order of min_lat, max_lat, min_lon, max_lon\n",
    "bounding_boxes = [utils.parse_bounding_box_from_lat_lon_tags(lat, lon) for lat, lon in lat_lon_tags]\n",
    "\n",
    "len(bounding_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run all tiles within the lat/lon box\n",
    "\n",
    "# min_lat = -90\n",
    "# max_lat = 90\n",
    "# min_lon = -180\n",
    "# max_lon = 180\n",
    "\n",
    "# tiles = utils.find_tiles_for_bounding_box(\n",
    "#     min_lat=min_lat, max_lat=max_lat, min_lon=min_lon, max_lon=max_lon\n",
    "# )\n",
    "# all_lat_lon_tags = [utils.get_lat_lon_tags_from_tile_path(tp) for tp in tiles]\n",
    "# bounding_boxes = [\n",
    "#     utils.parse_bounding_box_from_lat_lon_tags(lat, lon)\n",
    "#     for lat, lon in all_lat_lon_tags\n",
    "# ]\n",
    "# len(bounding_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "skip_existing = True\n",
    "tasks = []\n",
    "\n",
    "for bounding_box in bounding_boxes:\n",
    "    tasks.append(\n",
    "        client.compute(\n",
    "            process_one_tile(bounding_box, skip_existing, access_key_id, secret_access_key)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = dask.compute(tasks, retries=1)[0]\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, task in enumerate(tasks):\n",
    "    if task.status != \"pending\":\n",
    "        print(i)\n",
    "        print(task.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for task in tasks:\n",
    "#     task.cancel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = fs.get_mapper(\"s3://carbonplan-climatetrace/v1/biomass/50N_120W.zarr\")\n",
    "ds = xr.open_zarr(mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds.stack(unique_index=(\"record_index\", \"shot_number\")).dropna(dim=\"unique_index\", subset=[\"lat\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
