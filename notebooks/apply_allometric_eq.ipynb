{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fsspec\n",
    "import time\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from carbonplan_trace.v1.glas_preprocess import preprocess\n",
    "import carbonplan_trace.v1.glas_allometric_eq as allo\n",
    "import carbonplan_trace.v1.utils as utils\n",
    "from carbonplan_trace.v1.glas_height_metrics import get_all_height_metrics\n",
    "\n",
    "\n",
    "from gcsfs import GCSFileSystem\n",
    "\n",
    "fs = GCSFileSystem(cache_timeout=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "\n",
    "# warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run all tiles that doesn't exist in output yet\n",
    "\n",
    "# def get_list_of_mask_tiles(include=\"\"):\n",
    "#     \"\"\"\n",
    "#     Ecoregions mask is stored in 10 degree tiles, grab the filepaths\n",
    "#     \"\"\"\n",
    "#     no_data_tiles = [\"00N_070E\", \"20N_120W\", \"30N_170W\", \"40N_070W\"]\n",
    "\n",
    "#     fs = GCSFileSystem(cache_timeout=0)\n",
    "#     mask_folder = \"gs://carbonplan-climatetrace/intermediates/ecoregions_mask/\"\n",
    "#     # fs.ls includes the parent folder itself, skip that link\n",
    "#     mask_paths = [\n",
    "#         tp\n",
    "#         for tp in fs.ls(mask_folder)\n",
    "#         if not tp.endswith(\"/\") and include in tp\n",
    "#     ]\n",
    "\n",
    "#     all_lat_lon_tags = [\n",
    "#         utils.get_lat_lon_tags_from_tile_path(tp) for tp in mask_paths\n",
    "#     ]\n",
    "\n",
    "#     lat_lon_tags = []\n",
    "#     for lat, lon in all_lat_lon_tags:\n",
    "#         fn = f\"{lat}_{lon}\"\n",
    "#         output_path = f\"gs://carbonplan-climatetrace/intermediates/biomass/{lat}_{lon}.zarr/.zmetadata\"\n",
    "#         if not fs.exists(output_path) and not fn in no_data_tiles:\n",
    "#             lat_lon_tags.append((lat, lon))\n",
    "\n",
    "#     return lat_lon_tags\n",
    "\n",
    "\n",
    "# lat_lon_tags = get_list_of_mask_tiles()\n",
    "# # this should be in the order of min_lat, max_lat, min_lon, max_lon\n",
    "# bounding_boxes = [\n",
    "#     utils.parse_bounding_box_from_lat_lon_tags(lat, lon)\n",
    "#     for lat, lon in lat_lon_tags\n",
    "# ]\n",
    "\n",
    "# len(bounding_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(n_workers=3, threads_per_worker=1)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from dask_gateway import Gateway\n",
    "\n",
    "# gateway = Gateway()\n",
    "# options = gateway.cluster_options()\n",
    "# options.worker_cores = 4\n",
    "# options.worker_memory = 120\n",
    "# cluster = gateway.new_cluster(cluster_options=options)\n",
    "# cluster.adapt(minimum=1, maximum=10)\n",
    "# cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = cluster.get_client()\n",
    "# client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dask.distributed import PipInstall\n",
    "# plugin = PipInstall(packages=[\"git+https://github.com/carbonplan/trace.git@debug_biomass#egg=carbonplan_trace\"],\n",
    "#                     pip_options=[\"-e\"])\n",
    "# client.register_worker_plugin(plugin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "\n",
    "\n",
    "@dask.delayed\n",
    "def process_one_tile(bounding_box, skip_existing):\n",
    "    min_lat, max_lat, min_lon, max_lon = bounding_box\n",
    "    lat_tag, lon_tag = utils.get_lat_lon_tags_from_bounding_box(\n",
    "        max_lat, min_lon\n",
    "    )\n",
    "    biomass_path = f\"gs://carbonplan-climatetrace/intermediates/biomass/baccini_ground_all/{lat_tag}_{lon_tag}.zarr\"\n",
    "    preprocessed_path = f\"gs://carbonplan-climatetrace/intermediates/preprocessed_lidar/{lat_tag}_{lon_tag}.zarr\"\n",
    "\n",
    "    with dask.config.set(scheduler=\"single-threaded\"):\n",
    "        if skip_existing and fs.exists(biomass_path + \"/.zmetadata\"):\n",
    "            return (\"skipped\", biomass_path)\n",
    "\n",
    "        if fs.exists(preprocessed_path + \"/.zmetadata\"):\n",
    "            try:\n",
    "                preprocessed = (\n",
    "                    utils.open_zarr_file(preprocessed_path)\n",
    "                    .stack(unique_index=(\"record_index\", \"shot_number\"))\n",
    "                    .dropna(dim=\"unique_index\", subset=[\"lat\"])\n",
    "                )\n",
    "            except:\n",
    "                return (\"failed to open lidar\", biomass_path)\n",
    "\n",
    "        else:\n",
    "            try:\n",
    "                # read in data, this step takes about 5 mins\n",
    "                data01 = utils.open_glah01_data()\n",
    "                data14 = utils.open_glah14_data()\n",
    "\n",
    "                # subset data to the bounding box\n",
    "                sub14 = utils.subset_data_for_bounding_box(\n",
    "                    data14, min_lat, max_lat, min_lon, max_lon\n",
    "                )\n",
    "                sub01 = data01.where(\n",
    "                    data01.record_index.isin(sub14.record_index), drop=True\n",
    "                )\n",
    "                combined = sub14.merge(sub01, join=\"inner\")\n",
    "\n",
    "                if len(combined.record_index) == 0:\n",
    "                    return (\"no data in lidar\", biomass_path)\n",
    "\n",
    "                # preprocess data and persist\n",
    "                preprocessed = preprocess(\n",
    "                    combined, min_lat, max_lat, min_lon, max_lon\n",
    "                ).compute()\n",
    "                del combined, sub14, sub01\n",
    "\n",
    "                if len(preprocessed.record_index) == 0:\n",
    "                    return (\"no data after preprocess\", biomass_path)\n",
    "\n",
    "                preprocessed[\"datetime\"] = preprocessed.datetime.astype(\n",
    "                    \"datetime64[ns]\"\n",
    "                )\n",
    "                utils.save_to_zarr(\n",
    "                    ds=preprocessed.unstack(\"unique_index\").chunk(\n",
    "                        {\"record_index\": 10000, \"shot_number\": 40}\n",
    "                    ),\n",
    "                    url=preprocessed_path,\n",
    "                    mode=\"w\",\n",
    "                )\n",
    "            except:\n",
    "                return (\"failed in preprocess\", biomass_path)\n",
    "        # calculate biomass\n",
    "\n",
    "        try:\n",
    "            with_biomass = allo.apply_allometric_equation(\n",
    "                preprocessed, min_lat, max_lat, min_lon, max_lon\n",
    "            )\n",
    "\n",
    "            # saving output\n",
    "            height_metrics = [\n",
    "                \"VH\",\n",
    "                \"h25_Neigh\",\n",
    "                \"h50_Neigh\",\n",
    "                \"h75_Neigh\",\n",
    "                \"h90_Neigh\",\n",
    "                \"QMCH\",\n",
    "                \"MeanH\",\n",
    "                \"f_slope\",\n",
    "                \"senergy\",\n",
    "            ]\n",
    "\n",
    "            with_biomass = get_all_height_metrics(\n",
    "                with_biomass, height_metrics\n",
    "            ).compute()\n",
    "            variables = [\n",
    "                \"lat\",\n",
    "                \"lon\",\n",
    "                \"time\",\n",
    "                \"biomass\",\n",
    "                \"allometric_eq\",\n",
    "                \"glas_elev\",\n",
    "                \"ecoregion\",\n",
    "                \"eosd\",\n",
    "                \"nlcd\",\n",
    "                \"igbp\",\n",
    "                \"treecover2000_mean\",\n",
    "                \"burned\",\n",
    "            ]\n",
    "            utils.save_to_zarr(\n",
    "                ds=with_biomass.unstack(\"unique_index\").chunk(\n",
    "                    {\"record_index\": 10000, \"shot_number\": 40}\n",
    "                ),\n",
    "                url=biomass_path,\n",
    "                list_of_variables=variables + height_metrics,\n",
    "                mode=\"w\",\n",
    "            )\n",
    "\n",
    "            return (\"processed\", biomass_path)\n",
    "        except:\n",
    "            return (\"failed\", biomass_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessed_path = f\"gs://carbonplan-climatetrace/intermediates/preprocessed_lidar/{tiles[10]}.zarr\"\n",
    "\n",
    "# preprocessed = (\n",
    "#     utils.open_zarr_file(preprocessed_path)\n",
    "#     .stack(unique_index=(\"record_index\", \"shot_number\"))\n",
    "#     .dropna(dim=\"unique_index\", subset=[\"lat\"])\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fns = [\n",
    "#     '70N_010E', '70N_020E',\n",
    "#     '60N_000E', '60N_010E', '60N_020E', '60N_030E', '60N_040E', '60N_050E',\n",
    "#     '50N_090W', '50N_080W', '50N_010W', '50N_000E', '50N_010E', '50N_020E', '50N_030E',\n",
    "#     '40N_100W', '40N_090W', '40N_080W', '40N_030W', '40N_020W',\n",
    "#     '30N_100W', '30N_090W',\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_lat = 30\n",
    "max_lat = 40\n",
    "min_lon = 60\n",
    "max_lon = 140\n",
    "\n",
    "tiles = utils.find_tiles_for_bounding_box(\n",
    "    min_lat=min_lat, max_lat=max_lat, min_lon=min_lon, max_lon=max_lon\n",
    ")\n",
    "all_lat_lon_tags = [utils.get_lat_lon_tags_from_tile_path(tp) for tp in tiles]\n",
    "bounding_boxes = [\n",
    "    utils.parse_bounding_box_from_lat_lon_tags(lat, lon)\n",
    "    for lat, lon in all_lat_lon_tags\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "skip_existing = True\n",
    "\n",
    "tasks = []\n",
    "for bounding_box in bounding_boxes:\n",
    "    tasks.append(process_one_tile(bounding_box, skip_existing))\n",
    "#     process_one_tile(bounding_box, skip_existing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dask.compute(tasks, retries=10)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = fsspec.get_mapper(\n",
    "    \"s3://carbonplan-climatetrace/v1/data/intermediates/annual_averaged_worldclim.zarr\"\n",
    ")\n",
    "worldclim_ds = xr.open_zarr(mapper, consolidated=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worldclim_ds.nbytes / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worldclim_ds[\"BIO15\"].max().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worldclim_ds[\"BIO15\"].min().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for var in worldclim_ds.data_vars:\n",
    "    df = worldclim_ds[var].to_dataframe()\n",
    "    print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add ancillary data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biomass_folder = \"gs://carbonplan-climatetrace/intermediates/biomass/\"\n",
    "biomass_paths = [\n",
    "    path for path in fs.ls(biomass_folder) if not path.endswith(\"/\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = fsspec.get_mapper(\"gs://carbonplan-data/raw/worldclim/30s/raster.zarr\")\n",
    "worldclim = xr.open_zarr(mapper, consolidated=True).rename(\n",
    "    {\"x\": \"lon\", \"y\": \"lat\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "worldclim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group monthly worldclim data into seasons DJF MAM JJA SON\n",
    "days_in_month = {\n",
    "    1: 31,\n",
    "    2: 28.25,\n",
    "    3: 31,\n",
    "    4: 30,\n",
    "    5: 31,\n",
    "    6: 30,\n",
    "    7: 31,\n",
    "    8: 31,\n",
    "    9: 30,\n",
    "    10: 31,\n",
    "    11: 30,\n",
    "    12: 31,\n",
    "}\n",
    "\n",
    "months_in_season = [\n",
    "    (1, [12, 1, 2]),\n",
    "    (4, [3, 4, 5]),\n",
    "    (7, [6, 7, 8]),\n",
    "    (10, [9, 10, 11]),\n",
    "]\n",
    "\n",
    "month_to_season = {}\n",
    "for s, m in months_in_season:\n",
    "    month_to_season.update({mm: s for mm in m})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "monthly_variables = [\"prec\", \"srad\", \"tavg\", \"tmax\", \"tmin\", \"vapr\", \"wind\"]\n",
    "\n",
    "seasons = []\n",
    "seasonal_data = []\n",
    "for season, months in months_in_season:\n",
    "    weights = xr.DataArray(\n",
    "        [days_in_month[m] for m in months],\n",
    "        dims=[\"month\"],\n",
    "        coords={\"month\": months},\n",
    "    )\n",
    "\n",
    "    seasons.append(season)\n",
    "    seasonal_data.append(\n",
    "        worldclim[monthly_variables]\n",
    "        .sel(month=months)\n",
    "        .weighted(weights)\n",
    "        .mean(dim=\"month\")\n",
    "    )\n",
    "\n",
    "seasonal_data = xr.concat(seasonal_data, pd.Index(seasons, name=\"season\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonal_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_vars = [f\"BIO{str(n).zfill(2)}\" for n in range(1, 20)] + [\"elev\"]\n",
    "static_data = worldclim[static_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vars = (\n",
    "    [\n",
    "        \"VH\",\n",
    "        \"h25_Neigh\",\n",
    "        \"h50_Neigh\",\n",
    "        \"h75_Neigh\",\n",
    "        \"h90_Neigh\",\n",
    "        \"QMCH\",\n",
    "        \"MeanH\",\n",
    "        \"f_slope\",\n",
    "        \"senergy\",\n",
    "        \"lat\",\n",
    "        \"lon\",\n",
    "        \"time\",\n",
    "        \"biomass\",\n",
    "        \"allometric_eq\",\n",
    "        \"glas_elev\",\n",
    "        \"ecoregion\",\n",
    "        \"eosd\",\n",
    "        \"nlcd\",\n",
    "        \"igbp\",\n",
    "        \"treecover2000_mean\",\n",
    "        \"burned\",\n",
    "    ]\n",
    "    + static_vars\n",
    "    + monthly_variables\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "import time\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "failed = []\n",
    "\n",
    "for path in biomass_paths:\n",
    "    if fs.exists(path + \"/BIO01\"):\n",
    "        continue\n",
    "    else:\n",
    "        print(path)\n",
    "        lat, lon = utils.get_lat_lon_tags_from_tile_path(path)\n",
    "\n",
    "        # load the biomass data\n",
    "        biomass = (\n",
    "            utils.open_zarr_file(path)\n",
    "            .stack(unique_index=(\"record_index\", \"shot_number\"))\n",
    "            .dropna(dim=\"unique_index\", subset=[\"lat\"])\n",
    "        )\n",
    "\n",
    "        # find the static data to index to\n",
    "        records = utils.find_matching_records(\n",
    "            data=static_data, lats=biomass.lat, lons=biomass.lon\n",
    "        )\n",
    "        for v in static_vars:\n",
    "            biomass[v] = records[v]\n",
    "\n",
    "        # find the seasonal data to index to\n",
    "        biomass[\"datetime\"] = xr.apply_ufunc(\n",
    "            datetime.fromtimestamp,\n",
    "            biomass.time,\n",
    "            vectorize=True,\n",
    "            dask=\"parallelized\",\n",
    "        )\n",
    "        biomass[\"datetime\"] = biomass.datetime.astype(\"datetime64[ns]\")\n",
    "        biomass[\"month\"] = biomass.datetime.dt.month\n",
    "        biomass[\"season\"] = xr.apply_ufunc(\n",
    "            month_to_season.__getitem__,\n",
    "            biomass.month.astype(int),\n",
    "            vectorize=True,\n",
    "            dask=\"parallelized\",\n",
    "            output_dtypes=[int],\n",
    "        )\n",
    "\n",
    "        records = seasonal_data.sel(\n",
    "            lat=biomass.lat,\n",
    "            lon=biomass.lon,\n",
    "            season=biomass.season,\n",
    "            method=\"nearest\",\n",
    "        ).drop_vars([\"lat\", \"lon\", \"season\"])\n",
    "        for v in monthly_variables:\n",
    "            biomass[v] = records[v]\n",
    "\n",
    "        local_path = f\"/home/jovyan/temp/{lat}_{lon}.zarr\"\n",
    "        biomass[\"allometric_eq\"] = biomass.allometric_eq.astype(\n",
    "            np.dtype(\"<U35\")\n",
    "        )\n",
    "\n",
    "        utils.save_to_zarr(\n",
    "            ds=biomass.unstack(\"unique_index\").chunk(\n",
    "                {\"record_index\": 10000, \"shot_number\": 40}\n",
    "            ),\n",
    "            url=local_path,\n",
    "            list_of_variables=all_vars,\n",
    "            mode=\"w\",\n",
    "        )\n",
    "\n",
    "        fs.rm(path, recursive=True)\n",
    "        time.sleep(60)\n",
    "        fs.put(local_path, path, recursive=True)\n",
    "        time.sleep(60)\n",
    "        shutil.rmtree(local_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
