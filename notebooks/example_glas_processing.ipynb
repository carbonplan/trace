{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fsspec\n",
    "import h5py\n",
    "import os\n",
    "import scipy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import interpolate\n",
    "from scipy.ndimage import gaussian_filter1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local files\n",
    "# we downloaded all data within the bounding box of bounding_box = '-77,38.5,-76,39.5'\n",
    "data_dir = \"/home/jovyan/data/glas/example/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcsfs import GCSFileSystem\n",
    "\n",
    "fs = GCSFileSystem(cache_timeout=0)\n",
    "\n",
    "folder = f\"gs://carbonplan-climatetrace/inputs/glas-raw/\"\n",
    "files = [\n",
    "    f for f in fs.ls(folder) if \"GLAH01\" in f and not f.endswith(\"/\") and not f.endswith(\".xml\")\n",
    "]\n",
    "# open the volt table and compare them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "to_open = random.sample(population=files, k=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output = []\n",
    "\n",
    "for uri in files:\n",
    "    with fs.open(f\"gs://{uri}\") as f:\n",
    "        try:\n",
    "            f01 = h5py.File(f, \"r\")\n",
    "            rec_wf = f01[\"Data_40HZ/Waveform/RecWaveform/r_rng_wf\"][:]\n",
    "            table1 = f01[\"ANCILLARY_DATA\"].attrs[\"volt_table_1\"]\n",
    "\n",
    "            possible_values = np.unique(rec_wf)\n",
    "            for v in possible_values:\n",
    "                if v < 10:\n",
    "                    assert v in table1\n",
    "\n",
    "            output.append(table1)\n",
    "        except:\n",
    "            print(uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.all(np.unique(output) == table1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f01 = h5py.File(data_dir + \"GLAH01_033_2107_003_0241_4_02_0001.H5\", \"r\")\n",
    "f14 = h5py.File(data_dir + \"GLAH14_634_2107_003_0239_0_01_0001.H5\", \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f01[\"ANCILLARY_DATA\"].attrs['gain_table']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(1, len(table1)):\n",
    "    print(table1[i] - table1[i - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table1 = f01[\"ANCILLARY_DATA\"].attrs[\"volt_table_1\"]\n",
    "table2 = f01[\"ANCILLARY_DATA\"].attrs[\"volt_table_2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_wf = f01[\"Data_40HZ/Waveform/RecWaveform/r_rng_wf\"][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "possible_values = np.unique(rec_wf)\n",
    "for v in possible_values:\n",
    "    if v < 10:\n",
    "        if v in table1:\n",
    "            print(\"table1\")\n",
    "        elif v in table2:\n",
    "            print(\"table2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gain = f01[\"Data_40HZ/Waveform/Characteristics/i_gainSet1064\"][:]\n",
    "rawpkht = f01[\"Data_40HZ/Waveform/Characteristics/i_rawPkHt\"][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(rawpkht, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # files on gcp\n",
    "# fs = fsspec.get_filesystem_class('gs')(account_name='carbonplan')\n",
    "# data_dir = ''\n",
    "# all_files = fs.ls(path='carbonplan-scratch/glas-cache/')\n",
    "# example_files = [f for f in all_files if not f.endswith('.xml')]\n",
    "# print(len(example_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try to find the exact record that Sun et al 2008 used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to find the record Sun et al 2008 uses by the record number\n",
    "rec = 246268022\n",
    "\n",
    "\n",
    "def find_record_in_local_files(\n",
    "    target_rec,\n",
    "    var_name,\n",
    "    data_dir,\n",
    "    product_num=None,\n",
    "):\n",
    "    if not product_num:\n",
    "        product_num = \"\"\n",
    "\n",
    "    all_files = [\n",
    "        f\n",
    "        for f in os.listdir(data_dir)\n",
    "        if not f.endswith(\".xml\") and f.startswith(\"GLAH\" + product_num)\n",
    "    ]\n",
    "\n",
    "    target_file = None\n",
    "\n",
    "    for file in all_files:\n",
    "        f = h5py.File(data_dir + file, \"r\")\n",
    "        records = f[var_name][:]\n",
    "        if target_rec in records:\n",
    "            print(\"found record!\")\n",
    "            print(file)\n",
    "            target_file = file\n",
    "\n",
    "    if not target_file:\n",
    "        print(target_file or \"Did not find record\")\n",
    "\n",
    "    return target_file\n",
    "\n",
    "\n",
    "find_record_in_local_files(rec, \"Data_1HZ/Time/i_rec_ndx\", data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat = 39.013840  # 39.021599\n",
    "lon = 283.156830"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_record_in_local_files(lat, \"Data_40HZ/Geolocation/d_lat\", data_dir, \"14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_record_in_local_files(lon, \"Data_40HZ/Geolocation/d_lon\", data_dir, \"14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_record_in_local_files(lat, \"Data_1HZ/Geolocation/d1_pred_lat\", data_dir, \"01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how about find records in the vicinity of the lat/lon of our interest\n",
    "\n",
    "\n",
    "def match_lat_lng_in_record_files(\n",
    "    lat,\n",
    "    lon,\n",
    "    data_dir,\n",
    "    product_num=None,\n",
    "    buffer=0,\n",
    "):\n",
    "    if not product_num:\n",
    "        product_num = \"\"\n",
    "\n",
    "    all_files = [\n",
    "        f\n",
    "        for f in os.listdir(data_dir)\n",
    "        if not f.endswith(\".xml\") and f.startswith(\"GLAH\" + product_num)\n",
    "    ]\n",
    "\n",
    "    target_file = []\n",
    "\n",
    "    for file in all_files:\n",
    "        f = h5py.File(data_dir + file, \"r\")\n",
    "        lats = f[\"Data_40HZ/Geolocation/d_lat\"][:]\n",
    "        lons = f[\"Data_40HZ/Geolocation/d_lon\"][:]\n",
    "        rec_nums = f[\"Data_40HZ/Time/i_rec_ndx\"][:]\n",
    "\n",
    "        matching_records = rec_nums[\n",
    "            np.where(\n",
    "                (lats > lat - buffer)\n",
    "                & (lats < lat + buffer)\n",
    "                & (lons > lon - buffer)\n",
    "                & (lons < lon + buffer)\n",
    "            )\n",
    "        ]\n",
    "        if len(matching_records) > 0:\n",
    "            print(\"found record!\")\n",
    "            print(np.unique(matching_records))\n",
    "            print(file)\n",
    "            target_file.append(file)\n",
    "\n",
    "    if not target_file:\n",
    "        print(\"Did not find record\")\n",
    "\n",
    "    return target_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buff = 0.1\n",
    "\n",
    "match_lat_lng_in_record_files(\n",
    "    lat,\n",
    "    lon,\n",
    "    data_dir,\n",
    "    product_num=\"14\",\n",
    "    buffer=buff,\n",
    ")\n",
    "\n",
    "# we found many records!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looks like we can't find the exact record, but we can find nearby ones\n",
    "\n",
    "# Let's just pick a nearby one at random to work with\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's just pick one at random to work with\n",
    "\n",
    "record = 226677189"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the file names\n",
    "file01 = find_record_in_local_files(record, \"Data_1HZ/Time/i_rec_ndx\", data_dir, \"01\")\n",
    "file14 = find_record_in_local_files(record, \"Data_1HZ/Time/i_rec_ndx\", data_dir, \"14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at least we proved that we can find matching records from GLAH14 to GLAH01...\n",
    "# let's now take this example and try to duplicate Sun's analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f01 = h5py.File(data_dir + file01, \"r\")\n",
    "f14 = h5py.File(data_dir + file14, \"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get raw data from H5 file and put into xarray format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "\n",
    "def read_dimensions(file_handle):\n",
    "    # put the dimension columns into a dataframe\n",
    "    df = {}\n",
    "    df[\"record_index\"] = file_handle[\"Data_40HZ/Time/i_rec_ndx\"][:]\n",
    "    df[\"shot_number\"] = file_handle[\"Data_40HZ/Time/i_shot_count\"][:]\n",
    "    df = pd.DataFrame(df)\n",
    "\n",
    "    # concat two columns to get an unique index, but also set these two columns as multi level index\n",
    "    df[\"unique_index\"] = (\n",
    "        df.record_index.astype(str).str.zfill(9) + \"_\" + df.shot_number.astype(str).str.zfill(2)\n",
    "    )\n",
    "    df.set_index([\"record_index\", \"shot_number\"], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def read_1d_variables(file_handle, mapping, unique_index, replace_fill_values_with_nulls):\n",
    "    # put the 1D variables into a xarray dataset\n",
    "    ds = {}\n",
    "    for k, v in mapping.items():\n",
    "        temp = file_handle[v][:]\n",
    "        if replace_fill_values_with_nulls:\n",
    "            temp[(temp > 1e100)] = np.nan\n",
    "        ds[k] = xr.DataArray(temp, dims=[\"unique_index\"], coords={\"unique_index\": unique_index})\n",
    "\n",
    "    return xr.Dataset(ds)\n",
    "\n",
    "\n",
    "def extract_GLAH01_data(filename, replace_fill_values_with_nulls=True):\n",
    "    \"\"\"\n",
    "    Given file name of a HDF5 GLAH01 data, returns a xarray dataset with record index and shot number being the primary dimensions.\n",
    "    rec_bin and tx_bin are additional dimensions regarding the transmitted and received waveforms.\n",
    "    \"\"\"\n",
    "\n",
    "    # read file\n",
    "    f = h5py.File(filename, \"r\")\n",
    "\n",
    "    # list out all 1D variables we want and read into a dataframe\n",
    "    name_map = {\n",
    "        # Background Noise Mean Value for the 4 ns filter. From APID12/13, Offset 112.\n",
    "        \"noise_mean\": \"Data_40HZ/Waveform/Characteristics/d_4nsBgMean\",  # volts\n",
    "        # The standard deviation of the background noise for the 4 ns filter. From APID12/13, Offset 116.\n",
    "        \"noise_sd\": \"Data_40HZ/Waveform/Characteristics/d_4nsBgSDEV\",  # volts\n",
    "        \"rec_wf_location_ind\": \"Data_40HZ/Waveform/RecWaveform/i_rec_wf_location_index\",  # This is an index into the array of 544 times within the rec_wf_sample_location_table (found in the ANCILLARY_DATA group)\n",
    "        \"rec_wf_response_end_time\": \"Data_40HZ/Waveform/RecWaveform/i_RespEndTime\",\n",
    "        \"tx_wf_peak_time\": \"Data_40HZ/Waveform/TransmitWaveform/i_time_txWfPk\",  # Address in digitizer counts of the Transmit Pulse Peak as measured from the start of Acquisition Memory, i.e. start of digitization. From APID12/13, Offset 68.\n",
    "        #     'wf_type': 'Data_40HZ/Waveform/Characteristics/i_waveformType', # Indicates number of valid samples in waveform; 0 = missing; 1 = Long waveform (544 samples); 2 =Short waveform (200 samples),\n",
    "        #     'tx_wf_start_time': 'Data_40HZ/Waveform/TransmitWaveform/i_TxWfStart'  # Starting Address in digitizer counts of the Transmit Pulse sample relative to the start of digitization. From APID12/13, Offset 76.\n",
    "    }\n",
    "\n",
    "    # put the dimension columns into a dataframe\n",
    "    df = read_dimensions(f)\n",
    "\n",
    "    # put the 1D variables into a xarray dataset\n",
    "    ds = read_1d_variables(\n",
    "        file_handle=f,\n",
    "        mapping=name_map,\n",
    "        unique_index=df.unique_index.values,\n",
    "        replace_fill_values_with_nulls=False,\n",
    "    )\n",
    "\n",
    "    # read the 2D variables we want\n",
    "    # Transmit Pulse 48 waveform samples in calibrated volts. The delta times for transmit waveform sample j is provided in the attribute array tx_wf_sample_location_table (j).\n",
    "    tx_wf = f[\"Data_40HZ/Waveform/TransmitWaveform/r_tx_wf\"][:]\n",
    "    # The delta times for each echo of the 544 waveform samples is provided within the 544 times stored in rec_wf_sample_location_table\n",
    "    # (an attribute in the /ANCILLARY_DATA group) and indexed by i_rec_wf_location_index.\n",
    "    rec_wf = f[\"Data_40HZ/Waveform/RecWaveform/r_rng_wf\"][:]  # n (num shot * num records) x 544\n",
    "    tx_wf_sample_loc = f[\"ANCILLARY_DATA\"].attrs[\"tx_wf_sample_location_table\"]\n",
    "    rec_wf_sample_loc = f[\"ANCILLARY_DATA\"].attrs[\"rec_wf_sample_location_table\"]  # 5 x 544\n",
    "\n",
    "    if replace_fill_values_with_nulls:\n",
    "        rec_wf_sample_loc[(rec_wf_sample_loc > 1e10)] = np.nan\n",
    "\n",
    "    # put the 2D variables into xarray\n",
    "    ds[\"rec_wf\"] = xr.DataArray(\n",
    "        rec_wf,\n",
    "        dims=[\"unique_index\", \"rec_bin\"],\n",
    "        coords=[df.unique_index.values, np.arange(rec_wf.shape[1])],\n",
    "    )\n",
    "    ds[\"tx_wf\"] = xr.DataArray(\n",
    "        tx_wf,\n",
    "        dims=[\"unique_index\", \"tx_bin\"],\n",
    "        coords=[df.unique_index.values, np.arange(tx_wf.shape[1])],\n",
    "    )\n",
    "    ds[\"tx_wf_sample_loc\"] = xr.DataArray(\n",
    "        tx_wf_sample_loc, dims=[\"tx_bin\"], coords=[np.arange(tx_wf.shape[1])]\n",
    "    )\n",
    "\n",
    "    # store a copy of the sample location for each unique shot\n",
    "    ds[\"rec_wf_sample_loc\"] = xr.DataArray(\n",
    "        rec_wf_sample_loc[ds.rec_wf_location_ind - 1],\n",
    "        dims=[\"unique_index\", \"rec_bin\"],\n",
    "        coords=[df.unique_index.values, np.arange(rec_wf.shape[1])],\n",
    "    )\n",
    "\n",
    "    # expand the multi index\n",
    "    ds.coords[\"unique_index\"] = df.index\n",
    "    ds = ds.unstack(\"unique_index\")\n",
    "\n",
    "    return ds\n",
    "\n",
    "\n",
    "def extract_GLAH14_data(filename, replace_fill_values_with_nulls=True):\n",
    "    \"\"\"\n",
    "    Given file name of a HDF5 GLAH14 data, returns a xarray dataset with record index and shot number being the primary dimensions.\n",
    "    rec_bin and tx_bin are additional dimensions regarding the transmitted and received waveforms.\n",
    "    \"\"\"\n",
    "\n",
    "    # read file\n",
    "    f = h5py.File(filename, \"r\")\n",
    "\n",
    "    # list out all 1D variables we want and read into a dataframe\n",
    "    name_map = {\n",
    "        # The transmit time of each shot in the 1 second frame measured as UTC seconds elapsed since Jan 1 2000 12:00:00 UTC. This time has been derived from the GPS time accounting for leap seconds.\n",
    "        \"time\": \"Data_40HZ/Time/d_UTCTime_40\",\n",
    "        \"lat\": \"Data_40HZ/Geolocation/d_lat\",\n",
    "        \"lon\": \"Data_40HZ/Geolocation/d_lon\",\n",
    "        # the documentation mentioned two flags sat_corr_flg and i_satNdx to signal bad elevation, also and when correction is invalid the elevation is invalid\n",
    "        \"elevation\": \"Data_40HZ/Elevation_Surfaces/d_elev\",  # meters\n",
    "        \"elevation_correction\": \"Data_40HZ/Elevation_Corrections/d_satElevCorr\",  # should be added to elevation\n",
    "        # Range in distance calculated from the time between the centroid of the transmit pulse and the farthest gate from the spacecraft of the received pulse. See the rngcorrflg to determine\n",
    "        # any corrections that have been applied. unit is meters and values in the 600k range\n",
    "        \"ref_range\": \"Data_40HZ/Elevation_Surfaces/d_refRng\",  # meters\n",
    "        # these should be added to centroid according to the documentation\n",
    "        \"sig_begin_offset\": \"Data_40HZ/Elevation_Offsets/d_SigBegOff\",  # meters\n",
    "        \"sig_end_offset\": \"Data_40HZ/Elevation_Offsets/d_SigEndOff\",  # meters\n",
    "        # Range offset to be added to d_refRng to calculate the range using the algorithm deemed appropriate for land.\n",
    "        \"centroid_offset\": \"Data_40HZ/Elevation_Offsets/d_ldRngOff\",  # meters\n",
    "        # data for the 6 fitted gaussian peaks\n",
    "        #         'num_gaussian_peaks': 'Data_40HZ/Waveform/i_numPk',\n",
    "        #         'gaussian_mu': 'Data_40HZ/Elevation_Offsets/d_gpCntRngOff',  # meters\n",
    "        #         'gaussian_amp': 'Data_40HZ/Waveform/d_Gamp',  # volts\n",
    "        #         'gaussian_sigma': 'Data_40HZ/Waveform/d_Gsigma',  # ns\n",
    "    }\n",
    "\n",
    "    # put the dimension columns into a dataframe\n",
    "    df = read_dimensions(f)\n",
    "\n",
    "    # put the 1D variables into a xarray dataset\n",
    "    ds = read_1d_variables(\n",
    "        file_handle=f,\n",
    "        mapping=name_map,\n",
    "        unique_index=df.unique_index.values,\n",
    "        replace_fill_values_with_nulls=replace_fill_values_with_nulls,\n",
    "    )\n",
    "\n",
    "    # expand the multi index\n",
    "    ds.coords[\"unique_index\"] = df.index\n",
    "    ds = ds.unstack(\"unique_index\")\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d01 = extract_GLAH01_data(data_dir + file01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d14 = extract_GLAH14_data(data_dir + file14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d01.tx_wf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d01.rec_wf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the delta time for each sample to be added to the ending address of range (i_RespEndTime) in order to provide the range in ns for each range waveform sample\n",
    "rec_wf_sample_loc = f[\"ANCILLARY_DATA\"].attrs[\"rec_wf_sample_location_table\"]\n",
    "plt.figure(figsize=(15, 8))\n",
    "for i in range(d01.rec_wf_sample_loc.shape[0]):\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    plt.plot(rec_wf_sample_loc[i, :])\n",
    "    plt.title(f\"{i}th row\")\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rec_wf_sample_loc[4, 390:394])\n",
    "compression_switch_ind = 392"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = pd.merge(data01, data14, on=[\"record_index\", \"shot_number\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data01.record_index.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data14.record_index.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(combined_data.record_index.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find a specific shot to work with\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's find a waveform within the record of our interest with distinct 2 peaks to work with\n",
    "\n",
    "inds = np.where(record_index == record)[0]\n",
    "plt.figure(figsize=(20, 20))\n",
    "for i, ind in enumerate(inds[0:20]):\n",
    "    plt.subplot(5, 4, i + 1)\n",
    "    plt.plot(rec_wf[ind, :], label=str(ind))\n",
    "    plt.legend()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 57202\n",
    "\n",
    "shot = shot_number[ind]\n",
    "record = record_index[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"we are going to work with record index = {record} and shot number = {shot}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing of GLAS data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we want to be able to plot the following metrics on the same plot where x = Lidar return and\n",
    "y = distance:\n",
    "\n",
    "1. Received waveform (rec_wf)\n",
    "2. The filtered/smoothed waveform\n",
    "3. Signal beginning\n",
    "4. Signal end\n",
    "5. Centroid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to plot sig_begin_offset and sig_end_offset together with the waveform, some translation is needed\n",
    "# Since we eventually want to calculate things in distance, let's make the plot i\n",
    "\n",
    "# first, the offsets needed to be added to the centroid elevation to get distance from satelite\n",
    "combined_data[\"sig_begin_distance\"] = combined_data.sig_begin_offset + combined_data.ref_range\n",
    "combined_data[\"sig_end_distance\"] = combined_data.sig_end_offset + combined_data.ref_range\n",
    "combined_data[\"centroid_distance\"] = combined_data.centroid_offset + combined_data.ref_range\n",
    "\n",
    "# then, we want to find the coordinates of received waveform digital bins as distance as well\n",
    "# we can do this by combining a few parameters\n",
    "# for each record index, the corresponding distances are going to be:\n",
    "speed_of_light = 299792458  # m/s\n",
    "seconds_in_nanoseconds = 10 ** -9\n",
    "\n",
    "digital_bins_in_distance = (\n",
    "    (\n",
    "        rec_wf_sample_loc[\n",
    "            data01.rec_wf_location_ind - 1\n",
    "        ]  # ns (a len(data01) x 544 array with delta offset for all digital bins, offset for bin 0 = 0)\n",
    "        + data01[\"rec_wf_response_end_time\"].values.reshape(\n",
    "            -1, 1\n",
    "        )  # ns (last time stamp of signal returns, should correspond to digital bin 0)\n",
    "        - data01[\"tx_wf_start_time\"].values.reshape(\n",
    "            -1, 1\n",
    "        )  # ns (time stamp of the transmit waveform peak)\n",
    "    )\n",
    "    * seconds_in_nanoseconds\n",
    "    * speed_of_light\n",
    ") / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we also need to figure out how the raw waveform is smoothed, the Sun et al 2008 paper stated that\n",
    "# \"The waveform was first filtered by a Gaussian filter of a width similar to the transmitted laser pulse.\"\n",
    "# let's find out what is the width of the transmitted laser pulse\n",
    "# assuming \"width\" of the Gaussian = sigma of the Gaussian here...\n",
    "\n",
    "from scipy import optimize\n",
    "\n",
    "\n",
    "def gaussian(x, amplitude, mean, stddev):\n",
    "    return amplitude * np.exp(-1 * ((x - mean) ** 2) / (2 * (stddev ** 2)))\n",
    "\n",
    "\n",
    "inds = np.where(record_index == record)[0]\n",
    "plt.figure(figsize=(20, 20))\n",
    "for i, idx in enumerate(inds[0:20]):\n",
    "    y = tx_wf[idx, :]\n",
    "    x = np.arange(len(y))\n",
    "    # optimizer can't deal with negative numbers\n",
    "    offset = y.min()\n",
    "    y -= offset\n",
    "\n",
    "    popt, _ = optimize.curve_fit(gaussian, x, y, p0=[0.5, 25, 1])\n",
    "    plt.subplot(5, 4, i + 1)\n",
    "    plt.scatter(x, y + offset, label=\"Raw\")\n",
    "    plt.plot(x, gaussian(x, *popt) + offset, \"k-\", label=\"Fitted\")\n",
    "    plt.title(f\"For index {idx}, fitted sigma = {round(popt[2],2)}\")\n",
    "    plt.legend()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# eye balling sigma to be ~3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 57202"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_record(df, record_of_interest, shot_of_interest):\n",
    "    return df.loc[(df.record_index == record_of_interest) & (df.shot_number == shot_of_interest)]\n",
    "\n",
    "\n",
    "def plot_based_on_ind(ind):\n",
    "    shot = shot_number[ind]\n",
    "    record = record_index[ind]\n",
    "    y = digital_bins_in_distance[ind]\n",
    "    x = rec_wf[ind]\n",
    "    rec_to_plot = find_record(combined_data, record, shot)\n",
    "    filtered = gaussian_filter1d(input=x, sigma=3)\n",
    "\n",
    "    # move the reference range to the bottom of received wf\n",
    "    # TODO: is this legit...??\n",
    "    bias = 0\n",
    "    bias = y.max() - rec_to_plot.ref_range.iloc[0]\n",
    "\n",
    "    plt.figure(figsize=(6, 10))\n",
    "    plt.scatter(x, y, s=5, label=\"Raw\")\n",
    "\n",
    "    # plot various variables found in GLAH14\n",
    "    plt.plot(\n",
    "        [-0.05, 0.5],\n",
    "        np.array([rec_to_plot.sig_begin_distance, rec_to_plot.sig_begin_distance]) + bias,\n",
    "        \"r--\",\n",
    "        label=\"SigBeg\",\n",
    "    )\n",
    "    plt.plot(\n",
    "        [-0.05, 0.5],\n",
    "        np.array([rec_to_plot.centroid_distance, rec_to_plot.centroid_distance]) + bias,\n",
    "        \"c--\",\n",
    "        label=\"Centroid\",\n",
    "    )\n",
    "    plt.plot(\n",
    "        [-0.05, 0.5],\n",
    "        np.array([rec_to_plot.sig_end_distance, rec_to_plot.sig_end_distance]) + bias,\n",
    "        \"g--\",\n",
    "        label=\"SigEnd\",\n",
    "    )\n",
    "    plt.plot(\n",
    "        [-0.05, 0.5],\n",
    "        np.array([rec_to_plot.ref_range, rec_to_plot.ref_range]) + bias,\n",
    "        \"k--\",\n",
    "        label=\"Reference Range\",\n",
    "    )\n",
    "\n",
    "    # plot noise mean and std from GLAH01\n",
    "    plt.plot(\n",
    "        [rec_to_plot.noise_mean, rec_to_plot.noise_mean],\n",
    "        [y.min(), y.max()],\n",
    "        \"0.5\",\n",
    "        label=\"Noise Mean\",\n",
    "    )\n",
    "    n_sig = 3.5\n",
    "    noise_threshold = rec_to_plot.noise_mean + n_sig * rec_to_plot.noise_sd\n",
    "    plt.plot(\n",
    "        [noise_threshold, noise_threshold],\n",
    "        [y.min(), y.max()],\n",
    "        color=\"0.5\",\n",
    "        linestyle=\"dashed\",\n",
    "        label=\"Noise Threshold\",\n",
    "    )\n",
    "\n",
    "    # plot where compression started being different\n",
    "    # TODO: need to make sure compression ratio switch always happens before SigBeg\n",
    "    plt.plot(\n",
    "        [-0.05, 0.5],\n",
    "        [y[compression_switch_ind], y[compression_switch_ind]],\n",
    "        \"b--\",\n",
    "        label=\"Compression Ratio Switch\",\n",
    "    )\n",
    "    plt.plot(filtered, y, \"k-\", label=\"Filtered\")\n",
    "\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.xlabel(\"lidar return (joules)\")\n",
    "    plt.ylabel(\"distance from satelite (m)\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "plot_based_on_ind(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_based_on_ind(15727)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alright, how about the actual percentile energies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_wf(bins, wf, target, area_to_include):\n",
    "    \"\"\"\n",
    "    if area_to_include = 'above', the function returns the area between the target location to the \"upper bound\" (larger value) bin\n",
    "    if area_to_include = 'below', the function returns the area between the target location to the \"lower bound\" (smaller value) bin\n",
    "    \"\"\"\n",
    "    upper_ind = np.where(bins > target)[0].max()\n",
    "    lower_ind = np.where(bins < target)[0].min()\n",
    "    # since bins goes from large to small values, the \"upper bound\" index would be smaller than the \"lower bound\" index\n",
    "    assert lower_ind - upper_ind == 1\n",
    "\n",
    "    x_upper = bins[upper_ind]\n",
    "    x_lower = bins[lower_ind]\n",
    "    y_upper = wf[upper_ind]\n",
    "    y_lower = wf[lower_ind]\n",
    "\n",
    "    x_mid = (x_upper + x_lower) / 2.0\n",
    "    x_span = x_upper - x_lower\n",
    "\n",
    "    if area_to_include == \"above\":\n",
    "        if target < x_mid:\n",
    "            energy = (x_mid - target) / x_span * y_lower  # energy to add to the lower bin\n",
    "            bin_to_modify = lower_ind\n",
    "        else:\n",
    "            energy = (x_mid - target) / x_span * y_upper  # energy to subtract out of the upper bin\n",
    "            bin_to_modify = upper_ind\n",
    "    elif area_to_include == \"below\":\n",
    "        if target < x_mid:\n",
    "            energy = (target - x_mid) / x_span * y_lower  # energy to subtract out of the lower bin\n",
    "            bin_to_modify = lower_ind\n",
    "        else:\n",
    "            energy = (target - x_mid) / x_span * y_upper  # energy to add to the upper bin\n",
    "            bin_to_modify = upper_ind\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            \"Please specify whether we want to include area above or below the target to the bounds\"\n",
    "        )\n",
    "\n",
    "    return bin_to_modify, energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ground_peak(bins, wf, sig_end, buffer=1):\n",
    "    valid_area = select_valid_area_no_interpolation(bins, wf, bins.min(), sig_end)\n",
    "    assert buffer >= 1\n",
    "    i = buffer\n",
    "    while i < len(bins) - 1 and (\n",
    "        valid_area[i] <= valid_area[i - 1] or valid_area[i] <= valid_area[i + 1]\n",
    "    ):\n",
    "        i += 1\n",
    "\n",
    "    return bins[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_wf(input_wf, sigma=3):\n",
    "    return gaussian_filter1d(input=input_wf, sigma=sigma)\n",
    "\n",
    "\n",
    "def denoise_wf(input_wf, noise_level):\n",
    "    return input_wf - noise_level\n",
    "\n",
    "\n",
    "def select_valid_area_no_interpolation(bins, wf, beg, end):\n",
    "    # initialize output\n",
    "    output = np.zeros(len(wf))\n",
    "\n",
    "    # within signal beginning and end locations, set otuput to be equal to input wf\n",
    "    valid = np.where((bins > beg) & (bins < end))[0]\n",
    "    output[valid] = wf[valid]\n",
    "\n",
    "    # min at 0\n",
    "    output = np.maximum(output, 0)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def select_valid_area(bins, wf, beg, end):\n",
    "    # initialize output\n",
    "    output = np.zeros(len(wf))\n",
    "\n",
    "    # within signal beginning and end locations, set otuput to be equal to input wf\n",
    "    valid = np.where((bins > beg) & (bins < end))[0]\n",
    "    output[valid] = wf[valid]\n",
    "\n",
    "    # for the begining and end bin, interpolate\n",
    "    # bins goes from large values (furthest away from satellite) to small (closest to satellite)\n",
    "    bin_to_modify, energy = interpolate_wf(bins, wf, beg, \"above\")\n",
    "    bins[bin_to_modify] += energy\n",
    "    bin_to_modify, energy = interpolate_wf(bins, wf, end, \"below\")\n",
    "    bins[bin_to_modify] += energy\n",
    "\n",
    "    # min at 0\n",
    "    output = np.maximum(output, 0)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def plot_percentile_heights(ind, scipy_interpolate=False):\n",
    "    shot = shot_number[ind]\n",
    "    record = record_index[ind]\n",
    "\n",
    "    bins = digital_bins_in_distance[ind]\n",
    "    raw_wf = rec_wf[ind]\n",
    "    other_data = find_record(combined_data, record, shot)\n",
    "\n",
    "    # move the reference range to the bottom of received wf\n",
    "    bias = bins.max() - other_data.ref_range.iloc[0]\n",
    "    sig_beg = other_data.sig_begin_distance.iloc[0] + bias\n",
    "    sig_end = other_data.sig_end_distance.iloc[0] + bias\n",
    "    # if the compression ratio has a kink in it (ie the compression ratio is not the same for begininng/end of the waveform,\n",
    "    # then make sure that the compression switch happened before actual signal beginning that's used in processing\n",
    "    if other_data.rec_wf_location_ind.iloc[0] == 5:\n",
    "        assert bins[compression_switch_ind] < sig_beg\n",
    "\n",
    "    # use gaussian filter to smooth and denoise\n",
    "    denoised_filtered_wf = denoise_wf(\n",
    "        input_wf=smooth_wf(raw_wf), noise_level=other_data.noise_mean.iloc[0]\n",
    "    )\n",
    "\n",
    "    if scipy_interpolate:\n",
    "        bins_x = np.arange(len(bins))\n",
    "        interpolation_func_bins = interpolate.interp1d(bins_x, bins, kind=\"linear\")\n",
    "        bins_x_interpolated = np.linspace(bins_x[0], bins_x[-1], (len(bins_x) - 1) * 100 + 1)\n",
    "        bins_interpolated = interpolation_func_bins(bins_x_interpolated)\n",
    "\n",
    "        interpolated_func_wf = interpolate.interp1d(bins, denoised_filtered_wf, kind=\"nearest\")\n",
    "        wf_interpolated = interpolated_func_wf(bins_interpolated)\n",
    "\n",
    "        # select the waveform area between signal beg and end\n",
    "        valid_wf = select_valid_area_no_interpolation(\n",
    "            bins_interpolated, wf_interpolated, sig_beg, sig_end\n",
    "        )\n",
    "        total_energy = valid_wf.sum()\n",
    "        energy_cumsum = valid_wf.cumsum()\n",
    "        ind25 = get_percentile_ind(0.25, total_energy, energy_cumsum)\n",
    "        ind50 = get_percentile_ind(0.50, total_energy, energy_cumsum)\n",
    "        ind75 = get_percentile_ind(0.75, total_energy, energy_cumsum)\n",
    "        h25 = sig_end - bins_interpolated[ind25]\n",
    "        h50 = sig_end - bins_interpolated[ind50]\n",
    "        h75 = sig_end - bins_interpolated[ind75]\n",
    "\n",
    "    else:\n",
    "        denoised_filtered_wf = select_valid_area(bins, denoised_filtered_wf, sig_beg, sig_end)\n",
    "        total_energy = denoised_filtered_wf.sum()\n",
    "        energy_cumsum = denoised_filtered_wf.cumsum()\n",
    "        # TODO: interpolate!!\n",
    "        ind25 = get_percentile_ind(0.25, total_energy, energy_cumsum)\n",
    "        ind50 = get_percentile_ind(0.50, total_energy, energy_cumsum)\n",
    "        ind75 = get_percentile_ind(0.75, total_energy, energy_cumsum)\n",
    "        h25 = sig_end - bins[ind25]\n",
    "        h50 = sig_end - bins[ind50]\n",
    "        h75 = sig_end - bins[ind75]\n",
    "\n",
    "    ground_h = find_ground_peak(bins, denoised_filtered_wf, sig_end)\n",
    "\n",
    "    plt.figure(figsize=(6, 10))\n",
    "    plt.scatter(raw_wf, bins, s=5, label=\"Raw\")\n",
    "\n",
    "    # plot various variables found in GLAH14\n",
    "    plt.plot(\n",
    "        [-0.05, 0.5],\n",
    "        np.array([sig_beg, sig_beg]),\n",
    "        \"r--\",\n",
    "        label=\"Signal Beginning\",\n",
    "    )\n",
    "    plt.plot([-0.05, 0.5], np.array([sig_end, sig_end]), \"g--\", label=\"Signal End\")\n",
    "\n",
    "    # plot noise mean and std from GLAH01\n",
    "    plt.plot(\n",
    "        [other_data.noise_mean, other_data.noise_mean],\n",
    "        [bins.min(), bins.max()],\n",
    "        \"0.5\",\n",
    "        label=\"Noise Mean\",\n",
    "    )\n",
    "    n_sig = 3.5\n",
    "    noise_threshold = other_data.noise_mean + n_sig * other_data.noise_sd\n",
    "    plt.plot(\n",
    "        [noise_threshold, noise_threshold],\n",
    "        [bins.min(), bins.max()],\n",
    "        color=\"0.5\",\n",
    "        linestyle=\"dashed\",\n",
    "        label=\"Noise Threshold\",\n",
    "    )\n",
    "\n",
    "    # plot filtered wf\n",
    "    plt.plot(smooth_wf(raw_wf), bins, \"k-\", label=\"Filtered Waveform\")\n",
    "\n",
    "    # plot percentile heights\n",
    "    if scipy_interpolate:\n",
    "        plt.plot(\n",
    "            [-0.05, 0.5],\n",
    "            [bins_interpolated[ind25], bins_interpolated[ind25]],\n",
    "            \"b--\",\n",
    "            label=\"H25\",\n",
    "        )\n",
    "        plt.plot(\n",
    "            [-0.05, 0.5],\n",
    "            [bins_interpolated[ind50], bins_interpolated[ind50]],\n",
    "            \"c--\",\n",
    "            label=\"H50\",\n",
    "        )\n",
    "        plt.plot(\n",
    "            [-0.05, 0.5],\n",
    "            [bins_interpolated[ind75], bins_interpolated[ind75]],\n",
    "            \"m--\",\n",
    "            label=\"H75\",\n",
    "        )\n",
    "    else:\n",
    "        plt.plot([-0.05, 0.5], [bins[ind25], bins[ind25]], \"b--\", label=\"H25\")\n",
    "        plt.plot([-0.05, 0.5], [bins[ind50], bins[ind50]], \"c--\", label=\"H50\")\n",
    "        plt.plot([-0.05, 0.5], [bins[ind75], bins[ind75]], \"m--\", label=\"H75\")\n",
    "\n",
    "    plt.plot([-0.05, 0.5], [ground_h, ground_h], \"y--\", label=\"Ground Peak\")\n",
    "\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.xlabel(\"lidar return (joules)\")\n",
    "    plt.ylabel(\"distance from satelite (m)\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_percentile_heights(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_percentile_heights(ind, scipy_interpolate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "\n",
    "1. store data in better ways\n",
    "2. find out which exact allometric equations we're going to use and which references we will be\n",
    "   using\n",
    "\n",
    "- find ecoregions or NLCD maps to classify -- how to overlap raster and point data (xarray\n",
    "  ds.sel(method = nearest))\n",
    "-\n",
    "\n",
    "4. LandSat tutorial/example from Joe\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
