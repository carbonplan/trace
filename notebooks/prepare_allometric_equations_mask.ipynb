{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fatal-platinum",
   "metadata": {},
   "source": [
    "# Make allometric equations mask\n",
    "\n",
    "Following Harris et al 2021\n",
    "([paper](https://www.nature.com/articles/s41558-020-00976-6),\n",
    "[spreadsheet](https://docs.google.com/spreadsheets/d/1Hb67l3xYCfgxKu9TnpbEfY6iQo4ISNEvQewpmYvH9yQ/edit#gid=1620488341)),\n",
    "and Farina et al\n",
    "([paper](https://docs.google.com/document/d/1qoIoYBghr7FfqZlcT8h5BGMww_Obtnrw/edit)),\n",
    "we want to use the following datasets to determine which allometric equations to\n",
    "use:\n",
    "\n",
    "- Ecoregions2017. Dinerstein, Eric, David Olson, Anup Joshi, Carly Vynne, Neil D\n",
    "  Burgess, Eric Wikramanayake, Nathan Hahn, et al. 2017. “An Ecoregion-Based\n",
    "  Approach to Protecting Half the Terrestrial Realm.” BioScience 67 (6): 534–45.\n",
    "  https://doi.org/10.1093/biosci/bix014. Retrieved from\n",
    "  https://ecoregions2017.appspot.com/ on Mar 5th, 2021.\n",
    "\n",
    "- NLCD. Retrieved from CarbonPlan data storage on GCP.\n",
    "\n",
    "- EOSD.\n",
    "\n",
    "- IGBP. Friedl, M.A., A.H. Strahler, and J. Hodges. 2010. ISLSCP II MODIS\n",
    "  (Collection 4) IGPB Land Cover, 2000-2001. In Hall, Forrest G., G. Collatz, B.\n",
    "  Meeson, S. Los, E. Brown de Colstoun, and D. Landis (eds.). ISLSCP Initiative\n",
    "  II Collection. Data set. Available on-line [http://daac.ornl.gov/] from Oak\n",
    "  Ridge National Laboratory Distributed Active Archive Center, Oak Ridge,\n",
    "  Tennessee, U.S.A. doi:10.3334/ORNLDAAC/968. Retrieved from\n",
    "  https://daac.ornl.gov/daacdata/islscp_ii/vegetation/modis_landcover_xdeg/data/.\n",
    "  Documented in\n",
    "  https://daac.ornl.gov/daacdata/islscp_ii/vegetation/modis_landcover_xdeg/comp/1_modis_landcover_doc.pdf.\n",
    "\n",
    "In this notebook, we load in each dataset, transform everything to the target\n",
    "grid, and store the output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corporate-perspective",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "social-frequency",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import fsspec\n",
    "import os\n",
    "import shutil\n",
    "import regionmask\n",
    "import rioxarray\n",
    "\n",
    "from itertools import product\n",
    "from zarr.errors import GroupNotFoundError\n",
    "\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "import carbonplan_trace.v1.utils as utils\n",
    "\n",
    "# from carbonplan_trace.v1.glas_allometric_eq import (\n",
    "#     get_lat_lon_tags_from_tile_path,\n",
    "#     parse_bounding_lat_lon_for_tile,\n",
    "# )\n",
    "\n",
    "from gcsfs import GCSFileSystem\n",
    "\n",
    "fs = GCSFileSystem(cache_timeout=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "severe-audience",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tile_in_xr(path):\n",
    "    mapper = fsspec.get_mapper(path)\n",
    "    try:\n",
    "        ds = xr.open_zarr(mapper, chunks=None)\n",
    "        ds.attrs[\"crs\"] = \"EPSG:4326\"\n",
    "\n",
    "        return ds\n",
    "    except GroupNotFoundError:\n",
    "        print(f\"{path} empty, skipping\")\n",
    "\n",
    "\n",
    "import regionmask\n",
    "\n",
    "\n",
    "def convert_gdf_into_tiles(tile_ds, gdf, value_col, value_name):\n",
    "    # get coordinates of target tile\n",
    "    lon_res = tile_ds.lon.values[1] - tile_ds.lon.values[0]\n",
    "    lon = np.arange(\n",
    "        tile_ds.lon.values[0], tile_ds.lon.values[-1] + (lon_res / 2), lon_res\n",
    "    )\n",
    "    lat_res = tile_ds.lat.values[1] - tile_ds.lat.values[0]\n",
    "    lat = np.arange(\n",
    "        tile_ds.lat.values[0], tile_ds.lat.values[-1] + (lat_res / 2), lat_res\n",
    "    )\n",
    "\n",
    "    # turn gdf into xarray\n",
    "    output_da = regionmask.mask_geopandas(\n",
    "        gdf, numbers=value_col, lon_or_obj=lon, lat=lat\n",
    "    )\n",
    "    output_da.name = value_name\n",
    "\n",
    "    return output_da\n",
    "\n",
    "\n",
    "def convert_raster_into_tiles(tile_ds, raster):\n",
    "    output = raster.rio.reproject_match(tile_ds)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "international-increase",
   "metadata": {},
   "source": [
    "# Find target tiles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpha-kuwait",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target tiles\n",
    "lat_tags = [\n",
    "    \"80N\",\n",
    "    \"70N\",\n",
    "    \"60N\",\n",
    "    \"50N\",\n",
    "    \"40N\",\n",
    "    \"30N\",\n",
    "    \"20N\",\n",
    "    \"10N\",\n",
    "    \"00N\",\n",
    "    \"10S\",\n",
    "    \"20S\",\n",
    "    \"30S\",\n",
    "    \"40S\",\n",
    "    \"50S\",\n",
    "]\n",
    "lon_tags = [f\"{n:03}W\" for n in np.arange(10, 190, 10)] + [\n",
    "    f\"{n:03}E\" for n in np.arange(0, 180, 10)\n",
    "]\n",
    "\n",
    "tile_paths = []\n",
    "for lat, lon in list(product(lat_tags, lon_tags)):\n",
    "    tile_paths.append(f\"gs://carbonplan-climatetrace/v0/tiles/{lat}_{lon}.zarr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considerable-involvement",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lat_tags) * len(lon_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manual-image",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try reading each tile and remove tile name if empty\n",
    "empty_tiles = []\n",
    "for tp in tile_paths:\n",
    "    target_tile = get_tile_in_xr(tp)\n",
    "    if not target_tile:\n",
    "        empty_tiles.append(tp)\n",
    "\n",
    "for tp in empty_tiles:\n",
    "    tile_paths.remove(tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attractive-ghana",
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crazy-sight",
   "metadata": {},
   "source": [
    "# Read in each dataset, consolidate as appropriate, then turn into an raster in the same format as target tiles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lucky-mexico",
   "metadata": {},
   "source": [
    "### Ecoregions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monthly-banana",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = \"gs://carbonplan-climatetrace/inputs/Ecoregions2017/Ecoregions2017.shp\"\n",
    "ecoregions = gpd.read_file(fp)\n",
    "ecoregions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "successful-agriculture",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecoregions.loc[ecoregions.ECO_ID == 411]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outside-chemical",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_of_mask_tiles(include=\"\"):\n",
    "    \"\"\"\n",
    "    Ecoregions mask is stored in 10 degree tiles, grab the filepaths\n",
    "    \"\"\"\n",
    "    no_data_tiles = [\"00N_070E\", \"20N_120W\", \"30N_170W\", \"40N_070W\"]\n",
    "\n",
    "    fs = GCSFileSystem(cache_timeout=0)\n",
    "    mask_folder = \"gs://carbonplan-climatetrace/intermediates/ecoregions_mask/\"\n",
    "    # fs.ls includes the parent folder itself, skip that link\n",
    "    mask_paths = [\n",
    "        tp\n",
    "        for tp in fs.ls(mask_folder)\n",
    "        if not tp.endswith(\"/\") and include in tp\n",
    "    ]\n",
    "\n",
    "    all_lat_lon_tags = [\n",
    "        utils.get_lat_lon_tags_from_tile_path(tp) for tp in mask_paths\n",
    "    ]\n",
    "\n",
    "    lat_lon_tags = []\n",
    "    for lat, lon in all_lat_lon_tags:\n",
    "        fn = f\"{lat}_{lon}\"\n",
    "        output_path = f\"gs://carbonplan-climatetrace/intermediates/biomass/{lat}_{lon}.zarr/.zmetadata\"\n",
    "        if not fs.exists(output_path) and not fn in no_data_tiles:\n",
    "            lat_lon_tags.append((lat, lon))\n",
    "\n",
    "    return lat_lon_tags\n",
    "\n",
    "\n",
    "lat_lon_tags = get_list_of_mask_tiles()\n",
    "# this should be in the order of min_lat, max_lat, min_lon, max_lon\n",
    "bounding_boxes = [\n",
    "    utils.parse_bounding_box_from_lat_lon_tags(lat, lon)\n",
    "    for lat, lon in lat_lon_tags\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "played-guess",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bounding_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marked-carol",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tp in tile_paths:\n",
    "    # use the same filename as target tiles for output\n",
    "    fn = tp.split(\"/\")[-1]\n",
    "    output_path = (\n",
    "        f\"gs://carbonplan-climatetrace/intermediates/ecoregions_mask/{fn}\"\n",
    "    )\n",
    "\n",
    "    if fs.exists(output_path):\n",
    "        print(f\"Skipping {fn}\")\n",
    "        pass\n",
    "    else:\n",
    "        print(f\"Processing {fn}\")\n",
    "        # read in the target tile\n",
    "        target_tile = get_tile_in_xr(tp)\n",
    "\n",
    "        # convert ecoregions shapefile into target tile format\n",
    "        output_da = convert_gdf_into_tiles(\n",
    "            tile_ds=target_tile,\n",
    "            gdf=ecoregions,\n",
    "            value_col=\"ECO_ID\",\n",
    "            value_name=\"ecoregion\",\n",
    "        )\n",
    "\n",
    "        # save the output\n",
    "        save_to_zarr(ds=output_da.to_dataset(), url=output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accomplished-ghana",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a tile to double check output\n",
    "ds = get_tile_in_xr(\n",
    "    f\"gs://carbonplan-scratch/trace_scratch/ecoregions_mask/50N_130W.zarr\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honest-composer",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urban-pocket",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.ecoregion.isnull().sum().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passive-columbus",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.ecoregion[::100, ::100].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "featured-environment",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functional-multiple",
   "metadata": {},
   "outputs": [],
   "source": [
    "reruns = [\"60N_010E\"]\n",
    "rerun_paths = [\n",
    "    f\"carbonplan-climatetrace/intermediates/ecoregions_mask/{fn}.zarr\"\n",
    "    for fn in reruns\n",
    "]\n",
    "\n",
    "for path in rerun_paths:\n",
    "    print(path)\n",
    "    lat, lon = utils.get_lat_lon_tags_from_tile_path(path)\n",
    "    bounding_box = utils.parse_bounding_box_from_lat_lon_tags(lat, lon)\n",
    "    min_lat, max_lat, min_lon, max_lon = bounding_box\n",
    "    tile = Polygon(\n",
    "        [\n",
    "            [min_lon, max_lat],\n",
    "            [max_lon, max_lat],\n",
    "            [max_lon, min_lat],\n",
    "            [min_lon, min_lat],\n",
    "            [min_lon, max_lat],\n",
    "        ]\n",
    "    )\n",
    "    shapes = ecoregions.loc[(ecoregions.intersects(tile))]\n",
    "\n",
    "    local_path = f\"/home/jovyan/temp/{lat}_{lon}.zarr\"\n",
    "    cloud_path = f\"gs://carbonplan-climatetrace/intermediates/ecoregions_mask/{lat}_{lon}.zarr\"\n",
    "    source_path = f\"gs://carbonplan-climatetrace/v0/tiles/{lat}_{lon}.zarr\"\n",
    "\n",
    "    eco = utils.open_zarr_file(path)\n",
    "    list_of_vars = list(eco.variables.keys())\n",
    "    null_frac = eco.ecoregion.isnull().mean().values\n",
    "    print(list_of_vars)\n",
    "    print(\"null fraction = \", null_frac)\n",
    "\n",
    "    print(\"reprocessing\")\n",
    "    target_tile = utils.open_zarr_file(source_path)\n",
    "    output_da = convert_gdf_into_tiles(\n",
    "        tile_ds=target_tile,\n",
    "        gdf=shapes.reset_index(drop=True),\n",
    "        value_col=\"ECO_ID\",\n",
    "        value_name=\"ecoregion\",\n",
    "    )\n",
    "\n",
    "    new_frac = output_da.isnull().mean().values\n",
    "    print(\"new null fraction = \", new_frac)\n",
    "\n",
    "    print(\"re saving\")\n",
    "    utils.save_to_zarr(\n",
    "        ds=output_da.to_dataset(promote_attrs=True),\n",
    "        url=local_path,\n",
    "        mode=\"w\",\n",
    "    )\n",
    "\n",
    "    time.sleep(300)\n",
    "    fs.rm(cloud_path, recursive=True)\n",
    "    time.sleep(300)\n",
    "    fs.put(local_path, cloud_path, recursive=True)\n",
    "    time.sleep(300)\n",
    "    shutil.rmtree(local_path)\n",
    "    del output_da\n",
    "    del eco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spectacular-juvenile",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reruns = [\"30N_090E\"]\n",
    "rerun_paths = [\n",
    "    f\"carbonplan-climatetrace/intermediates/ecoregions_mask/{fn}.zarr\"\n",
    "    for fn in reruns\n",
    "]\n",
    "\n",
    "mask_folder = \"gs://carbonplan-climatetrace/intermediates/ecoregions_mask/\"\n",
    "# fs.ls includes the parent folder itself, skip that link\n",
    "mask_paths = [tp for tp in fs.ls(mask_folder) if not tp.endswith(\"/\")]\n",
    "\n",
    "for path in mask_paths:\n",
    "    print(path)\n",
    "    eco = utils.open_zarr_file(path)\n",
    "    list_of_vars = list(eco.variables.keys())\n",
    "    null_frac = eco.ecoregion.isnull().mean().values\n",
    "    print(list_of_vars)\n",
    "    print(\"null fraction = \", null_frac)\n",
    "\n",
    "    if (\n",
    "        \"nlcd\" not in list_of_vars\n",
    "        and \"eosd\" not in list_of_vars\n",
    "        and null_frac > 0\n",
    "    ):\n",
    "        print(\"reprocessing\")\n",
    "        lat, lon = utils.get_lat_lon_tags_from_tile_path(path)\n",
    "        bounding_box = utils.parse_bounding_box_from_lat_lon_tags(lat, lon)\n",
    "        min_lat, max_lat, min_lon, max_lon = bounding_box\n",
    "\n",
    "        tile = Polygon(\n",
    "            [\n",
    "                [min_lon, max_lat],\n",
    "                [max_lon, max_lat],\n",
    "                [max_lon, min_lat],\n",
    "                [min_lon, min_lat],\n",
    "                [min_lon, max_lat],\n",
    "            ]\n",
    "        )\n",
    "        shapes = ecoregions.loc[(ecoregions.intersects(tile))]\n",
    "\n",
    "        output_da = convert_gdf_into_tiles(\n",
    "            tile_ds=eco,\n",
    "            gdf=shapes.reset_index(drop=True),\n",
    "            value_col=\"ECO_ID\",\n",
    "            value_name=\"ecoregion\",\n",
    "        )\n",
    "\n",
    "        local_path = f\"/home/jovyan/temp/{lat}_{lon}.zarr\"\n",
    "        cloud_path = f\"gs://carbonplan-climatetrace/intermediates/ecoregions_mask/{lat}_{lon}.zarr\"\n",
    "\n",
    "        new_frac = output_da.isnull().mean().values\n",
    "        print(\"new null fraction = \", new_frac)\n",
    "\n",
    "        if new_frac < (null_frac - 0.01):\n",
    "            print(\"re saving\")\n",
    "            utils.save_to_zarr(\n",
    "                ds=output_da.to_dataset(promote_attrs=True),\n",
    "                url=local_path,\n",
    "                mode=\"w\",\n",
    "            )\n",
    "\n",
    "            time.sleep(60)\n",
    "            fs.rm(cloud_path, recursive=True)\n",
    "            time.sleep(60)\n",
    "            fs.put(local_path, cloud_path, recursive=True)\n",
    "            time.sleep(60)\n",
    "            shutil.rmtree(local_path)\n",
    "        del output_da\n",
    "    del eco"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "japanese-uncertainty",
   "metadata": {},
   "source": [
    "### NLCD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greenhouse-partition",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlcd_conus = xr.open_rasterio(\n",
    "    \"gs://carbonplan-data/raw/nlcd/conus/30m/2001.tif\", parse_coordinates=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enclosed-alexander",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlcd_conus[::100, ::100].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "included-convergence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the bounding box of NLCD data\n",
    "\n",
    "from pyproj import Transformer\n",
    "\n",
    "transformer = Transformer.from_crs(nlcd_conus.crs, \"EPSG:4326\")\n",
    "\n",
    "lat, lon = transformer.transform(nlcd_conus.x.values[0], nlcd_conus.y.values[0])\n",
    "lat, lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bronze-myrtle",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat, lon = transformer.transform(\n",
    "    nlcd_conus.x.values[-1], nlcd_conus.y.values[-1]\n",
    ")\n",
    "lat, lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hundred-pitch",
   "metadata": {},
   "outputs": [],
   "source": [
    "lons_of_interest = [\"130W\", \"120W\", \"110W\", \"100W\", \"090W\", \"080W\"]\n",
    "lats_of_interest = [\"50N\", \"40N\", \"30N\"]\n",
    "\n",
    "for lat, lon in list(product(lats_of_interest, lons_of_interest)):\n",
    "    # use the same filename as target tiles for output\n",
    "    fn = f\"{lat}_{lon}.zarr\"\n",
    "    #     output_path = f'gs://carbonplan-scratch/trace_scratch/nlcd_cache/{fn}'\n",
    "    output_path = f\"gs://carbonplan-scratch/trace_scratch/ecoregions_mask/{fn}\"\n",
    "\n",
    "    if not fs.exists(output_path):\n",
    "        # if the target tile doesn't exist, then pass\n",
    "        print(f\"Skipping {fn}, file does not exist\")\n",
    "        pass\n",
    "\n",
    "    elif fs.exists(output_path + \"/nlcd/\"):\n",
    "        # if we have already process this tile, also pass\n",
    "        print(f\"Skipping {fn}, NLCD data already present\")\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "        # otherwise reproject the tile\n",
    "        print(f\"Processing {fn}\")\n",
    "        # read in the target tile\n",
    "        target_tile = get_tile_in_xr(output_path)\n",
    "        target_tile = target_tile.rename(lon=\"x\", lat=\"y\")\n",
    "\n",
    "        # convert NLCD raster into target tile format\n",
    "        output_da = convert_raster_into_tiles(\n",
    "            tile_ds=target_tile, raster=nlcd_conus\n",
    "        )\n",
    "        output_da = output_da.drop_vars(\"spatial_ref\")\n",
    "        output_da = output_da.squeeze(dim=\"band\", drop=True)\n",
    "        output_da.attrs = {\"crs\": \"EPSG:4326\"}\n",
    "        output_da.coords[\"x\"] = target_tile.x\n",
    "        output_da.coords[\"y\"] = target_tile.y\n",
    "\n",
    "        target_tile[\"nlcd\"] = output_da\n",
    "        target_tile = target_tile.rename(x=\"lon\", y=\"lat\")\n",
    "\n",
    "        # save the output\n",
    "        save_to_zarr(\n",
    "            ds=target_tile,\n",
    "            url=output_path,\n",
    "            list_of_variables=[\"nlcd\"],\n",
    "            mode=\"a\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generic-psychiatry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a tile to double check output\n",
    "ds = get_tile_in_xr(\n",
    "    f\"gs://carbonplan-scratch/trace_scratch/ecoregions_mask/50N_130W.zarr\"\n",
    ")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "asian-gentleman",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.nlcd[::100, ::100].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precious-irish",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(ds.nlcd.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprised-adjustment",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.nlcd.isnull().sum().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "common-approval",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.ecoregion.isnull().sum().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enabling-atlas",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.ecoregion[::100, ::100].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nuclear-writer",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlcd_ak = xr.open_rasterio(\n",
    "    \"https://storage.googleapis.com/carbonplan-data/raw/nlcd/ak/30m/2011.tif\",\n",
    "    parse_coordinates=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noticed-fireplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlcd_ak[::100, ::100].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "armed-isaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the bounding box of NLCD data\n",
    "\n",
    "from pyproj import Transformer\n",
    "\n",
    "transformer = Transformer.from_crs(nlcd_ak.crs, \"EPSG:4326\")\n",
    "\n",
    "lat, lon = transformer.transform(nlcd_ak.x.values[0], nlcd_ak.y.values[0])\n",
    "lat, lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-logistics",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat, lon = transformer.transform(nlcd_ak.x.values[-1], nlcd_ak.y.values[-1])\n",
    "lat, lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-pilot",
   "metadata": {},
   "outputs": [],
   "source": [
    "lons_of_interest = [\n",
    "    \"150E\",\n",
    "    \"160E\",\n",
    "    \"170E\",\n",
    "    \"180W\",\n",
    "    \"170W\",\n",
    "    \"160W\",\n",
    "    \"150W\",\n",
    "    \"140W\",\n",
    "]\n",
    "lats_of_interest = [\"70N\", \"60N\"]\n",
    "\n",
    "for lat, lon in list(product(lats_of_interest, lons_of_interest)):\n",
    "    # use the same filename as target tiles for output\n",
    "    fn = f\"{lat}_{lon}.zarr\"\n",
    "    output_path = f\"gs://carbonplan-scratch/trace_scratch/ecoregions_mask/{fn}\"\n",
    "\n",
    "    if not fs.exists(output_path):\n",
    "        # if the target tile doesn't exist, then pass\n",
    "        print(f\"Skipping {fn}, file does not exist\")\n",
    "        pass\n",
    "\n",
    "    elif fs.exists(output_path + \"/nlcd/\"):\n",
    "        # if we have already process this tile, also pass\n",
    "        print(f\"Skipping {fn}, NLCD data already present\")\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "        # otherwise reproject the tile\n",
    "        print(f\"Processing {fn}\")\n",
    "        # read in the target tile\n",
    "        target_tile = get_tile_in_xr(output_path)\n",
    "        target_tile = target_tile.rename(lon=\"x\", lat=\"y\")\n",
    "\n",
    "        # convert NLCD raster into target tile format\n",
    "        output_da = convert_raster_into_tiles(\n",
    "            tile_ds=target_tile, raster=nlcd_ak\n",
    "        )\n",
    "        output_da = output_da.drop_vars(\"spatial_ref\")\n",
    "        output_da = output_da.squeeze(dim=\"band\", drop=True)\n",
    "        output_da.attrs = {\"crs\": \"EPSG:4326\"}\n",
    "        output_da.coords[\"x\"] = target_tile.x\n",
    "        output_da.coords[\"y\"] = target_tile.y\n",
    "\n",
    "        target_tile[\"nlcd\"] = output_da\n",
    "        target_tile = target_tile.rename(x=\"lon\", y=\"lat\")\n",
    "\n",
    "        # save the output\n",
    "        save_to_zarr(\n",
    "            ds=target_tile,\n",
    "            url=output_path,\n",
    "            list_of_variables=[\"nlcd\"],\n",
    "            mode=\"a\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offshore-message",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a tile to double check output\n",
    "ds = get_tile_in_xr(\n",
    "    f\"gs://carbonplan-scratch/trace_scratch/ecoregions_mask/70N_140W.zarr\"\n",
    ")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wooden-engagement",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(ds.nlcd.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shared-prefix",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.nlcd[::100, ::100].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sacred-special",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.ecoregion[::100, ::100].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handmade-maine",
   "metadata": {},
   "source": [
    "### IGBP old version\n",
    "\n",
    "documentations:\n",
    "https://daac.ornl.gov/daacdata/islscp_ii/vegetation/modis_landcover_xdeg/comp/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hidden-grammar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fn = '../data/IGBP/modis_landcover_class_qd.asc'\n",
    "# # the first 6 lines are additional info not data\n",
    "# headers = 6\n",
    "# igbp = np.genfromtxt(fn, skip_header=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "female-pitch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(fn) as f:\n",
    "#     head = [next(f) for x in range(headers)]\n",
    "# print(head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unknown-saying",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from rasterio.transform import Affine\n",
    "\n",
    "# # use info in the headers\n",
    "# ncols = 1440\n",
    "# nrows = 720\n",
    "# xll = -180\n",
    "# yll = -90\n",
    "# res = .25\n",
    "\n",
    "# transform = Affine.translation(xll, yll+res*nrows) * Affine.scale(res, -res)\n",
    "# transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addressed-happiness",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import rasterio\n",
    "# import xarray as xr\n",
    "\n",
    "# fn = 'IGBP.tif'\n",
    "# local_target = f\"../data/{fn}\"\n",
    "# remote_target = f\"gs://carbonplan-scratch/trace_scratch/{fn}\"\n",
    "\n",
    "# os.remove(local_target)\n",
    "# with rasterio.open(\n",
    "#     local_target,\n",
    "#     'w',\n",
    "#     driver='GTiff',\n",
    "#     height=igbp.shape[0],\n",
    "#     width=igbp.shape[1],\n",
    "#     count=1,\n",
    "#     dtype=igbp.dtype,\n",
    "#     crs='+proj=latlong',\n",
    "#     transform=transform,\n",
    "# ) as dst:\n",
    "#     dst.write(igbp, 1)\n",
    "\n",
    "# dst.close()\n",
    "\n",
    "# fs.put_file(local_target, remote_target)\n",
    "# os.remove(local_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identified-operations",
   "metadata": {},
   "outputs": [],
   "source": [
    "# igbp = xr.open_rasterio(\n",
    "#     \"https://storage.googleapis.com/carbonplan-scratch/trace_scratch/IGBP.tif\",\n",
    "#     parse_coordinates=True,\n",
    "# )\n",
    "# igbp = igbp.squeeze(dim=\"band\", drop=True)\n",
    "# igbp = igbp.rename(x=\"lon\", y=\"lat\")\n",
    "# igbp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaged-monte",
   "metadata": {},
   "outputs": [],
   "source": [
    "# igbp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atmospheric-advantage",
   "metadata": {},
   "source": [
    "## IGBP new version\n",
    "\n",
    "https://lpdaac.usgs.gov/products/mcd12q1v006/  \n",
    "https://lpdaac.usgs.gov/documents/101/MCD12_User_Guide_V6.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formed-marine",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = \"gs://carbonplan-climatetrace/inputs/igbp/\"\n",
    "files = [f for f in fs.ls(d) if not f.endswith(\"/\") and not f.endswith(\"zarr\")]\n",
    "years = [f.split(\"/\")[-1].split(\".\")[1] for f in files]\n",
    "file_df = pd.DataFrame({\"file_path\": files, \"year\": years})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proper-newman",
   "metadata": {},
   "outputs": [],
   "source": [
    "for yr, group in file_df.groupby(\"year\"):\n",
    "    #     if '2009' in yr:\n",
    "    print(yr[1:5])\n",
    "    igbp = []\n",
    "    for i, file in group.iterrows():\n",
    "        f = xr.open_rasterio(f\"gs://{file.file_path}\").squeeze(\n",
    "            dim=\"band\", drop=True\n",
    "        )\n",
    "        igbp.append(\n",
    "            f.to_dataset(name=\"igbp\", promote_attrs=True).chunk(\n",
    "                {\"x\": 2400, \"y\": 2400}\n",
    "            )\n",
    "        )\n",
    "    igbp = xr.combine_by_coords(igbp, combine_attrs=\"drop_conflicts\")\n",
    "    attrs = igbp.attrs\n",
    "    igbp = xr.where(igbp == 255, np.nan, igbp)\n",
    "    igbp = xr.where(igbp == 17, np.nan, igbp)\n",
    "    igbp.attrs = attrs\n",
    "    mapper = fsspec.get_mapper(\n",
    "        f\"gs://carbonplan-climatetrace/inputs/igbp/{yr[1:5]}.zarr\"\n",
    "    )\n",
    "    igbp.to_zarr(mapper, mode=\"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nervous-omaha",
   "metadata": {},
   "outputs": [],
   "source": [
    "igbp.igbp[::20, ::20].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "given-sponsorship",
   "metadata": {},
   "outputs": [],
   "source": [
    "tilepaths = [\n",
    "    f\n",
    "    for f in fs.ls(\n",
    "        \"gs://carbonplan-climatetrace/intermediates/ecoregions_mask/\"\n",
    "    )\n",
    "    if not f.endswith(\"/\")\n",
    "]\n",
    "len(tilepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olympic-diameter",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for each tile\n",
    "for tp in tilepaths[4:]:\n",
    "    print(tp)\n",
    "    # load tile\n",
    "    target_tile = get_tile_in_xr(\"gs://\" + tp)\n",
    "    # preprocess\n",
    "    target_tile = target_tile.rename(lon=\"x\", lat=\"y\")\n",
    "    target_tile = target_tile.coarsen({\"x\": 10, \"y\": 10}).mean()\n",
    "    target_tile.attrs[\"crs\"] = \"EPSG:4326\"\n",
    "    # get file names\n",
    "    fn = tp.split(\"/\")[-1].split(\".\")[0]\n",
    "    local_path = f\"/home/jovyan/temp/{fn}.zarr\"\n",
    "    cloud_path = f\"gs://carbonplan-climatetrace/intermediates/igbp/{fn}.zarr\"\n",
    "    if os.path.exists(local_path):\n",
    "        shutil.rmtree(local_path)\n",
    "\n",
    "    # load igbp per year\n",
    "    for yr in np.arange(2003, 2010):\n",
    "        print(yr)\n",
    "        mapper = fsspec.get_mapper(\n",
    "            f\"gs://carbonplan-climatetrace/inputs/igbp/{yr}.zarr\"\n",
    "        )\n",
    "        igbp = xr.open_zarr(mapper)\n",
    "        attrs = igbp.attrs\n",
    "        igbp = igbp.igbp\n",
    "        igbp.attrs = attrs\n",
    "\n",
    "        # transform\n",
    "        output_da = convert_raster_into_tiles(tile_ds=target_tile, raster=igbp)\n",
    "        output_da.attrs = {\"crs\": \"EPSG:4326\"}\n",
    "        output_da.coords[\"x\"] = target_tile.x\n",
    "        output_da.coords[\"y\"] = target_tile.y\n",
    "        output_da = output_da.rename(x=\"lon\", y=\"lat\")\n",
    "        output_da = output_da.assign_coords(year=yr).expand_dims(\"year\")\n",
    "\n",
    "        if not os.path.exists(local_path):\n",
    "            output_da.to_dataset(promote_attrs=True).to_zarr(\n",
    "                local_path, mode=\"w\"\n",
    "            )\n",
    "        else:\n",
    "            output_da.to_dataset(promote_attrs=True).to_zarr(\n",
    "                local_path, append_dim=\"year\"\n",
    "            )\n",
    "\n",
    "    fs.put(local_path, cloud_path, recursive=True)\n",
    "    shutil.rmtree(local_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supreme-withdrawal",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = fsspec.get_mapper(\n",
    "    f\"gs://carbonplan-climatetrace/intermediates/igbp/50N_130W.zarr\"\n",
    ")\n",
    "test = xr.open_zarr(mapper)\n",
    "\n",
    "test.sel(year=2003).igbp[::10, ::10].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forbidden-satisfaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exclusive-pharmaceutical",
   "metadata": {},
   "source": [
    "## Burned area\n",
    "\n",
    "https://lpdaac.usgs.gov/products/mcd64a1v006/  \n",
    "https://lpdaac.usgs.gov/documents/875/MCD64_User_Guide_V6.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clear-nightmare",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = \"gs://carbonplan-climatetrace/inputs/modis_burned_area/\"\n",
    "files = [f for f in fs.ls(d) if not f.endswith(\"/\") and not f.endswith(\"zarr\")]\n",
    "reference_date = [f.split(\"/\")[-1].split(\".\")[1] for f in files]\n",
    "rowpath = [f.split(\"/\")[-1].split(\".\")[2] for f in files]\n",
    "\n",
    "file_df = pd.DataFrame(\n",
    "    {\"file_path\": files, \"reference_date\": reference_date, \"rowpath\": rowpath}\n",
    ")\n",
    "file_df[\"year\"] = file_df.reference_date.str[1:5]\n",
    "file_df[\"day\"] = file_df.reference_date.str[5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broken-worst",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adult-distance",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(file_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "angry-census",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_df.groupby(\"rowpath\").file_path.count().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revolutionary-update",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(file_df.rowpath.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brown-merchandise",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final = []\n",
    "\n",
    "for rp, group in sub.groupby(\"rowpath\"):\n",
    "    print(rp)\n",
    "    rowpath = []\n",
    "    total = 0\n",
    "    for i, file in group.iterrows():\n",
    "        f = xr.open_rasterio(f\"gs://{file.file_path}\").squeeze(\n",
    "            dim=\"band\", drop=True\n",
    "        )\n",
    "        attrs = f.attrs\n",
    "        f = xr.where(f.isin([0, -1, -2]), np.nan, f)\n",
    "        total += (f > 0).astype(int).sum().values\n",
    "        f += float(file.year) * 1000\n",
    "        f.attrs = attrs\n",
    "        rowpath.append(\n",
    "            f.assign_coords(\n",
    "                day=float(file.year) * 1000 + float(file.day)\n",
    "            ).expand_dims(\"day\")\n",
    "        )\n",
    "\n",
    "    rowpath = xr.concat(rowpath, dim=\"day\").min(dim=\"day\", keep_attrs=True)\n",
    "    print(f\"total = {total}\")\n",
    "    print(\"composite = \", (rowpath > 0).astype(int).sum().values)\n",
    "    print(\"composite = \", (rowpath > 0).astype(int).mean().values)\n",
    "\n",
    "    final.append(\n",
    "        rowpath.to_dataset(name=\"burned date\", promote_attrs=True).chunk(\n",
    "            {\"x\": 2400, \"y\": 2400}\n",
    "        )\n",
    "    )\n",
    "\n",
    "final = xr.combine_by_coords(final, combine_attrs=\"drop_conflicts\")\n",
    "mapper = fsspec.get_mapper(\n",
    "    f\"gs://carbonplan-climatetrace/inputs/modis_burned_area/composite.zarr\"\n",
    ")\n",
    "final.to_zarr(mapper, mode=\"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "backed-makeup",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = fsspec.get_mapper(\n",
    "    f\"gs://carbonplan-climatetrace/inputs/modis_burned_area/composite.zarr\"\n",
    ")\n",
    "burned = xr.open_zarr(mapper)\n",
    "burned[\"burned date\"].attrs = burned.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "searching-strategy",
   "metadata": {},
   "outputs": [],
   "source": [
    "(burned[\"burned date\"] < 2000000).sum().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjusted-hayes",
   "metadata": {},
   "outputs": [],
   "source": [
    "((burned[\"burned date\"] % 1000) == 0).astype(int).sum().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conditional-george",
   "metadata": {},
   "outputs": [],
   "source": [
    "(burned[\"burned date\"] > 0).sum().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latin-civilization",
   "metadata": {},
   "outputs": [],
   "source": [
    "burned[\"burned date\"][::20, ::20].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chinese-translator",
   "metadata": {},
   "outputs": [],
   "source": [
    "tilepaths = [\n",
    "    f\n",
    "    for f in fs.ls(\n",
    "        \"gs://carbonplan-climatetrace/intermediates/ecoregions_mask/\"\n",
    "    )\n",
    "    if not f.endswith(\"/\")\n",
    "]\n",
    "len(tilepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limited-basket",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tp in tilepaths:\n",
    "    print(tp)\n",
    "    # load tile\n",
    "    target_tile = get_tile_in_xr(\"gs://\" + tp)\n",
    "    # preprocess\n",
    "    target_tile = target_tile.rename(lon=\"x\", lat=\"y\")\n",
    "    target_tile = target_tile.coarsen({\"x\": 10, \"y\": 10}).mean()\n",
    "    target_tile.attrs[\"crs\"] = \"EPSG:4326\"\n",
    "    # get file names\n",
    "    fn = tp.split(\"/\")[-1].split(\".\")[0]\n",
    "    local_path = f\"/home/jovyan/temp/{fn}.zarr\"\n",
    "    cloud_path = f\"gs://carbonplan-climatetrace/intermediates/modis_burned_area/{fn}.zarr\"\n",
    "    if os.path.exists(local_path):\n",
    "        shutil.rmtree(local_path)\n",
    "\n",
    "    # transform\n",
    "    output_da = convert_raster_into_tiles(\n",
    "        tile_ds=target_tile, raster=burned[\"burned date\"]\n",
    "    )\n",
    "    output_da.attrs = {\"crs\": \"EPSG:4326\"}\n",
    "    output_da.coords[\"x\"] = target_tile.x\n",
    "    output_da.coords[\"y\"] = target_tile.y\n",
    "    output_da = output_da.rename(x=\"lon\", y=\"lat\")\n",
    "    output_da.name = \"burned_date\"\n",
    "\n",
    "    output_da.to_dataset(promote_attrs=True).to_zarr(local_path, mode=\"w\")\n",
    "\n",
    "    fs.put(local_path, cloud_path, recursive=True)\n",
    "    shutil.rmtree(local_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesser-apollo",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = fsspec.get_mapper(\n",
    "    f\"gs://carbonplan-climatetrace/intermediates/modis_burned_area/50N_130W.zarr\"\n",
    ")\n",
    "test = xr.open_zarr(mapper)\n",
    "\n",
    "test.burned_date[::10, ::10].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-boutique",
   "metadata": {},
   "source": [
    "### EOSD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addressed-highland",
   "metadata": {},
   "source": [
    "The following block copies the shapefiles from a ftp site.\n",
    "\n",
    "Legend can be found\n",
    "[here](https://ftp.maps.canada.ca/pub/nrcan_rncan/vector/geobase_lcc_csc/doc/GeoBase_lcc_en_Catalogue.pdf).\n",
    "[Other docs](https://ftp.maps.canada.ca/pub/nrcan_rncan/vector/geobase_lcc_csc/doc/)\n",
    "\n",
    "[Alternative source for EOSD data in tif format](http://tree.pfc.forestry.ca/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "established-girlfriend",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# import ftplib\n",
    "# import urllib.request as request\n",
    "# from contextlib import closing\n",
    "\n",
    "# # source filepaths\n",
    "# ftp_server = 'ftp.maps.canada.ca'\n",
    "# path = '/pub/nrcan_rncan/vector/geobase_lcc_csc/shp_en/'\n",
    "# # dest filepaths\n",
    "# dest_path = 'carbonplan-scratch/trace_scratch/EOSD/'\n",
    "\n",
    "# ftp = ftplib.FTP(ftp_server)\n",
    "# ftp.login()\n",
    "# ftp.cwd(path)\n",
    "# folders = ftp.nlst()\n",
    "\n",
    "# for folder in folders:\n",
    "#     fnames = ftp.nlst(folder)\n",
    "#     for fn in fnames:\n",
    "#         fp = f'ftp://{ftp_server}{path}{fn}'\n",
    "#         print(fp)\n",
    "#         with closing(request.urlopen(fp)) as r:\n",
    "#             uri = dest_path + fn\n",
    "#             with fs.open(uri, 'wb') as f:\n",
    "#                 shutil.copyfileobj(r, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "express-official",
   "metadata": {},
   "source": [
    "The following blocks sorts the EOSD raw files according to the bounding box of\n",
    "each tile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animated-johnston",
   "metadata": {},
   "outputs": [],
   "source": [
    "from carbonplan_trace.v1.glas_allometric_eq import (\n",
    "    parse_bounding_lat_lon_for_tile,\n",
    ")\n",
    "from shapely import geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fresh-visiting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of tiles of interest and their respective bounding boxes\n",
    "all_tiles = [\n",
    "    p.split(\"/\")[-1].split(\".\")[0]\n",
    "    for p in fs.ls(\"gs://carbonplan-scratch/trace_scratch/ecoregions_mask/\")\n",
    "    if not p.endswith(\"/\")\n",
    "]\n",
    "\n",
    "tile_poly = []\n",
    "for tile in all_tiles:\n",
    "    min_lat, max_lat, min_lon, max_lon = parse_bounding_lat_lon_for_tile(tile)\n",
    "    tile_poly.append(\n",
    "        geometry.Polygon(\n",
    "            [\n",
    "                [min_lon, min_lat],\n",
    "                [min_lon, max_lat],\n",
    "                [max_lon, max_lat],\n",
    "                [max_lon, min_lat],\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "tile_gdf = gpd.GeoDataFrame(\n",
    "    {\"tile_name\": all_tiles, \"geometry\": tile_poly}, crs=\"EPSG:4326\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passive-gibraltar",
   "metadata": {},
   "outputs": [],
   "source": [
    "eosd_folder = \"gs://carbonplan-scratch/trace_scratch/EOSD/\"\n",
    "eosd_subfolders = [p for p in fs.ls(eosd_folder) if not p.endswith(\"/\")]\n",
    "\n",
    "for eosd_subfolder in eosd_subfolders:\n",
    "    zip_files = fs.ls(eosd_subfolder)\n",
    "    for zip_file in zip_files:\n",
    "        fn = zip_file.split(\"/\")[-1]\n",
    "        input_file = f\"gs://{zip_file}\"\n",
    "        eosd_raw = gpd.read_file(input_file)\n",
    "        min_lon, min_lat, max_lon, max_lat = eosd_raw.total_bounds\n",
    "        eosd_poly = geometry.Polygon(\n",
    "            [\n",
    "                [min_lon, min_lat],\n",
    "                [min_lon, max_lat],\n",
    "                [max_lon, max_lat],\n",
    "                [max_lon, min_lat],\n",
    "            ]\n",
    "        )\n",
    "        # figure out which tile it belongs to\n",
    "        intersect_tiles = tile_gdf.loc[\n",
    "            tile_gdf.intersects(eosd_poly)\n",
    "        ].tile_name.values\n",
    "        for intersect_tile in intersect_tiles:\n",
    "            fs.cp(\n",
    "                f\"gs://{zip_file}\",\n",
    "                f\"gs://carbonplan-scratch/trace_scratch/EOSD_sorted/{intersect_tile}/{fn}\",\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confirmed-amber",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = fsspec.get_mapper(\n",
    "    f\"gs://carbonplan-scratch/trace_scratch/ecoregions_mask/50N_080W.zarr\"\n",
    ")\n",
    "ds = xr.open_dataset(mapper, engine=\"zarr\", cache=False)\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corresponding-letter",
   "metadata": {},
   "source": [
    "For each tile, concatenate everything and turn into raster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "undefined-candle",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_tiles = [\n",
    "    p\n",
    "    for p in fs.ls(f\"gs://carbonplan-scratch/trace_scratch/EOSD_sorted/\")\n",
    "    if not p.endswith(\"/\")\n",
    "]\n",
    "\n",
    "for tile in all_tiles:\n",
    "    fn = tile.split(\"/\")[-1]\n",
    "    tile_path = (\n",
    "        f\"gs://carbonplan-scratch/trace_scratch/ecoregions_mask/{fn}.zarr\"\n",
    "    )\n",
    "\n",
    "    if fs.exists(tile_path + \"/eosd/\"):\n",
    "        # if we have already process this tile, also pass\n",
    "        print(f\"Skipping {fn}, EOSD data already present\")\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "        print(f\"Processing {fn}\")\n",
    "        zip_files = fs.ls(tile)\n",
    "        eosd = []\n",
    "        for zf in zip_files:\n",
    "            print(f\"    reading {zf}\")\n",
    "            temp = gpd.read_file(\"gs://\" + zf)\n",
    "            print(temp.total_bounds)\n",
    "            eosd.append(temp)\n",
    "\n",
    "        print(\"concat\")\n",
    "        eosd = pd.concat(eosd, ignore_index=True)\n",
    "\n",
    "        # read in the target tile\n",
    "        target_tile = get_tile_in_xr(tile_path)\n",
    "\n",
    "        eosd = eosd[[\"COVTYPE\", \"geometry\"]]\n",
    "        eosd = eosd.sort_values(by=\"COVTYPE\").reset_index(drop=True)\n",
    "\n",
    "        print(\"convert\")\n",
    "        # convert ecoregions shapefile into target tile format\n",
    "        eosd_index_a = (\n",
    "            convert_gdf_into_tiles(\n",
    "                tile_ds=target_tile.isel(\n",
    "                    lat=slice(0, 20000), lon=slice(0, 20000)\n",
    "                ),\n",
    "                gdf=eosd,\n",
    "                value_col=None,\n",
    "                value_name=\"eosd_index\",\n",
    "            )\n",
    "            .chunk({\"lat\": 625, \"lon\": 1250})\n",
    "            .to_dataset()\n",
    "        )\n",
    "\n",
    "        eosd_index_b = (\n",
    "            convert_gdf_into_tiles(\n",
    "                tile_ds=target_tile.isel(\n",
    "                    lat=slice(20000, 40000), lon=slice(0, 20000)\n",
    "                ),\n",
    "                gdf=eosd,\n",
    "                value_col=None,\n",
    "                value_name=\"eosd_index\",\n",
    "            )\n",
    "            .chunk({\"lat\": 625, \"lon\": 1250})\n",
    "            .to_dataset()\n",
    "        )\n",
    "\n",
    "        eosd_index_c = (\n",
    "            convert_gdf_into_tiles(\n",
    "                tile_ds=target_tile.isel(\n",
    "                    lat=slice(0, 20000), lon=slice(20000, 40000)\n",
    "                ),\n",
    "                gdf=eosd,\n",
    "                value_col=None,\n",
    "                value_name=\"eosd_index\",\n",
    "            )\n",
    "            .chunk({\"lat\": 625, \"lon\": 1250})\n",
    "            .to_dataset()\n",
    "        )\n",
    "\n",
    "        eosd_index_d = (\n",
    "            convert_gdf_into_tiles(\n",
    "                tile_ds=target_tile.isel(\n",
    "                    lat=slice(20000, 40000), lon=slice(20000, 40000)\n",
    "                ),\n",
    "                gdf=eosd,\n",
    "                value_col=None,\n",
    "                value_name=\"eosd_index\",\n",
    "            )\n",
    "            .chunk({\"lat\": 625, \"lon\": 1250})\n",
    "            .to_dataset()\n",
    "        )\n",
    "\n",
    "        eosd_index = xr.combine_by_coords(\n",
    "            [eosd_index_a, eosd_index_b, eosd_index_c, eosd_index_d]\n",
    "        )[\"eosd_index\"]\n",
    "\n",
    "        print(\"nulls in total dataset\", eosd_index.isnull().sum().values)\n",
    "\n",
    "        print(\"get output dataset\")\n",
    "        eosd_cov = xr.DataArray(\n",
    "            np.nan,\n",
    "            dims=[\"lat\", \"lon\"],\n",
    "            coords=[target_tile.coords[\"lat\"], target_tile.coords[\"lon\"]],\n",
    "        ).chunk({\"lat\": 625, \"lon\": 1250})\n",
    "\n",
    "        print(\"assigning covers\")\n",
    "        covers = eosd.COVTYPE.unique()\n",
    "        for c in covers:\n",
    "            min_ind = np.where(eosd.COVTYPE == c)[0].min()\n",
    "            max_ind = np.where(eosd.COVTYPE == c)[0].max()\n",
    "            eosd_cov = xr.where(\n",
    "                ((eosd_index >= min_ind) & (eosd_index <= max_ind)),\n",
    "                x=c,\n",
    "                y=eosd_cov,\n",
    "            )\n",
    "\n",
    "        print(\"put to output ds\")\n",
    "        target_tile[\"eosd\"] = eosd_cov\n",
    "\n",
    "        print(\"saving\")\n",
    "        # save the output\n",
    "        save_to_zarr(\n",
    "            ds=target_tile, url=tile_path, list_of_variables=[\"eosd\"], mode=\"a\"\n",
    "        )\n",
    "\n",
    "        del eosd_index\n",
    "        del eosd_index_a\n",
    "        del eosd_index_b\n",
    "        del eosd_index_c\n",
    "        del eosd_index_d\n",
    "        del eosd_cov\n",
    "        del target_tile\n",
    "        del eosd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imported-depression",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = fsspec.get_mapper(\n",
    "    f\"gs://carbonplan-scratch/trace_scratch/ecoregions_mask/70N_100W.zarr\"\n",
    ")\n",
    "ds = xr.open_zarr(mapper)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ignored-jones",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.ecoregion.isnull().sum().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honest-interface",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.ecoregion[::100, ::100].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-separate",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.eosd.isnull().sum().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proprietary-colonial",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.eosd[::100, ::100].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handled-norway",
   "metadata": {},
   "source": [
    "## SRTM\n",
    "\n",
    "https://lpdaac.usgs.gov/products/srtmgl1v003/#tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alleged-ranch",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "\n",
    "def parse_bounding_lat_lon_for_srtm_tile(srtm_tile):\n",
    "    lat = srtm_tile[0:3]\n",
    "    if lat.startswith(\"N\"):\n",
    "        min_lat = int(lat[1:])\n",
    "    else:\n",
    "        min_lat = -1 * int(lat[1:])\n",
    "\n",
    "    max_lat = min_lat + 1\n",
    "\n",
    "    lon = srtm_tile[3:]\n",
    "    if lon.startswith(\"E\"):\n",
    "        min_lon = int(lon[1:])\n",
    "    else:\n",
    "        min_lon = -1 * int(lon[1:])\n",
    "\n",
    "    max_lon = min_lon + 1\n",
    "\n",
    "    return min_lat, max_lat, min_lon, max_lon\n",
    "\n",
    "\n",
    "def convert_srtm_tile_to_10_by_10_tile(srtm_file):\n",
    "    srtm_tile = srtm_file.split(\"/\")[-1].split(\".\")[0]\n",
    "    min_lat, max_lat, min_lon, max_lon = parse_bounding_lat_lon_for_srtm_tile(\n",
    "        srtm_tile\n",
    "    )\n",
    "\n",
    "    tile_lat = int(math.ceil(max_lat / 10.0)) * 10\n",
    "    if tile_lat >= 0:\n",
    "        tile_lat = str(abs(tile_lat)).zfill(2) + \"N\"\n",
    "    else:\n",
    "        tile_lat = str(abs(tile_lat)).zfill(2) + \"S\"\n",
    "\n",
    "    tile_lon = int(math.floor(min_lon / 10.0)) * 10\n",
    "    if tile_lon >= 0:\n",
    "        tile_lon = str(abs(tile_lon)).zfill(3) + \"E\"\n",
    "    else:\n",
    "        tile_lon = str(abs(tile_lon)).zfill(3) + \"W\"\n",
    "\n",
    "    return [srtm_tile, f\"{tile_lat}_{tile_lon}\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sticky-space",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = \"gs://carbonplan-climatetrace/inputs/srtm/\"\n",
    "files = [f for f in fs.ls(d) if not f.endswith(\"/\") and not f.endswith(\"zarr\")]\n",
    "srtm_tiles = [convert_srtm_tile_to_10_by_10_tile(f)[0] for f in files]\n",
    "tile_names = [convert_srtm_tile_to_10_by_10_tile(f)[1] for f in files]\n",
    "\n",
    "file_df = pd.DataFrame(\n",
    "    {\"file_path\": files, \"srtm_tile\": srtm_tiles, \"tile_name\": tile_names}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "missing-talent",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_df.loc[file_df.srtm_tile.str.startswith(\"N\")].sort_values(\n",
    "    by=\"srtm_tile\"\n",
    ").tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "south-sailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(file_df.tile_name.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedicated-relative",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(row):\n",
    "    lat, lon = get_lat_lon_tags_from_tile_path(row.tile_name)\n",
    "    min_lat, max_lat, min_lon, max_lon = parse_bounding_lat_lon_for_tile(\n",
    "        lat, lon\n",
    "    )\n",
    "\n",
    "    (\n",
    "        smin_lat,\n",
    "        smax_lat,\n",
    "        smin_lon,\n",
    "        smax_lon,\n",
    "    ) = parse_bounding_lat_lon_for_srtm_tile(row.srtm_tile)\n",
    "\n",
    "    try:\n",
    "        assert min_lat <= smin_lat <= max_lat\n",
    "        assert min_lat <= smax_lat <= max_lat\n",
    "        assert min_lon <= smin_lon <= max_lon\n",
    "        assert min_lon <= smax_lon <= max_lon\n",
    "    except:\n",
    "        print(min_lat, max_lat, min_lon, max_lon)\n",
    "        print(smin_lat, smax_lat, smin_lon, smax_lon)\n",
    "        raise Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjacent-pioneer",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in file_df.iterrows():\n",
    "    check(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "third-nepal",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for tn, group in file_df.groupby(\"tile_name\"):\n",
    "    local_path = f\"/home/jovyan/temp/{tn}.zarr\"\n",
    "    cloud_path = f\"gs://carbonplan-climatetrace/intermediates/srtm/{tn}.zarr\"\n",
    "\n",
    "    if fs.exists(cloud_path):\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "        print(f\"processing {tn}\")\n",
    "        tile = []\n",
    "        for i, file in group.iterrows():\n",
    "            f = xr.open_rasterio(f\"gs://{file.file_path}\").squeeze(\n",
    "                dim=\"band\", drop=True\n",
    "            )\n",
    "            attrs = f.attrs\n",
    "            f = xr.where(f.isin([-32768]), np.nan, f)\n",
    "            f.attrs = attrs\n",
    "            f = f.isel(x=slice(0, 3600), y=slice(0, 3600))\n",
    "            f = f.rename({\"x\": \"lon\", \"y\": \"lat\"})\n",
    "            tile.append(\n",
    "                f.to_dataset(name=\"srtm\", promote_attrs=True).chunk(\n",
    "                    {\"lon\": 1200, \"lat\": 1200}\n",
    "                )\n",
    "            )\n",
    "\n",
    "        tile = xr.combine_by_coords(tile, combine_attrs=\"drop_conflicts\").chunk(\n",
    "            {\"lon\": 1200, \"lat\": 1200}\n",
    "        )\n",
    "        tile.attrs = {\"crs\": \"EPSG:4326\"}\n",
    "\n",
    "        save_to_zarr(\n",
    "            ds=tile, url=local_path, list_of_variables=[\"srtm\"], mode=\"w\"\n",
    "        )\n",
    "\n",
    "        fs.put(local_path, cloud_path, recursive=True)\n",
    "        shutil.rmtree(local_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revised-animal",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = fsspec.get_mapper(\n",
    "    f\"gs://carbonplan-climatetrace/intermediates/srtm/50N_130W.zarr\"\n",
    ")\n",
    "test = xr.open_zarr(mapper)\n",
    "test.srtm[::10, ::10].plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
