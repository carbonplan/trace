{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Landsat Processing\n",
    "\n",
    "Created by: Oriana Chegwidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "import boto3\n",
    "from rasterio.session import AWSSession\n",
    "from s3fs import S3FileSystem\n",
    "\n",
    "aws_session = AWSSession(\n",
    "    boto3.Session(profile_name=\"default\"), requester_pays=True\n",
    ")\n",
    "fs = S3FileSystem(profile=\"default\", requester_pays=True)\n",
    "\n",
    "from osgeo.gdal import VSICurlClearCache\n",
    "\n",
    "VSICurlClearCache()\n",
    "import rasterio as rio\n",
    "import xarray as xr\n",
    "import dask\n",
    "import os\n",
    "import fsspec\n",
    "import geopandas as gpd\n",
    "import regionmask as rm\n",
    "\n",
    "# from satsearch import Search\n",
    "from matplotlib.pyplot import imshow\n",
    "from intake import open_stac_item_collection\n",
    "import numcodecs\n",
    "import numpy as np\n",
    "import rioxarray  # for the extension to load\n",
    "import matplotlib.pyplot as plt\n",
    "import utm\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import json\n",
    "import zarr\n",
    "import awswrangler as wr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dask_gateway import Gateway\n",
    "\n",
    "gateway = Gateway()\n",
    "options = gateway.cluster_options()\n",
    "options.worker_cores = 2\n",
    "options.worker_memory = 32\n",
    "options.environment = {\"AWS_REQUEST_PAYER\": \"requester\"}\n",
    "cluster = gateway.new_cluster(cluster_options=options)\n",
    "cluster.adapt(minimum=1, maximum=10)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = cluster.get_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each Landsat scene is stored in cloud optimized geotiff (COG) according to a\n",
    "verbose (but once you understand it, human readable!) naming convention. Landsat\n",
    "Collection 2 uses the same naming convention as Collection 1 which is as follows\n",
    "(lifted from their docs at\n",
    "`https://prd-wret.s3.us-west-2.amazonaws.com/assets/palladium/production/atoms/files/LSDS-1656_%20Landsat_Collection1_L1_Product_Definition-v2.pdf`\n",
    "\n",
    "`LXSS_LLLL_PPPRRR_YYYYMMDD_yyyymmdd_CC_TX` where\n",
    "\n",
    "```\n",
    "L = Landsat  (constant)\n",
    "X = Sensor  (C = OLI / TIRS, O = OLI-only, T= TIRS-only, E = ETM+, T = TM, M= MSS)\n",
    "SS = Satellite  (e.g., 04 for Landsat 4, 05 for Landsat 5, 07 for Landsat 7, etc.)\n",
    "LLLL = Processing  level  (L1TP, L1GT, L1GS)\n",
    "PPP  = WRS path\n",
    "RRR  = WRS row\n",
    "YYYYMMDD = Acquisition  Year (YYYY) / Month  (MM) / Day  (DD)\n",
    "yyyymmdd  = Processing  Year (yyyy) / Month  (mm) / Day (dd)\n",
    "CC = Collection  number  (e.g., 01, 02, etc.)\n",
    "TX= RT for Real-Time, T1 for Tier 1 (highest quality), and T2 for Tier 2\n",
    "\n",
    "```\n",
    "\n",
    "Thus, we're looking for scenes coded in the following way:\n",
    "`LE07_????_PPP_RRR_YYYMMDD_yyyymmdd_02_T1` for Landsat 7 and\n",
    "`LT05_????_PPP_RRR_YYYMMDD_yyyymmdd_02_T1` for Landsat 5 (but T1 might be wrong\n",
    "there)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are re-implementing (to the best of our abilities) the methods from Wang et\n",
    "al (in review). Jon Wang's paper said:\n",
    "\n",
    "`To extend our AGB predictions through space and time, we used time series (1984 – 2014) of 30 m surface reflectance data from the Thematic Mapper onboard Landsat 5 and the Enhanced Thematic Mapper Plus onboard Landsat 7. We used the GLAS-derived estimates of AGB as a response variable and the mean growing season (June, July, August) and non-growing season values for each of Landsat’s six spectral reflectance bands as the predictors in an ensemble machine learning model`\n",
    "\n",
    "So we'll be looking for:\n",
    "\n",
    "- Landsat 5 (Thematic mapper) and 7 (Enhanced Thematic Mapper Plus)\n",
    "- Growing season (June-August) and non-growing season (Sept-May) averages at an\n",
    "  annual timestep. <--- will need to figure out around the calendar whether we\n",
    "  want consecutive\n",
    "- All six spectral reflectance bands\n",
    "- We'll do a quality thresholding of cloudless cover for now based upon their\n",
    "  thresholding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In orienting myeslf, these are the potential collection options I've figured out\n",
    "(by poking around here on the\n",
    "[sat-api catalog](https://landsatlook.usgs.gov/sat-api/collections):\n",
    "\n",
    "- `landsat-c2l2-sr` Landsat Collection 2 Level-2 UTM Surface Reflectance (SR)\n",
    "  Product\n",
    "- `landsat-c2l2alb-sr` Landsat Collection 2 Level-2 Albers Surface Reflectance\n",
    "  (SR) Product\n",
    "- `landsat-c1l2alb-sr` Landsat Collection 1 Level-2 Albers Surface Reflectance\n",
    "  (SR) Product <-- we don't want this one (b/c we'll go with collection 2)\n",
    "- `landsat-c2l1` Landsat Collection 2 Level-1 Product <-- don't think we want\n",
    "  this because we want surface reflectance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this once to apply the aws session to the rasterio environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_credentials(\n",
    "    aws_session,\n",
    "    canary_file=\"s3://usgs-landsat/collection02/level-2/standard/\"\n",
    "    + \"tm/2003/044/029/LT05_L2SP_044029_20030827_20200904_02_T1/\"\n",
    "    + \"LT05_L2SP_044029_20030827_20200904_02_T1_SR_B2.TIF\",\n",
    "):\n",
    "    #     VSICurlClearCache()\n",
    "    # this file is the canary in the coal mine\n",
    "    # if you can't open this one you've got *issues* because it exists!\n",
    "    # also the instantiation of the environment here\n",
    "    # might help you turn on the switch of the credentials\n",
    "    # but maybe that's just anecdotal i hate credential stuff SO MUCH\n",
    "    # if anyone is reading this message i hope you're enjoying my typing\n",
    "    # as i wait for my cluster to start up.... hmm....\n",
    "\n",
    "    with rio.Env(aws_session):\n",
    "        with rio.open(canary_file) as src:\n",
    "            profile = src.profile\n",
    "\n",
    "            arr = src.read(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_link(url):\n",
    "    return url.replace(\"https://landsatlook.usgs.gov/data\", \"s3://usgs-landsat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different kinds of QA/QC bands contained in L2SP:\n",
    "\n",
    "- SR_CLOUD_QA - I think we want this one because anything less than 2 is either\n",
    "  just dark dense vegetation or no flags. everything above is stuff like water,\n",
    "  snow, cloud (different levels of obscurity). This is the result of the fmask\n",
    "  algorithm from Zhu et al.\n",
    "- QA_PIXEL - this gets a little more specific and goes intot different kinds of\n",
    "  clouds. Super interesting but I don't think we want to use it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull in the SR_CLOUD_QA and use as a mask - see Table 5-3 in\n",
    "https://prd-wret.s3.us-west-2.amazonaws.com/assets/palladium/production/atoms/files/LSDS-1370_L4-7_C1-SurfaceReflectance-LEDAPS_ProductGuide-v3.pdf\n",
    "for description of cloud integer values to select which ones to use as drop. For\n",
    "now I'll drop anything greater than 1 (0= no QA concerns and 1 is Dark dense\n",
    "vegetation (DDV)).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cloud_qa(item):\n",
    "    if type(item) == str:\n",
    "        qa_path = item\n",
    "    else:\n",
    "        qa_path = fix_link(item._stac_obj.assets[\"SR_CLOUD_QA.TIF\"][\"href\"])\n",
    "    cog_mask = xr.open_rasterio(qa_path).squeeze().drop(\"band\")\n",
    "    return cog_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we make the query using sat-search to find every file in the STAC catalog\n",
    "that we want. We'll store that list of files. We'll do this first for a single\n",
    "tile (in this first exmaple just covering Washington State) but then we'll loop\n",
    "through in 1-degree by 1-degree tiles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rio.Env(aws_session):\n",
    "    test_ds = rioxarray.open_rasterio(\n",
    "        \"s3://usgs-landsat/collection02/level-2/standard/\"\n",
    "        + \"tm/2003/044/029/LT05_L2SP_044029_20030827_20200904_02_T1/\"\n",
    "        + \"LT05_L2SP_044029_20030827_20200904_02_T1_SR_B4.TIF\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xr.plot.imshow(test_ds.sel(band=1) * 0.0000275 - 0.2, vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.ls(\"s3://usgs-landsat/collection02/level-2/standard/etm/2021/044/029\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to memory constraints we'll average repeated captures of the same scene.\n",
    "Then we'll average all of those averaged scenes together to create the full\n",
    "mesh. As of now we're just doing a straight average but ideally we would carry\n",
    "the weights of the number of repeats of each scene and do a weighted average\n",
    "when quilting the scenes together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_ds(item, bands_of_interest, cog_mask, utm_zone):\n",
    "    if type(item) == str:\n",
    "        url_list = [item + \"_{}.TIF\".format(band) for band in bands_of_interest]\n",
    "    else:\n",
    "        url_list = [\n",
    "            fix_link(item._stac_obj.assets[\"{}.TIF\".format(band)][\"href\"])\n",
    "            for band in bands_of_interest\n",
    "        ]\n",
    "    da_list = []\n",
    "    for url in url_list:\n",
    "        da_list.append(\n",
    "            rioxarray.open_rasterio(url, chunks={\"x\": 1024, \"y\": 1024})\n",
    "        )  # .load())\n",
    "\n",
    "    # combine into one dataset\n",
    "    ds = (\n",
    "        xr.concat(da_list, dim=\"band\")\n",
    "        .to_dataset(dim=\"band\")\n",
    "        .rename({1: \"reflectance\"})\n",
    "    )\n",
    "    ds = ds.assign_coords({\"band\": bands_of_interest})\n",
    "    # fill value is 0; let's switch it to nan\n",
    "    ds = ds.where(ds != 0)\n",
    "    ds = ds.where(cog_mask < 2)  # .compute()\n",
    "    ds.attrs[\"utm_zone\"] = utm_zone\n",
    "    #     ds['reflectance'] = ds['reflectance'].astype('int16')\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_stack_of_scenes(ds_list):\n",
    "    \"\"\"\n",
    "    Average across scenes. This will work the same regardless\n",
    "    of whether your scenes are perfectly overlapping or they're offset.\n",
    "    However, if they're offset it requires a merge and so the entire\n",
    "    datacube (pre-collapsing) will be instantiated and might make\n",
    "    your kernel explode.\n",
    "    \"\"\"\n",
    "    utm_zone = []\n",
    "    for ds in ds_list:\n",
    "        utm_zone.append(ds.attrs[\"utm_zone\"])\n",
    "    if len(set(utm_zone)) > 1:\n",
    "        print(\n",
    "            \"WATCH OUT: youre averaging scenes from multiple utm projections!!\"\n",
    "        )\n",
    "\n",
    "    full_ds = xr.concat(ds_list, dim=\"scene\").mean(dim=\"scene\")  # .compute()\n",
    "    full_ds.attrs[\"utm_zone\"] = utm_zone[0]\n",
    "    #     full_ds = full_ds.chunk({'band': 1, 'x': 256, 'y': 256})\n",
    "    return full_ds  # .compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_out(ds, mapper, aws_session):\n",
    "    encoding = {\"reflectance\": {\"compressor\": numcodecs.Blosc()}}\n",
    "    #     with rio.Env(aws_session):\n",
    "    ds.to_zarr(store=mapper, encoding=encoding, mode=\"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def access_credentials():\n",
    "    with open(\"/home/jovyan/.aws/credentials\") as f:\n",
    "        credentials = f.read().splitlines()\n",
    "        access_key_id = credentials[1].split(\"=\")[1]\n",
    "        secret_access_key = credentials[2].split(\"=\")[1]\n",
    "    return access_key_id, secret_access_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scene_utm_zone(url):\n",
    "    metadata_url = url + \"_MTL.json\"\n",
    "    json_client = boto3.client(\"s3\")\n",
    "    data = json_client.get_object(\n",
    "        Bucket=\"usgs-landsat\", Key=metadata_url[18:], RequestPayer=\"requester\"\n",
    "    )\n",
    "    metadata = json.loads(data[\"Body\"].read())\n",
    "    utm_zone = metadata[\"LANDSAT_METADATA_FILE\"][\"PROJECTION_ATTRIBUTES\"][\n",
    "        \"UTM_ZONE\"\n",
    "    ]\n",
    "    return utm_zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @dask.delayed\n",
    "def scene_seasonal_average(\n",
    "    path,\n",
    "    row,\n",
    "    year,\n",
    "    bucket,\n",
    "    access_key_id,\n",
    "    secret_access_key,\n",
    "    bands_of_interest=\"all\",\n",
    "    season=\"JJA\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Given location/time specifications will grab all valid scenes,\n",
    "    mask each according to its time-specific cloud QA and then\n",
    "    return average across all masked scenes\n",
    "    \"\"\"\n",
    "    aws_session = AWSSession(\n",
    "        boto3.Session(\n",
    "            aws_access_key_id=access_key_id,\n",
    "            aws_secret_access_key=secret_access_key,\n",
    "        ),\n",
    "        requester_pays=True,\n",
    "    )  # profile_name='default'), requester_pays=True)\n",
    "    fs = S3FileSystem(\n",
    "        key=access_key_id, secret=secret_access_key, requester_pays=True\n",
    "    )\n",
    "\n",
    "    with dask.config.set(\n",
    "        scheduler=\"single-threaded\"\n",
    "    ):  # this? **** #threads #single-threaded # threads??\n",
    "        with rio.Env(aws_session):\n",
    "            test_credentials(aws_session)\n",
    "            #             print('it works!')\n",
    "\n",
    "            # set where you'll save the final seasonal average\n",
    "            url = f\"{bucket}{path}/{row}/{year}/{season}_reflectance.zarr\"\n",
    "            mapper = fs.get_mapper(url)  # used to be fsspec\n",
    "            # all of this is just to get the right formatting stuff to access the scenes\n",
    "\n",
    "            landsat_bucket = \"s3://usgs-landsat/collection02/level-2/standard/tm/{}/{:03d}/{:03d}/\"\n",
    "            month_keys = {\"JJA\": [\"06\", \"07\", \"08\"]}\n",
    "            valid_files, ds_list = [], []\n",
    "\n",
    "            if bands_of_interest == \"all\":\n",
    "                bands_of_interest = [\n",
    "                    \"SR_B1\",\n",
    "                    \"SR_B2\",\n",
    "                    \"SR_B3\",\n",
    "                    \"SR_B4\",\n",
    "                    \"SR_B5\",\n",
    "                    \"SR_B7\",\n",
    "                ]\n",
    "            scene_stores = fs.ls(landsat_bucket.format(year, path, row))\n",
    "            summer_datestamps = [\n",
    "                \"{}{}\".format(year, month) for month in month_keys[season]\n",
    "            ]\n",
    "            for scene_store in scene_stores:\n",
    "                for summer_datestamp in summer_datestamps:\n",
    "                    if summer_datestamp in scene_store:\n",
    "                        valid_files.append(scene_store)\n",
    "            for file in valid_files:\n",
    "                scene_id = file[-40:]\n",
    "                url = \"s3://{}/{}\".format(file, scene_id)\n",
    "                utm_zone = get_scene_utm_zone(url)\n",
    "                cloud_mask_url = url + \"_SR_CLOUD_QA.TIF\"\n",
    "                cog_mask = cloud_qa(cloud_mask_url)\n",
    "                ds_list.append(\n",
    "                    grab_ds(url, bands_of_interest, cog_mask, utm_zone)\n",
    "                )\n",
    "            seasonal_average = average_stack_of_scenes(ds_list)\n",
    "\n",
    "            write_out(\n",
    "                seasonal_average.chunk({\"band\": 6, \"x\": 1024, \"y\": 1024}),\n",
    "                mapper,\n",
    "                aws_session,\n",
    "            )\n",
    "            return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_key_id, secret_access_key = access_credentials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this is a good test on one cell to see it's working\n",
    "url = \"s3://carbonplan-climatetrace/v1/\"\n",
    "ds = scene_seasonal_average(\n",
    "    46, 27, 2005, url, access_key_id, secret_access_key\n",
    ")  # , aws_session)#.chunk({'band': 6, 'x': 1024, 'y': 1024})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landsat_bucket = (\n",
    "    \"s3://usgs-landsat/collection02/level-2/standard/etm/{}/{:03d}/{:03d}/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2003\n",
    "path = 178\n",
    "row = 61\n",
    "fs.ls(landsat_bucket.format(year, row, path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "for year in np.arange(2000, 2022):\n",
    "    files += fs.ls(\n",
    "        \"s3://usgs-landsat/collection02/level-2/standard/etm/{}/178/061\".format(\n",
    "            year\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = fs.get_mapper(\n",
    "    \"s3://carbonplan-climatetrace/v1/46/27/2005/JJA_reflectance.zarr\"\n",
    ")\n",
    "test_ds = xr.open_zarr(mapper).isel(x=slice(30, 50), y=slice(30, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds.utm_zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in tasks:\n",
    "    task.cancel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we take the list of files for a given year to average across growing season\n",
    "for each of the tiles and write it out to a mapper with those specifications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_landsat_utm_zone(scene_gdf):\n",
    "    \"\"\"\n",
    "    Grab sample file for each landsat scene and\n",
    "    extract the utm zone then add that to the\n",
    "    scene geodataframe\n",
    "    \"\"\"\n",
    "    scene_gdf[\"landsat_utm_zone\"] = \"\"\n",
    "    for scene_id in scene_gdf.index:\n",
    "        scene_row = washington_scenes.loc[scene_id][\"ROW\"]\n",
    "        scene_path = washington_scenes.loc[scene_id][\"PATH\"]\n",
    "        url = \"s3://carbonplan-climatetrace/v1/{}/{}/2004/JJA_reflectance.zarr\".format(\n",
    "            scene_path, scene_row\n",
    "        )\n",
    "        landsat_utm_zone = xr.open_zarr(fs.get_mapper(url)).utm_zone\n",
    "        washington_scenes.loc[scene_id, \"landsat_utm_zone\"] = int(\n",
    "            landsat_utm_zone\n",
    "        )\n",
    "    return scene_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.config.set({\"array.slicing.split_large_chunks\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file(\n",
    "    \"https://prd-wret.s3-us-west-2.amazonaws.com/assets/\"\n",
    "    \"palladium/production/s3fs-public/atoms/files/\"\n",
    "    \"WRS2_descending_0.zip\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congo = gdf.cx[20:21, -1:-0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "washington_scenes = gdf.cx[-125:-115, 45:49]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = \"s3://carbonplan-climatetrace/v1/\"\n",
    "# PANGEO_SCRATCH=os.environ['PANGEO_SCRATCH_PREFIX']+'/orianac/'\n",
    "tasks = []\n",
    "rerun = False\n",
    "if rerun:\n",
    "    with rio.Env(aws_session):  # delete\n",
    "        #         for every year in GLAS record\n",
    "        for year in np.arange(2003, 2009):\n",
    "            # for every row path in the domain\n",
    "            for [path, row] in washington_scenes[[\"PATH\", \"ROW\"]].values:\n",
    "                for season in [\"JJA\"]:\n",
    "                    tasks.append(\n",
    "                        client.compute(\n",
    "                            scene_seasonal_average(\n",
    "                                path,\n",
    "                                row,\n",
    "                                year,\n",
    "                                bucket,\n",
    "                                access_key_id,\n",
    "                                secret_access_key,\n",
    "                                bands_of_interest=\"all\",\n",
    "                                season=season,\n",
    "                            ),\n",
    "                            retries=4,\n",
    "                        )\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "washington_scenes = add_landsat_utm_zone(washington_scenes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for year in np.arange(2003, 2009):\n",
    "    i = 0\n",
    "    # for every row path in the domain\n",
    "    for [path, row] in washington_row_paths:\n",
    "        for season in [\"JJA\"]:\n",
    "            url = f\"{bucket}{path}/{row}/{year}/{season}_reflectance.zarr\"\n",
    "            if fs.exists(url):\n",
    "                print(url)\n",
    "            break\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that you've inspected the landsat scenes,make sure you have the correct utm zones for each row/path combo, because sometimes they use one that is not what you would expect. For now we'll just do it once, but should probs check that it doesn't change year-to-year\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "\n",
    "- ~fix why the attrs aren't working in zarr~\n",
    "- ~rerun all of the washington state scenes~\n",
    "\n",
    "THEN\n",
    "\n",
    "- ~add column for utm in the scene shapefile~\n",
    "- ~loop through all scenes, grab utm from sample file and fill into df~\n",
    "- ~then use that column to force the projection~\n",
    "- rerun the projections and create biomass dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's link with GLAS. We'll loop through every 10x10 degree GLAS tile and repeat this process\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### copy over single biomass tile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "copy = False\n",
    "if copy:\n",
    "    source_mapper = fsspec.get_mapper(\n",
    "        \"gs://carbonplan-climatetrace/intermediates/biomass/50N_120W.zarr\"\n",
    "    )\n",
    "    # url = os.environ['PANGEO_SCRATCH_PREFIX']+'/orianac/'\n",
    "    url = \"s3://carbonplan-climatetrace/v1/\"\n",
    "    dest_mapper = fs.get_mapper(\n",
    "        \"carbonplan-climatetrace/v1/biomass/50N_120W.zarr\"\n",
    "    )\n",
    "    # fsspec.get_mapper(url+'biomass/50N_120W.zarr', storage_options={'profile': 'default'})\n",
    "    ds = xr.open_zarr(source_mapper)\n",
    "    ds = ds.drop(\"allometric_eq\")\n",
    "    with dask.config.set(\n",
    "        scheduler=\"threads\"\n",
    "    ):  # this? **** #threads #single-threaded\n",
    "        ds.to_zarr(dest_mapper, mode=\"w\", consolidated=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in the biomass dataset for this one tile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ul_lat, ul_lon = 50, 120\n",
    "file_mapper = fs.get_mapper(\n",
    "    \"carbonplan-climatetrace/v1/biomass/{}N_{}W.zarr\".format(ul_lat, ul_lon)\n",
    ")\n",
    "# biomass = xr.open_zarr(PANGEO_SCRATCH+'biomass/{}N_{}W.zarr'.format(ul_lat, ul_lon), consolidated=True).load()\n",
    "\n",
    "biomass = (\n",
    "    xr.open_zarr(file_mapper, consolidated=True).load().drop(\"spatial_ref\")\n",
    ")\n",
    "biomass_df = (\n",
    "    biomass.stack(unique_index=(\"record_index\", \"shot_number\"))\n",
    "    .to_dataframe()\n",
    "    .dropna(how=\"all\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert it to a geodataframe and attach the landsat row/col info to it. This will form the mapping from IceSAT to Landsat.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biomass_gdf = gpd.GeoDataFrame(\n",
    "    biomass_df, geometry=gpd.points_from_xy(biomass_df.lon, biomass_df.lat)\n",
    ").set_crs(\"EPSG:4326\")\n",
    "linked_gdf = gpd.sjoin(\n",
    "    biomass_gdf,\n",
    "    washington_scenes,  # gdf.cx[-125:-115,45:49], # gdf.cx[-ul_lon:-ul_lon+10,ul_lat-10:ul_lat],\n",
    "    how=\"inner\",\n",
    ")  # 'left' # by selecting inner you're grabbing the intersection (so dropping any shots\n",
    "# that don't have scenes or scenes that don't have shots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now you have the row and path for each shot. Let's now get the url to the appropriate COG. For this we'll need the\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_utm(df):\n",
    "    return utm.from_latlon(\n",
    "        df[\"lat\"], df[\"lon\"], force_zone_number=df[\"landsat_utm_zone\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_projection_info(df):\n",
    "    projection_info = df.apply(convert_to_utm, axis=1).to_list()\n",
    "    projected_column_names = [\"proj_x\", \"proj_y\", \"utm_zone\", \"utm_letter\"]\n",
    "    projection_df = pd.DataFrame(\n",
    "        projection_info, columns=projected_column_names, index=df.index\n",
    "    )\n",
    "    updated_df = pd.concat([df, projection_df], axis=1)\n",
    "    return updated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_year(df):\n",
    "    return datetime.fromtimestamp(df[\"time\"]).year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_linking(df):\n",
    "    df = add_projection_info(df)\n",
    "    df[\"year\"] = df.apply(grab_year, axis=1)\n",
    "    df[\"url\"] = df.apply(build_url, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_url(df):\n",
    "    return (\n",
    "        \"s3://carbonplan-climatetrace/v1/{}/{}/{}/JJA_reflectance.zarr\".format(\n",
    "            df[\"PATH\"], df[\"ROW\"], df[\"year\"]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linked_gdf = all_linking(linked_gdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do all unique row/path/year combos at same time since they'll be grabbing the\n",
    "same file. Open the file and loop through each x/y location to get reflectance\n",
    "values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_urls = linked_gdf[\"url\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_combined_landsat_biomass_df(\n",
    "    landsat_ds,\n",
    "    biomass_df,\n",
    "    biomass_variables=[\"biomass\", \"glas_elev\", \"ecoregion\"],\n",
    "):\n",
    "    selected_landsat = (\n",
    "        landsat_ds.sel(\n",
    "            x=xr.DataArray(biomass_df[\"proj_x\"].values, dims=\"shot\"),\n",
    "            y=xr.DataArray(biomass_df[\"proj_y\"].values, dims=\"shot\"),\n",
    "            method=\"nearest\",\n",
    "        )\n",
    "        .to_dataframe()\n",
    "        .drop([\"x\", \"y\"], axis=1)\n",
    "    )\n",
    "    selected_landsat.index = biomass_df.index\n",
    "    out_df = pd.concat(\n",
    "        [biomass_df[biomass_variables], selected_landsat], axis=1\n",
    "    )\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "washington_row_path_strings = [\n",
    "    \"{}/{}\".format(row, path) for (row, path) in washington_row_paths\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unique_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wr.s3.to_parquet(\n",
    "    df=washington_df,\n",
    "    index=True,\n",
    "    path=\"s3://carbonplan-climatetrace/v1/washington_biomass_landsat.parquet\",\n",
    "    boto3_session=boto3.Session(\n",
    "        aws_access_key_id=access_key_id, aws_secret_access_key=secret_access_key\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "washington_dfs = []\n",
    "# group the biomass shots according to the scene they overlap\n",
    "groupby = linked_gdf.groupby(\"url\")\n",
    "for year in np.arange(2003, 2009):\n",
    "    for (i, (path, row)) in washington_scenes[[\"PATH\", \"ROW\"]].iterrows():\n",
    "        for season in [\"JJA\"]:\n",
    "            url = f\"{bucket}{path}/{row}/{year}/{season}_reflectance.zarr\"\n",
    "            try:\n",
    "                shots_one_scene = groupby.get_group(url)\n",
    "                ds = (\n",
    "                    xr.open_zarr(fs.get_mapper(url))\n",
    "                    .drop(\"spatial_ref\")\n",
    "                    .reflectance.to_dataset(dim=\"band\")\n",
    "                )\n",
    "                washington_dfs.append(\n",
    "                    create_combined_landsat_biomass_df(ds, shots_one_scene)\n",
    "                )\n",
    "            except:\n",
    "                print(\"{} had no shots\".format(url))\n",
    "washington_df = pd.concat(washington_dfs)\n",
    "wr.s3.to_parquet(\n",
    "    df=washington_df,\n",
    "    index=True,\n",
    "    path=\"s3://carbonplan-climatetrace/v1/washington_biomass_landsat.parquet\",\n",
    "    boto3_session=boto3.Session(\n",
    "        aws_access_key_id=access_key_id, aws_secret_access_key=secret_access_key\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "washington_df[\"SR_B4\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rio.Env(aws_session):\n",
    "    test_ds = rioxarray.open_rasterio(\n",
    "        \"s3://usgs-landsat/collection02/level-2/standard/\"\n",
    "        + \"tm/2003/045/028/LT05_L2SP_045028_20030106_20200905_02_T1/\"\n",
    "        + \"LT05_L2SP_045028_20030106_20200905_02_T1_SR_B2.TIF\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds.isel(band=0).x.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landsat_bucket = (\n",
    "    \"s3://usgs-landsat/collection02/level-2/standard/tm/{}/{:03d}/{:03d}/\"\n",
    ")\n",
    "fs.ls(landsat_bucket.format(2003, 45, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client(\"s3\")\n",
    "object = s3.get_object(Bucket=\"mytestbucket\", Key=\"EmpId007\")\n",
    "serializedObject = object[\"Body\"].read()\n",
    "\n",
    "myData = json.loads(serializedObject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_url = (\n",
    "    \"s3://usgs-landsat/collection02/level-2/standard/tm/2003/045/028/\"\n",
    "    \"LT05_L2SP_045028_20030106_20200905_02_T1/LT05_L2SP_045028_20030106_20200905_02_T1_MTL.json\"\n",
    ")\n",
    "fs.from_json(sample_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_url = (\n",
    "    \"s3://usgs-landsat/collection02/level-2/standard/tm/2003/045/028/\"\n",
    "    \"LT05_L2SP_045028_20030106_20200905_02_T1/LT05_L2SP_045028_20030106_20200905_02_T1_MTL.txt\"\n",
    ")\n",
    "\n",
    "with fs.from_json(sample_url, \"rb\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf[gdf[\"PATH\"] == 45][gdf[\"ROW\"] == 28].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.SR_B1.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_url = 's3://carbonplan-climatetrace/v1/44/29/2003/JJA_reflectance.zarr'\n",
    "ds = xr.open_zarr(fs.get_mapper(sample_url)).drop('spatial_ref').reflectance.to_dataset(dim='band')\n",
    "ds = ds.\n",
    "# ds['SR_B3'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfasdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_landsat = (\n",
    "    ds.sel(\n",
    "        x=xr.DataArray(shots_one_scene[\"proj_x\"].values, dims=\"shot\"),\n",
    "        y=xr.DataArray(shots_one_scene[\"proj_y\"].values, dims=\"shot\"),\n",
    "        method=\"nearest\",\n",
    "    )\n",
    "    .to_dataframe()\n",
    "    .drop([\"x\", \"y\"], axis=1)\n",
    ")\n",
    "selected_landsat.index = shots_one_scene.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([shots_one_scene, selected_landsat], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_landsat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ideally i could just select out all of the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.sel_points(x=, y=4856174.32039988, method='nearest').to_dataframe()['reflectance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_strings = []\n",
    "for string in unique_urls:\n",
    "    if \"44/29\" in string:\n",
    "        test_strings.append(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in test_strings:\n",
    "    try:\n",
    "        ds = xr.open_zarr(fs.get_mapper(url))\n",
    "        print(url)\n",
    "        break\n",
    "    except:\n",
    "        print(\"nope\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf[gdf[\"PATH\"] == 44][gdf[\"ROW\"] == 29].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the landsat datasets aren't lat lon- they're spatial- so we need to index in and find the closest landsat pixel (or pixels?) to the lat/lon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Landsat is in meters with the utm projection of that path/row (need to check for\n",
    "weirdness at utm boundaries). So, if we give it meters locations we can get back\n",
    "the values from dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 376435\n",
    "y = 5409203\n",
    "ds.sel(x=x, y=y, method=\"nearest\").reflectance.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we need to do now is make a column of projected x/y in the dataframe\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
