{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"100\" src=\"https://carbonplan-assets.s3.amazonaws.com/monogram/dark-small.png\" style=\"margin-left:0px;margin-top:20px\"/>\n",
    "\n",
    "# Forest Emissions Tracking - Phase I\n",
    "\n",
    "_by Joe Hamman and Jeremy Freeman (CarbonPlan)_\n",
    "\n",
    "March 29, 2020\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In general, greenhouse gasses (GHGs) arising from forest land use changes can be attributed to both\n",
    "natural factors (e.g. wildfire) and human activities (e.g. deforestation). Our approach is to build\n",
    "upon an existing body of research that has provided high-resolution satellite-based estimates of\n",
    "aboveground biomass (Spawn et al., 2020), forest cover change (Hansen et al., 2013), and change\n",
    "attribution (Curtis et al., 2018). While many of the necessary data products already exist, we can\n",
    "integrate, extend, or update these resources to provide global, current estimates that can be\n",
    "integrated with the other resources produced by the coalition.\n",
    "\n",
    "Specifically, for any given spatial extent and time duration ($t1$ to $t2$), we can use three\n",
    "quantities — existing biomass, forest cover change, and change attribution — to estimate the\n",
    "effective GHG emissions from land use changes. The simplest estimate is:\n",
    "\n",
    "$\\Delta Biomass (t) = TotalBiomass (t) * \\Delta ForestCover (\\%)$\n",
    "\n",
    "$Emissions (tCO_2) = \\Delta Biomass (t) * 0.5 (tC/t) * 3.67 (tC02 / tC)$\n",
    "\n",
    "where $\\Delta ForestCover$ is the fraction of pixels within the given spatial extent that\n",
    "experienced a stand-replacement disturbance between $t1$ and $t2$. The $TotalBiomass$ is estimated\n",
    "as the aboveground biomass at time $t1$. This estimate can be further refined by attributing, for\n",
    "each pixel, the source of forest cover loss (e.g. wildfire, deforestation, etc.), and using those\n",
    "sources to express emissions fractionally and/or exclude certain categories from total estimates\n",
    "(e.g. rotational clear-cutting within tree plantations). Pixel-wise estimates can then be aggregated\n",
    "into province and country-wide estimates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "To begin, we'll import a handful of Python libraries and set a few constants.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# import dask\n",
    "import intake\n",
    "import xarray as xr\n",
    "from tqdm import tqdm\n",
    "import numcodecs\n",
    "import numpy as np\n",
    "\n",
    "TC02_PER_TC = 3.67\n",
    "TC_PER_TBM = 0.5\n",
    "SQM_PER_HA = 10000\n",
    "ORNL_SCALING = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_gateway import Gateway\n",
    "\n",
    "gateway = Gateway()\n",
    "options = gateway.cluster_options()\n",
    "options.worker_cores = 2\n",
    "options.worker_memory = 24\n",
    "cluster = gateway.new_cluster(cluster_options=options)\n",
    "cluster.adapt(minimum=1, maximum=300)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = cluster.get_client()\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data catalog\n",
    "cat = intake.open_catalog(\n",
    "    \"https://raw.githubusercontent.com/carbonplan/forest-emissions-tracking/master/catalog.yaml\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fsspec\n",
    "\n",
    "with fsspec.open(\n",
    "    \"https://storage.googleapis.com/earthenginepartners-hansen/GFC-2018-v1.6/treecover2000.txt\"\n",
    ") as f:\n",
    "    lines = f.read().decode().splitlines()\n",
    "print(len(lines))\n",
    "\n",
    "# americas tiles\n",
    "lat_tags = [\n",
    "    \"80N\",\n",
    "    \"70N\",\n",
    "    \"60N\",\n",
    "    \"50N\",\n",
    "    \"40N\",\n",
    "    \"30N\",\n",
    "    \"20N\",\n",
    "    \"10N\",\n",
    "    \"00N\",\n",
    "    \"10S\",\n",
    "    \"20S\",\n",
    "    \"30S\",\n",
    "    \"40S\",\n",
    "    \"50S\",\n",
    "]\n",
    "lon_tags = [f\"{n:03}W\" for n in np.arange(10, 190, 10)] + [\n",
    "    f\"{n:03}E\" for n in np.arange(0, 190, 10)\n",
    "]\n",
    "\n",
    "# all tiles\n",
    "lats = []\n",
    "lons = []\n",
    "for line in lines:\n",
    "    pieces = line.split(\"_\")\n",
    "    lat = pieces[-2]\n",
    "    lon = pieces[-1].split(\".\")[0]\n",
    "\n",
    "    if (lat in lat_tags) and (lon in lon_tags):\n",
    "        lats.append(lat)\n",
    "        lons.append(lon)\n",
    "print(len(lats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _preprocess(da, lat=None, lon=None):\n",
    "    da = da.rename({\"x\": \"lon\", \"y\": \"lat\"}).squeeze(drop=True)\n",
    "    if lat is not None:\n",
    "        da = da.assign_coords(lat=lat, lon=lon)\n",
    "    return da\n",
    "\n",
    "\n",
    "def open_hansen_2018_tile(lat, lon, emissions=False):\n",
    "    ds = xr.Dataset()\n",
    "\n",
    "    # Min Hansen data\n",
    "    variables = [\n",
    "        \"treecover2000\",\n",
    "        \"gain\",\n",
    "        \"lossyear\",\n",
    "        \"datamask\",\n",
    "    ]  # , \"first\", \"last\"]\n",
    "    for v in variables:\n",
    "        da = cat.hansen_2018(variable=v, lat=lat, lon=lon).to_dask().pipe(_preprocess)\n",
    "        # force coords to be identical\n",
    "        if ds:\n",
    "            da = da.assign_coords(lat=ds.lat, lon=ds.lon)\n",
    "        ds[v] = da\n",
    "\n",
    "    ds[\"treecover2000\"] /= 100.0\n",
    "    ds[\"lossyear\"] += 2000\n",
    "\n",
    "    # Hansen biomass\n",
    "    ds[\"agb\"] = (\n",
    "        cat.hansen_biomass(lat=lat, lon=lon).to_dask().pipe(_preprocess, lat=ds.lat, lon=ds.lon)\n",
    "    )\n",
    "    if emissions:\n",
    "        # Hansen emissions\n",
    "        ds[\"emissions_ha\"] = (\n",
    "            cat.hansen_emissions_ha(lat=lat, lon=lon)\n",
    "            .to_dask()\n",
    "            .pipe(_preprocess, lat=ds.lat, lon=ds.lon)\n",
    "        )\n",
    "        ds[\"emissions_px\"] = (\n",
    "            cat.hansen_emissions_px(lat=lat, lon=lon)\n",
    "            .to_dask()\n",
    "            .pipe(_preprocess, lat=ds.lat, lon=ds.lon)\n",
    "        )\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open a single 10x10degree tile of the Hansen 30x30m data\n",
    "lat = lats[1]\n",
    "lon = lons[1]\n",
    "box = dict(lat=slice(0, 40000, 100), lon=slice(0, 40000, 100))\n",
    "\n",
    "ds = open_hansen_2018_tile(lat, lon)\n",
    "display(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = {\"emissions\": {\"compressor\": numcodecs.Blosc()}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def process_one_tile(lat, lon):\n",
    "    url = f\"gs://carbonplan-scratch/global-forest-emissions/{lat}_{lon}.zarr\"\n",
    "\n",
    "    mapper = fsspec.get_mapper(url)\n",
    "\n",
    "    with dask.config.set(scheduler=\"threads\"):\n",
    "        ds = open_hansen_2018_tile(lat, lon)\n",
    "        ds = calc_one_tile(ds)[[\"emissions\"]]\n",
    "        ds = ds.chunk({\"lat\": 4000, \"lon\": 4000, \"year\": 2})\n",
    "        ds.to_zarr(mapper, encoding=encoding, mode=\"w\")\n",
    "        return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grid_area(da):\n",
    "    R = 6.371e6\n",
    "    total_area = 4 * np.pi * R ** 2\n",
    "    dϕ = np.radians((da[\"lat\"][1] - da[\"lat\"][0]).values)\n",
    "    dλ = np.radians((da[\"lon\"][1] - da[\"lon\"][0]).values)\n",
    "    dA = R ** 2 * np.abs(dϕ * dλ) * np.cos(np.radians(da[\"lat\"]))\n",
    "    areacella = dA * (0 * da + 1)\n",
    "    return areacella / SQM_PER_HA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "all_tiles = list(product(lat_tags, lon_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rerun_tiles = False\n",
    "if rerun_tiles:\n",
    "    tiles = []\n",
    "    for lat, lon in tqdm(list(zip(lats, lons))):\n",
    "        tiles.append(client.persist(process_one_tile(lat, lon), retries=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll load each of the tiles from above, coarsen them 100-fold in each direction (by summing\n",
    "emissions spatially), and then mosaic all the tiles together into a single xr dataset. Two notes for\n",
    "this step. First, it gets a little funky if you're running using a client/cluster. So, it might be\n",
    "worth your time to just power down your cluster and run this on a single machine that you have for\n",
    "this notebook. It'll run slower but your skull might be a little less bruised from banging your head\n",
    "against the wall. Second, you'll get a bunch of failed tiles below due to ocean cells that don't\n",
    "have active tiles. That's great and normal. Bask in the glory of the the NaNs that will fill that\n",
    "space!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(zip(lats, lons)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recombine = True\n",
    "if recombine:\n",
    "    list_all_coarsened = []\n",
    "    for lat, lon in tqdm(list(zip(lats, lons))):\n",
    "        try:\n",
    "            mapper = fsspec.get_mapper(\n",
    "                f\"gs://carbonplan-scratch/global-forest-emissions/{lat}_{lon}.zarr\"\n",
    "            )\n",
    "            da_global = xr.open_zarr(mapper)\n",
    "            da_mask = da_global.isel(year=0, drop=True)\n",
    "            da_area = compute_grid_area(da_mask)\n",
    "            list_all_coarsened.append(\n",
    "                (da_global * da_area).coarsen(lat=100, lon=100).sum().compute()\n",
    "            )\n",
    "        except:\n",
    "            print(\"{} {} didnt work booooo\".format(lat, lon))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then combine that whole list into a single dataset and write it out.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coarsened_url = f\"gs://carbonplan-scratch/global-forest-emissions/global/3000m/raster.zarr\"\n",
    "mapper = fsspec.get_mapper(coarsened_url)\n",
    "recombine = False\n",
    "\n",
    "if recombine:\n",
    "    # with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
    "    combined_ds = xr.combine_by_coords(list_all_coarsened, compat=\"override\", coords=\"minimal\")\n",
    "#     combined_ds.to_zarr(mapper,\n",
    "#                      encoding=encoding,\n",
    "#                      mode='w')\n",
    "else:\n",
    "    combined_ds = xr.open_zarr(mapper)\n",
    "#     combined_ds = combined_ds.rename({'emissions': 'Emissions [tC02/year]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_ds.coarsen(lat=100, lon=100).isel(year=0).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coarsened_sel = combined_ds.isel(year=0).coarsen(lat=10, lon=10).sum().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coarsened_sel.emissions.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_ds.isel(year=17).emissions.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = combined_ds_testing.sel(year=2005) - combined_ds.sel(year=2005).sel(\n",
    "    lat=combined_ds_testing.lat, lon=combined_ds_testing.lon\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff.emissions.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_ds = combined_ds.rename({\"emissions\": \"Emissions [tC02/year]\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we'll load all of these back into a single array and concatenate along the year dimension.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity checks!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the dataset and get a handle on whether this makes sense. Let's do it by\n",
    "looking at a handful of big fires as a gut check for biomass loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's check out the Thomas Fire in Santa Barbara in 2017, the second largest (outside of 2020)\n",
    "California history since we don't have 2020 data yet. This ranking is according to wikipedia -\n",
    "https://en.wikipedia.org/wiki/List_of_California_wildfires. I include this because there's a\n",
    "narrative here - we're on a Sherlock Holmes level hunt for fires. Using satellite biomass data as\n",
    "our clues!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "santa_barbara = combined_ds.sel(lat=slice(34, 34.5), lon=slice(-119.75, -119))[\n",
    "    \"Emissions [tC02/year]\"\n",
    "]\n",
    "santa_barbara.sel(year=2017).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But then let's see how that fire compared to years previous. It turns out that, indeed, it was a big\n",
    "one, burning over 40 megatons of biomass in 2017. Some of that fire carried over into 2018 too.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "santa_barbara.sum(dim=[\"lat\", \"lon\"]).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of curiosity, we can take a look at a slightly bigger box, extending further north. When we do\n",
    "that, we notice that the 2017 Thomas Fire was actually dwarfed by something in 2007. What is this\n",
    "aberration!? I thought wikipedia, the paragon of truth, told us that the Thomas Fire was the\n",
    "biggest!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "santa_barbara_bigger = combined_ds.sel(lat=slice(34, 35), lon=slice(-119.75, -119))[\n",
    "    \"Emissions [tC02/year]\"\n",
    "]\n",
    "santa_barbara_bigger.sum(dim=[\"lat\", \"lon\"]).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out that actually there was _giant_ complex of fires (also in Santa Barbara county) in 2007.\n",
    "Over a million acres burned. And they were picked up by the loss in biomass. So this is some\n",
    "rejection of bias on my end! Excellent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go check somewhere else just to see if it's working elsewhere. Let's travel up to my home\n",
    "state of Washington State. As a gut check, the 2015 fire season was the largest in recent memory.\n",
    "Let's see if it jumps out at all in total Washington losses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "washington_state = combined_ds.sel(lat=slice(45.5, 49), lon=slice(-124.6, -117))[\n",
    "    \"Emissions [tC02/year]\"\n",
    "]\n",
    "washington_state.sum(dim=[\"lat\", \"lon\"]).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, let's take a look at the Okanaogan region where they had a giant fire in 2015. So, let's see\n",
    "if zooming in a little further into specific regions of big fires leap out.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "washington_state.sel(lat=slice(48.4, 48.6), lon=slice(-119.8, -119.6)).sum(\n",
    "    dim=[\"lat\", \"lon\"]\n",
    ").plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check out Brazil. First we can look at the state of Mato Grosso, one site of heavy\n",
    "deforestation for conversion to agriculure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_ds.sel(lat=slice(-14, -11), lon=slice(-55, -54)).sum(dim=[\"lat\", \"lon\"])[\n",
    "    \"Emissions [tC02/year]\"\n",
    "].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can switch to the state of Pará.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_ds.sel(lat=slice(-9, -2), lon=slice(-54, -49)).sum(dim=[\"lat\", \"lon\"])[\n",
    "    \"Emissions [tC02/year]\"\n",
    "].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting a year we can see individual plots of land where mass deforestation has occurred.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_ds.sel(lat=slice(-9, -2), lon=slice(-54, -49)).sel(year=2017)[\n",
    "    \"Emissions [tC02/year]\"\n",
    "].plot(vmax=1e6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gutcheck by trying to reproduce Zarin et al (2016)\n",
    "\n",
    "## a.k.a. Reproducibility Rumble\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to reproduce a figure from Zarin et al (2016). We'll try out Mexico since we can\n",
    "reasonably easily select a box around all of Mexico without including other countries. We include\n",
    "Guatemala and a bit of Southern Texas/Arizona but let's just assume those are negligible compared to\n",
    "the country of Mexico. So, comparing this to the plot from\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mexico = combined_ds.sel(lat=slice(14, 31), lon=slice(-117, -87))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mexico.sum(dim=[\"lat\", \"lon\"])[\"emissions\"].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAD FACE! We're off by a _ton_ <--- a pun! because we're talking about tons of carbon!! :grin:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "our estimates are off by a factor of ~100. now, zarin's plot for mexico only includes deforestation,\n",
    "(so maybe that excludes the fire-attributed losses?). But they're def off by orders of magnitude.\n",
    "maybe these estimates we're calculating by combining everything is actually spatially-averaged\n",
    "quantities - tCO2/hectare. which would mean that an average is really what we want. and that\n",
    "coarsening isn't quite right because the geographic transformation matters. nevertheless, i think\n",
    "the coarsening effect might be okay as an approximation for now.\n",
    "\n",
    "HOWEVER, the per hectare unit thing is an issue- we summed things to go from 30m-->3km, so we'd need\n",
    "to divide them by ~10000 (to account for the coarsening 100-fold in each direction) to get back to\n",
    "the per hectare unit. and then we use that per hectare unit and average it across regions and\n",
    "multiply by the hectare area to get the right tCO2. as a gut check i applied that \"divide by 10,000\"\n",
    "correction that around my bounding box for mexico, multiplied it by mexico's area (in hectares) and\n",
    "the units are on the same order of magnitude of zarin. now, this is very back-of-the-envelope. but i\n",
    "think it makes sense! so, for reporting values, each of our coarsened pixels just needs to be\n",
    "divided by 10,000 and then scaled by how many hectares that pixel is (which decreases going away\n",
    "from the equator, but we could make a raster of that).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hectares_in_mexico = 197253494\n",
    "correction_for_having_summed_during_coarsening = 100 * 100\n",
    "(\n",
    "    mexico.mean(dim=[\"lat\", \"lon\"])\n",
    "    * hectares_in_mexico\n",
    "    / correction_for_having_summed_during_coarsening\n",
    ")[\"Emissions [tC02/year]\"].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ba da bing ba da boom! Now we're in the ballpark of Zarin. Don't miss the next episode, when we\n",
    "figure out why our wiggles are totally off from theirs.... tune in!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"here!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:notebook] *",
   "language": "python",
   "name": "conda-env-notebook-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
