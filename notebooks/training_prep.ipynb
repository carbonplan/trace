{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep datasets for training\n",
    "\n",
    "Created by: Oriana Chegwidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pyproj import CRS\n",
    "import boto3\n",
    "from rasterio.session import AWSSession\n",
    "from s3fs import S3FileSystem\n",
    "aws_session = AWSSession(boto3.Session(),#profile_name='default'), \n",
    "                         requester_pays=True)\n",
    "fs = S3FileSystem(requester_pays=True) #profile='default', \n",
    "import xgboost as xgb\n",
    "\n",
    "from osgeo.gdal import VSICurlClearCache\n",
    "import rasterio as rio\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import dask\n",
    "import os\n",
    "import fsspec\n",
    "import geopandas as gpd\n",
    "import rioxarray # for the extension to load\n",
    "import matplotlib.pyplot as plt\n",
    "import utm\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import json\n",
    "import zarr\n",
    "import awswrangler as wr\n",
    "from dask_gateway import Gateway\n",
    "from carbonplan_trace.v1.landsat_preprocess import access_credentials, test_credentials\n",
    "from carbonplan_trace.v1.inference import predict, predict_delayed\n",
    "from carbonplan_trace.v1 import utils\n",
    "from carbonplan_trace.v1.training_prep import prep_training_dataset, prep_training_dataset_delayed, add_parquet_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# kind_of_cluster = \"local\"\n",
    "kind_of_cluster = \"remote\"\n",
    "if kind_of_cluster == \"local\":\n",
    "    # spin up local cluster. must be on big enough machine\n",
    "    from dask.distributed import Client\n",
    "\n",
    "    client = Client(n_workers=1, threads_per_worker=1)  # _per_worker=4\n",
    "    client\n",
    "else:\n",
    "    gateway = Gateway()\n",
    "    options = gateway.cluster_options()\n",
    "    options.environment = {\n",
    "        \"AWS_REQUEST_PAYER\": \"requester\",\n",
    "        \"AWS_REGION_NAME\": \"us-west-2\",\n",
    "    }\n",
    "    options.worker_cores = 1\n",
    "    options.worker_memory = 200\n",
    "    options.image = \"carbonplan/trace-python-notebook:latest\"\n",
    "    cluster = gateway.new_cluster(cluster_options=options)\n",
    "    #     cluster.adapt(minimum=2, maximum=100)\n",
    "    cluster.scale(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = cluster.get_client()\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each Landsat scene is stored in cloud optimized geotiff (COG) according to a verbose (but once you\n",
    "understand it, human readable!) naming convention. Landsat Collection 2 uses the same naming\n",
    "convention as Collection 1 which is as follows (lifted from their docs at\n",
    "`https://prd-wret.s3.us-west-2.amazonaws.com/assets/palladium/production/atoms/files/LSDS-1656_%20Landsat_Collection1_L1_Product_Definition-v2.pdf`\n",
    "\n",
    "`LXSS_LLLL_PPPRRR_YYYYMMDD_yyyymmdd_CC_TX` where\n",
    "\n",
    "```\n",
    "L = Landsat  (constant)\n",
    "X = Sensor  (C = OLI / TIRS, O = OLI-only, T= TIRS-only, E = ETM+, T = TM, M= MSS)\n",
    "SS = Satellite  (e.g., 04 for Landsat 4, 05 for Landsat 5, 07 for Landsat 7, etc.)\n",
    "LLLL = Processing  level  (L1TP, L1GT, L1GS)\n",
    "PPP  = WRS path\n",
    "RRR  = WRS row\n",
    "YYYYMMDD = Acquisition  Year (YYYY) / Month  (MM) / Day  (DD)\n",
    "yyyymmdd  = Processing  Year (yyyy) / Month  (mm) / Day (dd)\n",
    "CC = Collection  number  (e.g., 01, 02, etc.)\n",
    "TX= RT for Real-Time, T1 for Tier 1 (highest quality), and T2 for Tier 2\n",
    "\n",
    "```\n",
    "\n",
    "Thus, we're looking for scenes coded in the following way:\n",
    "`LE07_????_PPP_RRR_YYYMMDD_yyyymmdd_02_T1` for Landsat 7 and\n",
    "`LT05_????_PPP_RRR_YYYMMDD_yyyymmdd_02_T1` for Landsat 5 (but T1 might be wrong there)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are re-implementing (to the best of our abilities) the methods from Wang et al (in review). Jon\n",
    "Wang's paper said:\n",
    "\n",
    "`To extend our AGB predictions through space and time, we used time series (1984 – 2014) of 30 m surface reflectance data from the Thematic Mapper onboard Landsat 5 and the Enhanced Thematic Mapper Plus onboard Landsat 7. We used the GLAS-derived estimates of AGB as a response variable and the mean growing season (June, July, August) and non-growing season values for each of Landsat’s six spectral reflectance bands as the predictors in an ensemble machine learning model`\n",
    "\n",
    "So we'll be looking for:\n",
    "\n",
    "- Landsat 5 (Thematic mapper) and 7 (Enhanced Thematic Mapper Plus)\n",
    "- Growing season (June-August) and non-growing season (Sept-May) averages at an annual timestep.\n",
    "  <--- will need to figure out around the calendar whether we want consecutive\n",
    "- All six spectral reflectance bands\n",
    "- We'll do a quality thresholding of cloudless cover for now based upon their thresholding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In orienting myeslf, these are the potential collection options I've figured out (by poking around\n",
    "here on the [sat-api catalog](https://landsatlook.usgs.gov/sat-api/collections):\n",
    "\n",
    "- `landsat-c2l2-sr` Landsat Collection 2 Level-2 UTM Surface Reflectance (SR) Product\n",
    "- `landsat-c2l2alb-sr` Landsat Collection 2 Level-2 Albers Surface Reflectance (SR) Product\n",
    "- `landsat-c1l2alb-sr` Landsat Collection 1 Level-2 Albers Surface Reflectance (SR) Product <-- we\n",
    "  don't want this one (b/c we'll go with collection 2)\n",
    "- `landsat-c2l1` Landsat Collection 2 Level-1 Product <-- don't think we want this because we want\n",
    "  surface reflectance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this once to apply the aws session to the rasterio environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different kinds of QA/QC bands contained in L2SP:\n",
    "\n",
    "- SR_CLOUD_QA - I think we want this one because anything less than 2 is either just dark dense\n",
    "  vegetation or no flags. everything above is stuff like water, snow, cloud (different levels of\n",
    "  obscurity). This is the result of the fmask algorithm from Zhu et al.\n",
    "- QA_PIXEL - this gets a little more specific and goes intot different kinds of clouds. Super\n",
    "  interesting but I don't think we want to use it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull in the SR_CLOUD_QA and use as a mask - see Table 5-3 in\n",
    "https://prd-wret.s3.us-west-2.amazonaws.com/assets/palladium/production/atoms/files/LSDS-1370_L4-7_C1-SurfaceReflectance-LEDAPS_ProductGuide-v3.pdf\n",
    "for description of cloud integer values to select which ones to use as drop. For now I'll drop\n",
    "anything greater than 1 (0= no QA concerns and 1 is Dark dense vegetation (DDV)).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we make the query using sat-search to find every file in the STAC catalog that we want. We'll\n",
    "store that list of files. We'll do this first for a single tile (in this first exmaple just covering\n",
    "Washington State) but then we'll loop through in 1-degree by 1-degree tiles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shutdown_cluster(kind_of_cluster):\n",
    "    if kind_of_cluster == \"local\":\n",
    "        client.shutdown()\n",
    "    elif kind_of_cluster == \"remote\":\n",
    "        cluster.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shutdown_cluster(kind_of_cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to memory constraints we'll average repeated captures of the same scene. Then we'll average all\n",
    "of those averaged scenes together to create the full mesh. As of now we're just doing a straight\n",
    "average but ideally we would carry the weights of the number of repeats of each scene and do a\n",
    "weighted average when quilting the scenes together.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we take the list of files for a given year to average across growing season for each of the\n",
    "tiles and write it out to a mapper with those specifications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_key_id, secret_access_key = access_credentials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_credentials(aws_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.config.set({\"array.slicing.split_large_chunks\": True})\n",
    "# tcp is a transmission control protocol\n",
    "dask.config.set({\"distributed.comm.timeouts.tcp\": \"50s\"})\n",
    "dask.config.set({\"distributed.comm.timeouts.connect\": \"50s\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file(\n",
    "    \"https://prd-wret.s3-us-west-2.amazonaws.com/assets/\"\n",
    "    \"palladium/production/s3fs-public/atoms/files/\"\n",
    "    \"WRS2_descending_0.zip\"\n",
    ")\n",
    "bucket = \"s3://carbonplan-climatetrace/v1\"\n",
    "\n",
    "biomass_folder = bucket + \"/biomass/\"\n",
    "biomass_files = fs.ls(biomass_folder)\n",
    "lat_lon_tags = [utils.get_lat_lon_tags_from_tile_path(fp) for fp in biomass_files]\n",
    "bounding_boxes = [utils.parse_bounding_box_from_lat_lon_tags(lat, lon) for lat, lon in lat_lon_tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = 144\n",
    "row = 60\n",
    "year = 2006\n",
    "\n",
    "\n",
    "l = np.array(\n",
    "    [\n",
    "        [66, 45, 2003],\n",
    "        [66, 45, 2004],\n",
    "        [66, 45, 2005],\n",
    "        [66, 45, 2006],\n",
    "        [66, 45, 2007],\n",
    "        [66, 45, 2008],\n",
    "        [66, 45, 2009],\n",
    "        [6, 32, 2004],\n",
    "    ]\n",
    ")\n",
    "for i in l:\n",
    "    print(i)\n",
    "    prep_training_dataset(\n",
    "        path=task_id[i][0],\n",
    "        row=task_id[i][1],\n",
    "        year=task_id[i][2],\n",
    "        access_key_id=access_key_id,\n",
    "        secret_access_key=secret_access_key,\n",
    "        training_write_bucket=f\"{bucket}/training\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from carbonplan_trace.v1.glas_allometric_eq import REALM_GROUPINGS\n",
    "\n",
    "realms = list(REALM_GROUPINGS.keys())\n",
    "processed_scenes = []\n",
    "for realm in realms + [\"no_data\"]:\n",
    "    print(realm)\n",
    "    for year in np.arange(2003, 2010):\n",
    "        processed_scenes.extend(fs.ls(f\"{bucket}/training/{realm}/{year}\", recursive=True))\n",
    "\n",
    "processed_scenes = [scene[-19:-8] for scene in processed_scenes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(processed_scenes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(processed_scenes) - 66151"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "landsat_bucket = \"s3://usgs-landsat/collection02/level-2/standard/etm/{}/{:03d}/{:03d}/\"\n",
    "\n",
    "with rio.Env(aws_session):\n",
    "    tasks = []\n",
    "    task_id = []\n",
    "    for bounding_box in bounding_boxes:\n",
    "        print(bounding_box)\n",
    "        min_lat, max_lat, min_lon, max_lon = bounding_box\n",
    "        scenes_in_tile = gdf.cx[min_lon:max_lon, min_lat:max_lat][[\"PATH\", \"ROW\"]].values\n",
    "        for year in np.arange(2003, 2010):\n",
    "            for [path, row] in scenes_in_tile:\n",
    "                scene_stores = fs.ls(landsat_bucket.format(year, path, row))\n",
    "                output_name = f\"{year}/{path:03d}{row:03d}\"\n",
    "                if len(scene_stores) == 0:\n",
    "                    continue\n",
    "                elif output_name in processed_scenes:\n",
    "                    continue\n",
    "                else:\n",
    "                    tasks.append(\n",
    "                        #                         prep_training_dataset(\n",
    "                        #                         prep_training_dataset_delayed(\n",
    "                        client.compute(\n",
    "                            prep_training_dataset_delayed(\n",
    "                                path=path,\n",
    "                                row=row,\n",
    "                                year=year,\n",
    "                                access_key_id=access_key_id,\n",
    "                                secret_access_key=secret_access_key,\n",
    "                                training_write_bucket=f\"{bucket}/training\",\n",
    "                                error=\"raise\",\n",
    "                            )\n",
    "                        )\n",
    "                    )\n",
    "                    task_id.append([path, row, year])\n",
    "    print(len(tasks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# results = dask.compute(tasks, retries=1)[0]\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for i, task in enumerate(tasks):\n",
    "#     try:\n",
    "#         task.cancel()\n",
    "#     except:\n",
    "#         print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "access_denied = []\n",
    "\n",
    "# three types of error\n",
    "# KilledWorker == memory usage\n",
    "# Assertion Error => need to figure out what we want to do for inference, can throw away for now\n",
    "# .zmetadata\n",
    "# AccessDenied should have been fixed\n",
    "\n",
    "for i, task in enumerate(tasks):\n",
    "    if task.status == \"error\" and i not in []:\n",
    "        print(i)\n",
    "        #         print(task.result())\n",
    "        try:\n",
    "            print(task.result())\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "#             if isinstance(e, PermissionError):\n",
    "#                 access_denied.append(i)\n",
    "#                 print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_id[120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in access_denied:\n",
    "    print(i)\n",
    "    prep_training_dataset(\n",
    "        path=task_id[i][0],\n",
    "        row=task_id[i][1],\n",
    "        year=task_id[i][2],\n",
    "        access_key_id=access_key_id,\n",
    "        secret_access_key=secret_access_key,\n",
    "        training_write_bucket=f\"{bucket}/training\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# for _ in range(100):\n",
    "#     check = random.choice(processed_scenes)\n",
    "#     print(check)\n",
    "#     if 'no_data' in check:\n",
    "#         continue\n",
    "#     df = pd.read_parquet(f's3://{check}')\n",
    "#     for v in ['SR_B1', 'SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B7']:\n",
    "#         print(v, df[v].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In case you want to aggregate the parquet files the following snippets could be useful. They're written currently to aggregate back to make a file for a 10x10 tile.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine_parquet_files_full_tile(\n",
    "#     ul_lat,\n",
    "#     ul_lon,\n",
    "#     write=True,\n",
    "#     access_key_id=access_key_id,\n",
    "#     secret_access_key=secret_access_key,\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
