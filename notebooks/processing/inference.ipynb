{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference pipeline\n",
    "\n",
    "Created by: Oriana Chegwidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pyproj import CRS\n",
    "import boto3\n",
    "from rasterio.session import AWSSession\n",
    "from s3fs import S3FileSystem\n",
    "aws_session = AWSSession(boto3.Session(),#profile_name='default'), \n",
    "                         requester_pays=True)\n",
    "fs = S3FileSystem(requester_pays=True) #profile='default', \n",
    "import xgboost as xgb\n",
    "\n",
    "from osgeo.gdal import VSICurlClearCache\n",
    "import rasterio as rio\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import dask\n",
    "import os\n",
    "import fsspec\n",
    "import geopandas as gpd\n",
    "import rioxarray # for the extension to load\n",
    "import matplotlib.pyplot as plt\n",
    "import utm\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "import json\n",
    "import zarr\n",
    "import awswrangler as wr\n",
    "from dask_gateway import Gateway\n",
    "from carbonplan_trace.v1.landsat_preprocess import access_credentials, test_credentials\n",
    "from carbonplan_trace.v1.inference import predict, predict_delayed\n",
    "from carbonplan_trace.v1 import utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyproj\n",
    "\n",
    "pyproj.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from carbonplan_trace import version\n",
    "\n",
    "print(version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.config.set({\"array.slicing.split_large_chunks\": True})\n",
    "# tcp is a transmission control protocol\n",
    "dask.config.set({\"distributed.comm.timeouts.tcp\": \"50s\"})\n",
    "dask.config.set({\"distributed.comm.timeouts.connect\": \"50s\"})\n",
    "# dask.config.set({\"distributed.worker.resources.WORKERTOKEN\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kind_of_cluster = \"local\"\n",
    "# kind_of_cluster = \"remote\"\n",
    "if kind_of_cluster == \"local\":\n",
    "    # spin up local cluster. must be on big enough machine\n",
    "    from dask.distributed import Client\n",
    "\n",
    "    # when very very huge use 8,8\n",
    "    client = Client(n_workers=8, threads_per_worker=8, resources={\"workertoken\": 1})\n",
    "    client\n",
    "else:\n",
    "    gateway = Gateway()\n",
    "    options = gateway.cluster_options()\n",
    "    options.environment = {\n",
    "        \"AWS_REQUEST_PAYER\": \"requester\",\n",
    "        \"AWS_REGION_NAME\": \"us-west-2\",\n",
    "        \"DASK_DISTRIBUTED__WORKER__RESOURCES__WORKERTOKEN\": \"1\",\n",
    "    }\n",
    "    options.worker_cores = 8\n",
    "    options.worker_memory = 100\n",
    "\n",
    "    options.image = \"carbonplan/trace-python-notebook:latest\"\n",
    "    cluster = gateway.new_cluster(cluster_options=options)\n",
    "    cluster.adapt(minimum=1, maximum=10)\n",
    "#     cluster.scale(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = cluster.get_client()\n",
    "client\n",
    "\n",
    "# check this link first\n",
    "# possible scenario:\n",
    "# 1) everything is succeeding and cluster still running, no need to do anything\n",
    "# 2) most things are failing but cluster still running, restart, increase mem and decrease num worker, re start and run all\n",
    "# 3) 404 error -> cluster died -> restart and run all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shutdown_cluster(kind_of_cluster):\n",
    "    if kind_of_cluster == \"local\":\n",
    "        client.shutdown()\n",
    "    elif kind_of_cluster == \"remote\":\n",
    "        cluster.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "shutdown_cluster(\"local\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_key_id, secret_access_key = access_credentials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_credentials(aws_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we take the list of files for a given year to average across growing season for each of the\n",
    "tiles and write it out to a mapper with those specifications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ul_lats = [\"10S\", \"20S\", \"30S\"]\n",
    "ul_lons = [f\"{lon}E\" for lon in np.arange(110, 151, 10)]\n",
    "lat_lon_tags = []\n",
    "for ul_lat in ul_lats:\n",
    "    for ul_lon in ul_lons:\n",
    "        lat_lon_tags.append((ul_lat, ul_lon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file(\n",
    "    \"https://prd-wret.s3-us-west-2.amazonaws.com/assets/\"\n",
    "    \"palladium/production/s3fs-public/atoms/files/\"\n",
    "    \"WRS2_descending_0.zip\"\n",
    ")\n",
    "bucket = \"s3://carbonplan-climatetrace/v2.1\"\n",
    "\n",
    "# biomass_folder = \"s3://carbonplan-climatetrace/intermediate/ecoregions_mask/\"\n",
    "# biomass_files = fs.ls(biomass_folder) # just to get list of lat_lon tiles we want\n",
    "# lat_lon_tags = [utils.get_lat_lon_tags_from_tile_path(fp) for fp in biomass_files]\n",
    "# lat_lon_tags = [('60N', '130W')]#, ('40N', '130W')]#, ('00N', '060W')] #('50N', '130W'),\n",
    "bounding_boxes = [utils.parse_bounding_box_from_lat_lon_tags(lat, lon) for lat, lon in lat_lon_tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from carbonplan_trace.v1.glas_allometric_eq import REALM_GROUPINGS\n",
    "\n",
    "processed_scenes = []\n",
    "for year in np.arange(2011, 2022):\n",
    "    processed_scenes.extend(fs.ls(f\"{bucket}/inference/xg/{year}\", recursive=True))\n",
    "\n",
    "processed_scenes = [scene[-19:-8] for scene in processed_scenes]\n",
    "len(processed_scenes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import carbonplan_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll loop through every scene and every year and calculate biomass for that scene. Will produce\n",
    "table of values [x, y, (both specific to utm projection), lat, lon, biomass].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bounding_box in bounding_boxes:\n",
    "    min_lat, max_lat, min_lon, max_lon = bounding_box\n",
    "    valid_scenes = gdf.cx[min_lon:max_lon, min_lat:max_lat][[\"PATH\", \"ROW\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_lengths = pd.DataFrame(\n",
    "    columns=[\"v1-rf\", \"v2-rf\", \"v2-xg\"],\n",
    "    index=[\"_\".join([str(path), str(row)]) for (path, row) in valid_scenes],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# rerun_scenes = {'2010':[], '2014':[]}\n",
    "# setups = [('v2', 'rf')]#, ('v2', 'xg')] #('v1', 'rf'),\n",
    "# for year in ['2010', '2014']:\n",
    "#     for (version, model) in setups:\n",
    "#         for [path, row] in valid_scenes:\n",
    "#             output_name = f\"{year}/{path:03d}{row:03d}.parquet\"\n",
    "#             print(f's3://carbonplan-climatetrace/{version}/inference/{model}/{output_name}')\n",
    "#             if len(fs.ls(f's3://carbonplan-climatetrace/{version}/inference/{model}/{output_name}')) == 0:\n",
    "#                 if [path, row] not in rerun_scenes[year]:\n",
    "#                     rerun_scenes[year].append([path, row])\n",
    "#         i+=1\n",
    "#             file_length = len(pd.read_parquet(f's3://carbonplan-climatetrace/{version}/inference/{model}/{output_name}'))\n",
    "#         except FileNotFoundError:\n",
    "#             file_length = np.nan\n",
    "\n",
    "#         file_lengths.loc[f'{path}_{row}', f'{version}-{model}'] = file_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_lengths.to_csv('files_to_repeat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove each entry in index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "landsat_bucket = \"s3://usgs-landsat/collection02/level-2/standard/etm/{}/{:03d}/{:03d}/\"\n",
    "with rio.Env(aws_session):\n",
    "    tasks = []\n",
    "    task_ids = []\n",
    "    for bounding_box in bounding_boxes:\n",
    "        print(bounding_box)\n",
    "        min_lat, max_lat, min_lon, max_lon = bounding_box\n",
    "        scenes_in_tile = gdf.cx[min_lon:max_lon, min_lat:max_lat][[\"PATH\", \"ROW\"]].values\n",
    "        for year in np.arange(2011, 2022):\n",
    "            for [path, row] in scenes_in_tile:\n",
    "                scene_stores = fs.ls(landsat_bucket.format(year, path, row))\n",
    "                output_name = f\"{year}/{path:03d}{row:03d}\"\n",
    "                if len(scene_stores) == 0:\n",
    "                    continue\n",
    "                elif output_name in processed_scenes:\n",
    "                    continue\n",
    "                elif output_name in task_id:\n",
    "                    continue\n",
    "                else:\n",
    "                    tasks.append(\n",
    "                        #                         predict(\n",
    "                        client.compute(\n",
    "                            predict_delayed(\n",
    "                                model_folder=f\"{bucket}/models/\",\n",
    "                                path=path,\n",
    "                                row=row,\n",
    "                                year=year,\n",
    "                                access_key_id=access_key_id,\n",
    "                                secret_access_key=secret_access_key,\n",
    "                                output_write_bucket=f\"{bucket}/inference\",\n",
    "                            ),\n",
    "                            resources={\"workertoken\": 1},\n",
    "                        )\n",
    "                    )\n",
    "                    task_id.append(output_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(rerun_scenes[\"2014\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = dask.compute(tasks, retries=1, resources={\"workertoken\": 1})[0]\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# i = 0\n",
    "# path = task_id[i][0]\n",
    "# row = task_id[i][1]\n",
    "# year = task_id[i][2]\n",
    "\n",
    "path = 48\n",
    "row = 22\n",
    "year = 2014\n",
    "\n",
    "print(path, row, year)\n",
    "\n",
    "predict(\n",
    "    model_folder=f\"{bucket}/models/\",\n",
    "    path=path,\n",
    "    row=row,\n",
    "    year=year,\n",
    "    access_key_id=access_key_id,\n",
    "    secret_access_key=secret_access_key,\n",
    "    output_write_bucket=f\"{bucket}/inference\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.ls(\"s3://carbonplan-climatetrace/v2/inference/rf/2014/054018.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.ls(\"s3://carbonplan-climatetrace/v2/inference/xg/2014/054018.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fs.ls(\"s3://carbonplan-climatetrace/v2/inference/rf/2014/054018.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 0\n",
    "# path = task_id[i][0]\n",
    "# row = task_id[i][1]\n",
    "# year = task_id[i][2]\n",
    "\n",
    "path = 54\n",
    "row = 18\n",
    "year = 2010\n",
    "\n",
    "print(path, row, year)\n",
    "\n",
    "predict(\n",
    "    model_folder=f\"{bucket}/models/\",\n",
    "    path=path,\n",
    "    row=row,\n",
    "    year=year,\n",
    "    access_key_id=access_key_id,\n",
    "    secret_access_key=secret_access_key,\n",
    "    output_write_bucket=f\"{bucket}/inference\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_parquet(\"s3://carbonplan-climatetrace/v2/inference/rf/2010/054018.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, task in enumerate(tasks):\n",
    "    if task.status == \"error\" and i not in []:\n",
    "        print(i)\n",
    "        print(task.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "exclude_list = []\n",
    "errors = []\n",
    "\n",
    "for i, task in enumerate(tasks):\n",
    "    if task.status == \"error\" and i not in []:\n",
    "        print(i)\n",
    "        #         print(task.result())\n",
    "        try:\n",
    "            print(task.result())\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            exclude_list.append(list(task_id[i]))\n",
    "\n",
    "pd.DataFrame(exclude_list, columns=[\"path\", \"row\", \"year\"]).to_csv(\"inference_failed_tasks.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "exclude_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, task in enumerate(tasks):\n",
    "    try:\n",
    "        task.cancel()\n",
    "    except:\n",
    "        print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:notebook] *",
   "language": "python",
   "name": "conda-env-notebook-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
