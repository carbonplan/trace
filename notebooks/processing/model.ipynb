{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff16ae6-8be5-4eb9-b413-e84b8c9c8306",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0f286f-e4ec-4c6d-bb41-8b1ae3882433",
   "metadata": {},
   "outputs": [],
   "source": [
    "from carbonplan_trace.v1.glas_allometric_eq import REALM_GROUPINGS\n",
    "from carbonplan_trace.v1 import load\n",
    "import carbonplan_trace.v1.model as m\n",
    "import pandas as pd\n",
    "from carbonplan_trace.v1.landsat_preprocess import access_credentials\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bb0de6-2917-43f9-8cef-206270f1b980",
   "metadata": {},
   "outputs": [],
   "source": [
    "access_key_id, secret_access_key = access_credentials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67552a86-0c5a-4e88-8b8e-d018194e6ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we train one model per realm\n",
    "\n",
    "# realms = list(REALM_GROUPINGS.keys())\n",
    "# only use australia for example, but we would want all when rerunning this\n",
    "realms = [\"australia\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5ec827-baea-4dd3-926a-dcf77818a7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block of code is used for generating difference parameter sets for hyperparameter optimization (HPO) of the model\n",
    "# the params here are for the xgboost model\n",
    "\n",
    "# import itertools\n",
    "\n",
    "# def product_dict(**kwargs):\n",
    "#     keys = kwargs.keys()\n",
    "#     vals = kwargs.values()\n",
    "#     for instance in itertools.product(*vals):\n",
    "#         yield dict(zip(keys, instance))\n",
    "\n",
    "\n",
    "# param_set = {\n",
    "#     \"learning_rate\": [0.07, 0.05, 0.03],\n",
    "#     \"max_depth\": [10, 12, 14],\n",
    "#     \"colsample_bytree\": [0.5, 0.7, 0.9],\n",
    "#     \"subsample\": [0.5, 0.7, 0.9],\n",
    "#     \"min_child_weight\": [2, 4, 6],\n",
    "#     \"lambda\": [1, 1.5, 2],\n",
    "#     \"alpha\": [0, 0.5, 1],\n",
    "#     \"gamma\": [0, 0.5, 1],\n",
    "# }\n",
    "\n",
    "# groupings = [\n",
    "#     [\"learning_rate\"],\n",
    "#     [\"max_depth\"],\n",
    "#     [\"colsample_bytree\", \"subsample\", \"min_child_weight\"],\n",
    "#     [\"lambda\", \"alpha\", \"gamma\"],\n",
    "# ]\n",
    "\n",
    "# dims = [list(range(len(param_set[g[0]]))) for g in groupings]\n",
    "# param_set_list = []\n",
    "# for orders in list(itertools.product(*dims)):\n",
    "#     d = {}\n",
    "#     for o, g in zip(orders, groupings):\n",
    "#         for k in g:\n",
    "#             d[k] = param_set[k][o]\n",
    "#     param_set_list.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b090b3-a9eb-4841-ab84-d779ae51a8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions for assessing model performances\n",
    "\n",
    "\n",
    "def get_all_prediction_result(model, df_train, df_test, df_val):\n",
    "\n",
    "    df_train[\"biomass_pred\"] = model._predict(df_train)\n",
    "    df_test[\"biomass_pred\"] = model._predict(df_test)\n",
    "    df_val[\"biomass_pred\"] = model._predict(df_val)\n",
    "\n",
    "    return df_train, df_test, df_val\n",
    "\n",
    "\n",
    "def calculate_temporal_variability(df, y1=2007, y2=2008, precision=3):\n",
    "    year1 = df.loc[df.year == y1, [\"lat\", \"lon\", \"biomass\"]]\n",
    "    year2 = df.loc[df.year == y2, [\"lat\", \"lon\", \"biomass\"]]\n",
    "\n",
    "    year1[\"lat_round\"] = year1.lat.round(precision)\n",
    "    year1[\"lon_round\"] = year1.lon.round(precision)\n",
    "    year2[\"lat_round\"] = year2.lat.round(precision)\n",
    "    year2[\"lon_round\"] = year2.lon.round(precision)\n",
    "\n",
    "    merged = year1.merge(year2, on=[\"lat_round\", \"lon_round\"], suffixes=[\"_year1\", \"_year2\"])\n",
    "\n",
    "    mae = (merged.biomass_year2 - merged.biomass_year1).abs().mean()\n",
    "    me = (merged.biomass_year2 - merged.biomass_year1).mean()\n",
    "\n",
    "    return {\"mae\": mae, \"me\": me}\n",
    "\n",
    "\n",
    "def plot_scatter(sub, title, n=500000):\n",
    "    xmin = -10\n",
    "    size = min(len(sub), n)\n",
    "    toplot = sub.sample(n=size)\n",
    "    xmax = toplot.biomass.quantile(0.95)\n",
    "    plt.scatter(toplot.biomass, toplot.biomass_pred, s=1, alpha=0.03)\n",
    "    plt.plot([xmin, xmax], [xmin, xmax], \"k\")\n",
    "    plt.xlabel(\"True Biomass (Mg/ha)\")\n",
    "    plt.ylabel(\"Predicted Biomass (Mg/ha)\")\n",
    "    plt.xlim(xmin, xmax)\n",
    "    plt.ylim(xmin, xmax)\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceec60dd-c68a-448e-b40d-1408fff426e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scores = []\n",
    "# whether to randomly split the train/test data or to split train/test based on year\n",
    "# doesn't seem to make too big of a difference on validation performance\n",
    "random_split = True\n",
    "# whether to reload the training data from individual years, or use the compiled data directly\n",
    "# only needs to be True when the training data is re-generated\n",
    "reload = False\n",
    "# whether to overwrite the models already trained\n",
    "overwrite = False\n",
    "\n",
    "for model_class in [m.random_forest_model, m.xgb_model]:\n",
    "    for realm in realms:\n",
    "        print(f\"Building model for {realm} realm\")\n",
    "\n",
    "        # load data, add year information\n",
    "        df = load.training(\n",
    "            realm=realm,\n",
    "            reload=reload,\n",
    "            access_key_id=access_key_id,\n",
    "            secret_access_key=secret_access_key,\n",
    "        )\n",
    "        print(f\"    size of entire df is {round(df.size / 1e9, 2)}Gb\")\n",
    "\n",
    "        for strategy in [\"none\"]:  # [\"first\", \"last\", \"none\"]:\n",
    "            # strategy = \"first\" means that the first year is used for validation, and \"last\" means the last year is used for validation\n",
    "            # strategy = none means that no data is reserved for validation => used for training the final production model,\n",
    "            # whereas first/last allow us to assess model performance during the model design and tuning phases\n",
    "            df_train, df_test, df_val = m.train_test_split_based_on_year(\n",
    "                df, val_strategy=strategy, random_train_test=random_split\n",
    "            )\n",
    "            print(f\"    training sample size = {len(df_train)}\")\n",
    "            print(f\"    testing sample size = {len(df_test)}\")\n",
    "            print(f\"    eval sample size = {len(df_val)}\")\n",
    "\n",
    "            # this for loop is for running different parameter sets in HPO\n",
    "            for params in [{}]:\n",
    "\n",
    "                # instantiating the model also does .fit\n",
    "                # this will load the model if it already exist and overwrite=False, and fit the model if overwrite=True or the model does not exist\n",
    "                model = model_class(\n",
    "                    realm=realm,\n",
    "                    df_train=df_train,\n",
    "                    df_test=df_test,\n",
    "                    output_folder=\"s3://carbonplan-climatetrace/v2.1/models/\",  # v1 or v2\n",
    "                    overwrite=overwrite,\n",
    "                    validation_year=strategy,\n",
    "                    params=params,\n",
    "                )\n",
    "\n",
    "                # do model evaluation on each split of the data: train, test, and validation\n",
    "                for split, sub in zip((\"train\", \"test\", \"val\"), (df_train, df_test, df_val)):\n",
    "                    # validation data can be empty if val strategy = 'none'\n",
    "                    if len(sub) > 0:\n",
    "                        model_score = model.evaluate(sub)\n",
    "                        model_score[\"model_name\"] = model.name\n",
    "                        model_score[\"split\"] = split\n",
    "                        model_score[\"realm\"] = realm\n",
    "                        model_score[\"validation_year\"] = strategy\n",
    "                        model_score[\"random_split\"] = random_split\n",
    "                        model_score[\"sample_size\"] = len(sub)\n",
    "                        model_score.update(params)\n",
    "                        scores.append(model_score)\n",
    "\n",
    "                df_train[\"biomass_pred\"] = model.predict(df_train)\n",
    "                df_test[\"biomass_pred\"] = model.predict(df_test)\n",
    "\n",
    "            # plot the prediction result\n",
    "            plt.figure(figsize=(10, 4.5))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plot_scatter(df_train, title=f\"{realm} train samples\")\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plot_scatter(df_test, title=f\"{realm} test samples\")\n",
    "            plt.savefig(f\"{realm}_model_scatter.png\")\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "            # plotting feature importance if the model being trained is random forest\n",
    "            if \"rf\" in model.name:\n",
    "                plt.figure(figsize=(10, 4))\n",
    "                plt.title(f\"{realm} feature importance\")\n",
    "                xticks = np.arange(len(m.features)) * 2\n",
    "                plt.bar(xticks, model.model.feature_importances_)\n",
    "                plt.xticks(ticks=xticks, labels=m.features, rotation=\"vertical\")\n",
    "                plt.savefig(f\"{realm}_feature_imp.png\")\n",
    "                plt.show()\n",
    "                plt.close()\n",
    "            # TODO: plot something else if we're training the xgboost model\n",
    "\n",
    "scores = pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8716960a-ba23-42d3-abe9-03fb633dad2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores\n",
    "\n",
    "# only selecting everything that's test or val split\n",
    "# scores.loc[scores.split == 'val]\n",
    "\n",
    "# doing weighted average of the scores\n",
    "# (scores.loc[scores.split == 'test'].r2 * scores.loc[scores.split == 'test'].sample_size).sum() / scores.loc[scores.split == 'test'].sample_size.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9b1afc-86f1-4c5a-a50c-812d7c8bf85a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for random_split in [True, False]:\n",
    "    print(random_split)\n",
    "    sub = scores.loc[(scores.split == \"val\") & (scores.random_split == random_split)]\n",
    "    print(f\"validation score = {(sub.r2 * sub.sample_size).sum() / sub.sample_size.sum()}\")\n",
    "    sub = scores.loc[(scores.split == \"test\") & (scores.random_split == random_split)]\n",
    "    print(f\"testing score    = {(sub.r2 * sub.sample_size).sum() / sub.sample_size.sum()}\")\n",
    "    sub = scores.loc[(scores.split == \"train\") & (scores.random_split == random_split)]\n",
    "    print(f\"training score   = {(sub.r2 * sub.sample_size).sum() / sub.sample_size.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a13440-6361-46b7-8d8b-f21fc0619f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "for validation_year in [\"first\", \"last\"]:\n",
    "    print(validation_year)\n",
    "    sub = scores.loc[(scores.split == \"val\") & (scores.validation_year == validation_year)]\n",
    "    print(f\"validation score = {(sub.r2 * sub.sample_size).sum() / sub.sample_size.sum()}\")\n",
    "    sub = scores.loc[(scores.split == \"test\") & (scores.validation_year == validation_year)]\n",
    "    print(f\"testing score    = {(sub.r2 * sub.sample_size).sum() / sub.sample_size.sum()}\")\n",
    "    sub = scores.loc[(scores.split == \"train\") & (scores.validation_year == validation_year)]\n",
    "    print(f\"training score   = {(sub.r2 * sub.sample_size).sum() / sub.sample_size.sum()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
