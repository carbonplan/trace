{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff16ae6-8be5-4eb9-b413-e84b8c9c8306",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0f286f-e4ec-4c6d-bb41-8b1ae3882433",
   "metadata": {},
   "outputs": [],
   "source": [
    "from carbonplan_trace.v1.glas_allometric_eq import REALM_GROUPINGS\n",
    "from carbonplan_trace.v1 import load\n",
    "import carbonplan_trace.v1.model as m\n",
    "import pandas as pd\n",
    "from carbonplan_trace.v1.landsat_preprocess import access_credentials\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bb0de6-2917-43f9-8cef-206270f1b980",
   "metadata": {},
   "outputs": [],
   "source": [
    "access_key_id, secret_access_key = access_credentials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67552a86-0c5a-4e88-8b8e-d018194e6ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "realms = list(REALM_GROUPINGS.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5ec827-baea-4dd3-926a-dcf77818a7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HPO\n",
    "import itertools\n",
    "\n",
    "\n",
    "def product_dict(**kwargs):\n",
    "    keys = kwargs.keys()\n",
    "    vals = kwargs.values()\n",
    "    for instance in itertools.product(*vals):\n",
    "        yield dict(zip(keys, instance))\n",
    "\n",
    "\n",
    "param_set = {\n",
    "    \"learning_rate\": [0.07, 0.05, 0.03],\n",
    "    \"max_depth\": [10, 12, 14],\n",
    "    \"colsample_bytree\": [0.5, 0.7, 0.9],\n",
    "    \"subsample\": [0.5, 0.7, 0.9],\n",
    "    \"min_child_weight\": [2, 4, 6],\n",
    "    \"lambda\": [1, 1.5, 2],\n",
    "    \"alpha\": [0, 0.5, 1],\n",
    "    \"gamma\": [0, 0.5, 1],\n",
    "}\n",
    "\n",
    "groupings = [\n",
    "    [\"learning_rate\"],\n",
    "    [\"max_depth\"],\n",
    "    [\"colsample_bytree\", \"subsample\", \"min_child_weight\"],\n",
    "    [\"lambda\", \"alpha\", \"gamma\"],\n",
    "]\n",
    "\n",
    "dims = [list(range(len(param_set[g[0]]))) for g in groupings]\n",
    "param_set_list = []\n",
    "for orders in list(itertools.product(*dims)):\n",
    "    d = {}\n",
    "    for o, g in zip(orders, groupings):\n",
    "        for k in g:\n",
    "            d[k] = param_set[k][o]\n",
    "    param_set_list.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b090b3-a9eb-4841-ab84-d779ae51a8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_prediction_result(model, df_train, df_test, df_val):\n",
    "\n",
    "    df_train[\"biomass_pred\"] = model._predict(df_train)\n",
    "    df_test[\"biomass_pred\"] = model._predict(df_test)\n",
    "    df_val[\"biomass_pred\"] = model._predict(df_val)\n",
    "\n",
    "    return df_train, df_test, df_val\n",
    "\n",
    "\n",
    "def calculate_temporal_variability(df, y1=2007, y2=2008, precision=3):\n",
    "    year1 = df.loc[df.year == y1, [\"lat\", \"lon\", \"biomass\"]]\n",
    "    year2 = df.loc[df.year == y2, [\"lat\", \"lon\", \"biomass\"]]\n",
    "\n",
    "    year1[\"lat_round\"] = year1.lat.round(precision)\n",
    "    year1[\"lon_round\"] = year1.lon.round(precision)\n",
    "    year2[\"lat_round\"] = year2.lat.round(precision)\n",
    "    year2[\"lon_round\"] = year2.lon.round(precision)\n",
    "\n",
    "    merged = year1.merge(year2, on=[\"lat_round\", \"lon_round\"], suffixes=[\"_year1\", \"_year2\"])\n",
    "\n",
    "    mae = (merged.biomass_year2 - merged.biomass_year1).abs().mean()\n",
    "    me = (merged.biomass_year2 - merged.biomass_year1).mean()\n",
    "\n",
    "    return {\"mae\": mae, \"me\": me}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceec60dd-c68a-448e-b40d-1408fff426e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scores = []\n",
    "random_split = False\n",
    "reload = False\n",
    "overwrite = False\n",
    "\n",
    "for model_class in [m.random_forest_model]:  # m.xgb_model\n",
    "    for realm in realms:\n",
    "        print(f\"Building model for {realm} realm\")\n",
    "\n",
    "        # load data, add year information\n",
    "        df = load.training(\n",
    "            realm=realm,\n",
    "            reload=reload,\n",
    "            access_key_id=access_key_id,\n",
    "            secret_access_key=secret_access_key,\n",
    "        )\n",
    "        print(f\"    size of entire df is {round(df.size / 1e9, 2)}Gb\")\n",
    "\n",
    "        for strategy in [\"last\"]:  # [\"first\", \"last\", \"no\"]:\n",
    "            # split into train/test based on year\n",
    "            df_train, df_test, df_val = m.train_test_split_based_on_year(\n",
    "                df, val_strategy=strategy, random_train_test=random_split\n",
    "            )\n",
    "            print(f\"    training sample size = {len(df_train)}\")\n",
    "            print(f\"    testing sample size = {len(df_test)}\")\n",
    "            print(f\"    eval sample size = {len(df_val)}\")\n",
    "\n",
    "            # build 2 models: 1) baseline/mean, 2) xgboost\n",
    "            # TODO: build linear model as another baseline model\n",
    "            # m.baseline_model, m.gradient_boost_model, m.random_forest_model\n",
    "\n",
    "            for params in [{}]:\n",
    "\n",
    "                model = model_class(\n",
    "                    realm=realm,\n",
    "                    df_train=df_train,\n",
    "                    df_test=df_test,\n",
    "                    output_folder=\"s3://carbonplan-climatetrace/v1/models/\",\n",
    "                    overwrite=overwrite,\n",
    "                    validation_year=\"none\",\n",
    "                    params=params,\n",
    "                )\n",
    "\n",
    "                for split, sub in zip((\"train\", \"test\", \"val\"), (df_train, df_test, df_val)):\n",
    "                    if len(sub) > 0:\n",
    "                        model_score = model.evaluate(sub)\n",
    "                        model_score[\"model_name\"] = model.name\n",
    "                        model_score[\"split\"] = split\n",
    "                        model_score[\"realm\"] = realm\n",
    "                        model_score[\"validation_year\"] = strategy\n",
    "                        model_score[\"random_split\"] = random_split\n",
    "                        model_score[\"sample_size\"] = len(sub)\n",
    "                        model_score.update(params)\n",
    "                        scores.append(model_score)\n",
    "\n",
    "                df_train[\"biomass_pred\"] = model.predict(df_train)\n",
    "                df_test[\"biomass_pred\"] = model.predict(df_test)\n",
    "\n",
    "            plt.figure(figsize=(10, 4.5))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plot_scatter(df_train, title=f\"{realm} train samples\")\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plot_scatter(df_test, title=f\"{realm} test samples\")\n",
    "            plt.savefig(f\"{realm}_model_scatter.png\")\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "            plt.figure(figsize=(10, 4))\n",
    "            plt.title(f\"{realm} feature importance\")\n",
    "            xticks = np.arange(len(m.features)) * 2\n",
    "            plt.bar(xticks, model.model.feature_importances_)\n",
    "            plt.xticks(ticks=xticks, labels=m.features, rotation=\"vertical\")\n",
    "            plt.savefig(f\"{realm}_feature_imp.png\")\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "scores = pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6519687-11f4-41e0-b8fe-0191fecc98ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter(sub, title, n=500000):\n",
    "    xmin = -10\n",
    "    size = min(len(sub), n)\n",
    "    toplot = sub.sample(n=size)\n",
    "    xmax = toplot.biomass.quantile(0.95)\n",
    "    plt.scatter(toplot.biomass, toplot.biomass_pred, s=1, alpha=0.03)\n",
    "    plt.plot([xmin, xmax], [xmin, xmax], \"k\")\n",
    "    plt.xlabel(\"True Biomass (Mg/ha)\")\n",
    "    plt.ylabel(\"Predicted Biomass (Mg/ha)\")\n",
    "    plt.xlim(xmin, xmax)\n",
    "    plt.ylim(xmin, xmax)\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527a24c3-477f-4023-8816-3f2cb8d91ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.year.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8716960a-ba23-42d3-abe9-03fb633dad2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9b1afc-86f1-4c5a-a50c-812d7c8bf85a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for random_split in [True, False]:\n",
    "    print(random_split)\n",
    "    sub = scores.loc[(scores.split == \"val\") & (scores.random_split == random_split)]\n",
    "    print(f\"validation score = {(sub.r2 * sub.sample_size).sum() / sub.sample_size.sum()}\")\n",
    "    sub = scores.loc[(scores.split == \"test\") & (scores.random_split == random_split)]\n",
    "    print(f\"testing score    = {(sub.r2 * sub.sample_size).sum() / sub.sample_size.sum()}\")\n",
    "    sub = scores.loc[(scores.split == \"train\") & (scores.random_split == random_split)]\n",
    "    print(f\"training score   = {(sub.r2 * sub.sample_size).sum() / sub.sample_size.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a13440-6361-46b7-8d8b-f21fc0619f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "for validation_year in [\"first\", \"last\"]:\n",
    "    print(validation_year)\n",
    "    sub = scores.loc[(scores.split == \"val\") & (scores.validation_year == validation_year)]\n",
    "    print(f\"validation score = {(sub.r2 * sub.sample_size).sum() / sub.sample_size.sum()}\")\n",
    "    sub = scores.loc[(scores.split == \"test\") & (scores.validation_year == validation_year)]\n",
    "    print(f\"testing score    = {(sub.r2 * sub.sample_size).sum() / sub.sample_size.sum()}\")\n",
    "    sub = scores.loc[(scores.split == \"train\") & (scores.validation_year == validation_year)]\n",
    "    print(f\"training score   = {(sub.r2 * sub.sample_size).sum() / sub.sample_size.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea208ad-daa1-4811-bbe0-cbc6e1ff75dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_variability = pd.read_csv(\"temporal_variability.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8e436c-3a67-4541-9772-3ca633323102",
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_variability[\"realm\"] = temporal_variability.model_name.apply(lambda x: x.split(\"_\")[1])\n",
    "temporal_variability[\"model_type\"] = temporal_variability.model_name.apply(\n",
    "    lambda x: x.split(\"_\")[0]\n",
    ")\n",
    "\n",
    "sample_size = (\n",
    "    scores.loc[(scores.random_split == True) & (scores.model_name.str.startswith(\"xgb\"))]\n",
    "    .groupby(\"realm\")\n",
    "    .sample_size.sum()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716e1e97-4f6c-4609-8f2f-bfd7bcd6bdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighted average\n",
    "merged = temporal_variability.loc[temporal_variability.random_split != True].merge(\n",
    "    sample_size, how=\"left\", on=\"realm\"\n",
    ")\n",
    "name_dict = {\n",
    "    \"gb\": \"gradient boosting\",\n",
    "    \"ground\": \"lidar derived\",\n",
    "    \"rf\": \"random forest\",\n",
    "    \"xgb\": \"xgboost\",\n",
    "}\n",
    "merged[\"model_type\"] = merged.model_type.apply(lambda x: name_dict[x])\n",
    "\n",
    "print(\n",
    "    \"Biomass MAE between years 2007 and 2008 of the same location using different model architecture\"\n",
    ")\n",
    "print(\"\")\n",
    "for model, g in merged.groupby(\"model_type\"):\n",
    "    print(\n",
    "        model.ljust(20),\n",
    "        np.round((g.mae * g.sample_size).sum() / g.sample_size.sum(), 4),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90400beb-c2bc-443f-9aa8-a2e76a57a9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple average\n",
    "temporal_variability.loc[temporal_variability.random_split != True].merge(\n",
    "    sample_size, how=\"left\", on=\"realm\"\n",
    ").groupby(\"model_type\").mae.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3498408-2ad8-4aed-a8bb-f6d722ba025a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.read_csv(\"HPO_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5d4987-11af-4f8b-bc48-965bb7838310",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.split == \"test\"].groupby(\n",
    "    [\"learning_rate\", \"max_depth\", \"colsample_bytree\", \"lambda\"]\n",
    ").mean().sort_values(by=\"r2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea82ac5c-5cbb-4b87-b739-f98249a73b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# igbp_encoder = OneHotEncoder(sparse=False, categories='auto', handle_unknown='ignore').fit(df_train[['igbp']])\n",
    "#     # one hot encoding for igbp\n",
    "#     encoded_igbp = igbp_encoder.transform(X[['igbp']])\n",
    "#     X = X.drop(['igbp'], axis=1)\n",
    "#     for i in range(encoded_igbp.shape[1]):\n",
    "#         X[f'igbp_cat_{str(i+1)}'] = encoded_igbp[:, i]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
