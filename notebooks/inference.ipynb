{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference pipeline\n",
    "\n",
    "Created by: Oriana Chegwidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import boto3\n",
    "from rasterio.session import AWSSession\n",
    "from s3fs import S3FileSystem\n",
    "aws_session = AWSSession(boto3.Session(profile_name='default'), \n",
    "                         requester_pays=True)\n",
    "fs = S3FileSystem(profile='default', requester_pays=True)\n",
    "\n",
    "from osgeo.gdal import VSICurlClearCache\n",
    "VSICurlClearCache() \n",
    "import rasterio as rio\n",
    "import xarray as xr\n",
    "import dask\n",
    "import os\n",
    "import fsspec\n",
    "import geopandas as gpd\n",
    "import regionmask as rm\n",
    "from matplotlib.pyplot import imshow\n",
    "from intake import open_stac_item_collection\n",
    "import numcodecs\n",
    "import numpy as np\n",
    "import rioxarray # for the extension to load\n",
    "import matplotlib.pyplot as plt\n",
    "import utm\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import json\n",
    "import zarr\n",
    "import awswrangler as wr\n",
    "from dask.distributed import PipInstall\n",
    "from dask_gateway import Gateway\n",
    "import fsspec\n",
    "import xgboost as xgb \n",
    "from carbonplan_trace.v1.landsat_preprocess import access_credentials\n",
    "from carbonplan_trace.v1 import utils\n",
    "from carbonplan_trace.v1.inference import predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster = \"local\"\n",
    "if cluster == \"local\":\n",
    "    # spin up local cluster. must be on big enough machine\n",
    "    from dask.distributed import Client\n",
    "\n",
    "    client = Client(n_workers=2, threads_per_worker=2)  # _per_worker=4\n",
    "    client\n",
    "else:\n",
    "    gateway = Gateway()\n",
    "    options = gateway.cluster_options()\n",
    "    options.environment = {\"AWS_REQUEST_PAYER\": \"requester\"}\n",
    "    options.worker_cores = 2\n",
    "    options.worker_memory = 48\n",
    "    cluster = gateway.new_cluster(cluster_options=options)\n",
    "    cluster.adapt(minimum=1, maximum=10)\n",
    "    cluster\n",
    "\n",
    "    client = cluster.get_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_key_id, secret_access_key = access_credentials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_model_path = (\n",
    "    \"s3://carbonplan-climatetrace/v1/models/xgb_biomass_50N_120W.bin\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we take the list of files for a given year to average across growing season\n",
    "for each of the tiles and write it out to a mapper with those specifications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.config.set({\"array.slicing.split_large_chunks\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file(\n",
    "    \"https://prd-wret.s3-us-west-2.amazonaws.com/assets/\"\n",
    "    \"palladium/production/s3fs-public/atoms/files/\"\n",
    "    \"WRS2_descending_0.zip\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tile = gdf.cx[-120:-110, 40:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll loop through every scene and every year and calculate biomass for that\n",
    "scene. Will produce table of values [x, y, (both specific to utm projection),\n",
    "lat, lon, biomass].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bucket = \"s3://carbonplan-climatetrace/v1\"\n",
    "tasks = []\n",
    "rerun = True\n",
    "if rerun:\n",
    "    with rio.Env(aws_session):\n",
    "        for year in np.arange(2003, 2004):\n",
    "            for [path, row] in sample_tile[[\"PATH\", \"ROW\"]].values[0:1]:\n",
    "                #                 tasks.append(\n",
    "                predict(\n",
    "                    sample_model_path,\n",
    "                    path,\n",
    "                    row,\n",
    "                    year,\n",
    "                    access_key_id,\n",
    "                    secret_access_key,\n",
    "                    output_write_bucket=\"/\".join(\n",
    "                        [\n",
    "                            bucket,\n",
    "                            \"inference\",\n",
    "                            str(path),\n",
    "                            str(row),\n",
    "                            str(year),\n",
    "                            \"data.parquet\",\n",
    "                        ]\n",
    "                    ),\n",
    "                    input_write_bucket=None,  #'/'.join([bucket, 'inference', 'test_inputs.parquet'])\n",
    "                )\n",
    "#                 )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
