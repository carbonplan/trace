{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference pipeline\n",
    "\n",
    "Created by: Oriana Chegwidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pyproj import CRS\n",
    "import boto3\n",
    "from rasterio.session import AWSSession\n",
    "from s3fs import S3FileSystem\n",
    "aws_session = AWSSession(boto3.Session(),#profile_name='default'), \n",
    "                         requester_pays=True)\n",
    "fs = S3FileSystem(requester_pays=True) #profile='default', \n",
    "import xgboost as xgb\n",
    "\n",
    "from osgeo.gdal import VSICurlClearCache\n",
    "import rasterio as rio\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import dask\n",
    "import os\n",
    "import fsspec\n",
    "import geopandas as gpd\n",
    "import rioxarray # for the extension to load\n",
    "import matplotlib.pyplot as plt\n",
    "import utm\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import json\n",
    "import zarr\n",
    "import awswrangler as wr\n",
    "from dask_gateway import Gateway\n",
    "from carbonplan_trace.v1.landsat_preprocess import access_credentials, test_credentials\n",
    "from carbonplan_trace.v1.inference import predict, predict_delayed\n",
    "from carbonplan_trace.v1 import utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from carbonplan_trace import version\n",
    "\n",
    "print(version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# kind_of_cluster = \"local\"\n",
    "kind_of_cluster = \"remote\"\n",
    "if kind_of_cluster == \"local\":\n",
    "    # spin up local cluster. must be on big enough machine\n",
    "    from dask.distributed import Client\n",
    "\n",
    "    client = Client(n_workers=4, threads_per_worker=1)  # _per_worker=4\n",
    "    client\n",
    "else:\n",
    "    gateway = Gateway()\n",
    "    options = gateway.cluster_options()\n",
    "    options.environment = {\n",
    "        \"AWS_REQUEST_PAYER\": \"requester\",\n",
    "        \"AWS_REGION_NAME\": \"us-west-2\",\n",
    "        \n",
    "    }\n",
    "    options.worker_cores = 1\n",
    "    options.worker_memory = 80\n",
    "    options.image = \"carbonplan/trace-python-notebook:latest\"\n",
    "    cluster = gateway.new_cluster(cluster_options=options)\n",
    "    #     cluster.adapt(minimum=2, maximum=100)\n",
    "    cluster.scale(100)\n",
    "\n",
    "# lower latitudes = lower memory\n",
    "# I've tried 200gb mem for worker with 100 workers => 150 nodes, where our limit = 500 nodes\n",
    "# 32gb => i've tried 400 workers, can probably go higher\n",
    "# increase memory when you start seeing a lot of Killed Worker Errors\n",
    "\n",
    "# recipe\n",
    "# try running 60gb with 300 workers for bounding_boxes 0-100\n",
    "# 120gb with 200 workers for bounding boxes 100-200\n",
    "# 200gb with 100 workers for bounding boxes 200-end (there are 280 boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client\n",
    "\n",
    "# check this link first\n",
    "# possible scenario:\n",
    "# 1) everything is succeeding and cluster still running, no need to do anything\n",
    "# 2) most things are failing but cluster still running, restart, increase mem and decrease num worker, re start and run all\n",
    "# 3) 404 error -> cluster died -> restart and run all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shutdown_cluster(kind_of_cluster):\n",
    "    if kind_of_cluster == \"local\":\n",
    "        client.shutdown()\n",
    "    elif kind_of_cluster == \"remote\":\n",
    "        cluster.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shutdown_cluster(kind_of_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_key_id, secret_access_key = access_credentials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_credentials(aws_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we take the list of files for a given year to average across growing season\n",
    "for each of the tiles and write it out to a mapper with those specifications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.config.set({\"array.slicing.split_large_chunks\": True})\n",
    "# tcp is a transmission control protocol\n",
    "dask.config.set({\"distributed.comm.timeouts.tcp\": \"50s\"})\n",
    "dask.config.set({\"distributed.comm.timeouts.connect\": \"50s\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file(\n",
    "    \"https://prd-wret.s3-us-west-2.amazonaws.com/assets/\"\n",
    "    \"palladium/production/s3fs-public/atoms/files/\"\n",
    "    \"WRS2_descending_0.zip\"\n",
    ")\n",
    "bucket = \"s3://carbonplan-climatetrace/v1\"\n",
    "\n",
    "biomass_folder = bucket + \"/biomass/\"\n",
    "biomass_files = fs.ls(biomass_folder)\n",
    "lat_lon_tags = [\n",
    "    utils.get_lat_lon_tags_from_tile_path(fp) for fp in biomass_files\n",
    "]\n",
    "bounding_boxes = [\n",
    "    utils.parse_bounding_box_from_lat_lon_tags(lat, lon)\n",
    "    for lat, lon in lat_lon_tags\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from carbonplan_trace.v1.glas_allometric_eq import REALM_GROUPINGS\n",
    "\n",
    "processed_scenes = []\n",
    "for year in np.arange(2015, 2021):\n",
    "    processed_scenes.extend(\n",
    "        fs.ls(f\"{bucket}/inference/rf/{year}\", recursive=True)\n",
    "    )\n",
    "\n",
    "processed_scenes = [scene[-19:-8] for scene in processed_scenes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(processed_scenes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll loop through every scene and every year and calculate biomass for that\n",
    "scene. Will produce table of values [x, y, (both specific to utm projection),\n",
    "lat, lon, biomass].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "landsat_bucket = (\n",
    "    \"s3://usgs-landsat/collection02/level-2/standard/etm/{}/{:03d}/{:03d}/\"\n",
    ")\n",
    "\n",
    "with rio.Env(aws_session):\n",
    "    tasks = []\n",
    "    task_id = []\n",
    "    # for all bounding boxes ==> 70k tasks, will take 15 mins to append all tasks\n",
    "    for bounding_box in bounding_boxes[10:20]:\n",
    "        print(bounding_box)\n",
    "        min_lat, max_lat, min_lon, max_lon = bounding_box\n",
    "        scenes_in_tile = gdf.cx[min_lon:max_lon, min_lat:max_lat][\n",
    "            [\"PATH\", \"ROW\"]\n",
    "        ].values\n",
    "        for year in np.arange(2015, 2021):\n",
    "            for [path, row] in scenes_in_tile:\n",
    "                scene_stores = fs.ls(landsat_bucket.format(year, path, row))\n",
    "                output_name = f\"{year}/{path:03d}{row:03d}\"\n",
    "                if len(scene_stores) == 0:\n",
    "                    continue\n",
    "                elif output_name in processed_scenes:\n",
    "                    continue\n",
    "                else:\n",
    "                    tasks.append(\n",
    "                        # predict(\n",
    "                        #                         client.compute(   # what cindy usually runs\n",
    "                        predict_delayed(\n",
    "                            model_folder=f\"{bucket}/models/\",\n",
    "                            path=path,\n",
    "                            row=row,\n",
    "                            year=year,\n",
    "                            access_key_id=access_key_id,\n",
    "                            secret_access_key=secret_access_key,\n",
    "                            output_write_bucket=f\"{bucket}/inference\",\n",
    "                        )\n",
    "                    )\n",
    "                    task_id.append([path, row, year])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# path = 165\n",
    "# row = 65\n",
    "# year = 2016\n",
    "\n",
    "# predict(\n",
    "#     model_folder=f\"{bucket}/models/\",\n",
    "#     path=path,\n",
    "#     row=row,\n",
    "#     year=year,\n",
    "#     access_key_id=access_key_id,\n",
    "#     secret_access_key=secret_access_key,\n",
    "#     output_write_bucket=f\"{bucket}/inference\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "test_run = random.choices(tasks, k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = dask.compute(test_run, retries=1)[0]\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_list = []\n",
    "\n",
    "# three types of error\n",
    "# KilledWorker == memory usage\n",
    "# Assertion Error => need to figure out what we want to do for inference, can throw away for now\n",
    "# .zmetadata\n",
    "# AccessDenied should have been fixed\n",
    "\n",
    "for i, task in enumerate(tasks):\n",
    "    if task.status == \"error\" and i not in []:\n",
    "        print(i)\n",
    "        #         print(task.result())\n",
    "        try:\n",
    "            print(task.result())\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "#             if isinstance(e, PermissionError):  # you can change this to other error\n",
    "#                 exclude_list.append(task_id[i])\n",
    "#                 print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for task in tasks:\n",
    "#     task.cancel()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
