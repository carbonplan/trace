{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72b7732a-58e1-439b-b6aa-046fb18ac615",
   "metadata": {},
   "source": [
    "## Post processing of aboveground biomass dataset\n",
    "\n",
    "### Input\n",
    "\n",
    "Random forest model prediction results from inference.ipynb. These are parquet\n",
    "files (1 for each landsat scene x year) with columns x, y, biomass. x, y are in\n",
    "lat/lon coordinates, and biomass is in unit of Mg biomass / ha and only accounts\n",
    "for aboveground, live, woody biomass.\n",
    "\n",
    "### Processes\n",
    "\n",
    "For each 10x10 degree tile in our template\n",
    "\n",
    "1. merge and mosaic all landsat scenes within a 10x10 degree tile for all years\n",
    "   available and store the data in zarr format\n",
    "2. fill gaps within the biomass dataset by xarray interpolate_na with linear\n",
    "   method (first through dim time, then through dim x, then dim y)\n",
    "3. mask with MODIS MCD12Q1 land cover dataset to only select the forest pixels\n",
    "4. calculate belowground biomass and deadwood and litter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbce1e4-4cb9-452b-b683-d26f3b0f8720",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pyproj import CRS\n",
    "import boto3\n",
    "from rasterio.session import AWSSession\n",
    "from s3fs import S3FileSystem\n",
    "aws_session = AWSSession(boto3.Session(),#profile_name='default'), \n",
    "                         requester_pays=True)\n",
    "fs = S3FileSystem(requester_pays=True)\n",
    "import xgboost as xgb\n",
    "\n",
    "from osgeo.gdal import VSICurlClearCache\n",
    "import rasterio as rio\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import dask\n",
    "import os\n",
    "import fsspec\n",
    "\n",
    "import rioxarray # for the extension to load\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "from dask_gateway import Gateway\n",
    "from carbonplan_trace.v1.landsat_preprocess import access_credentials, test_credentials\n",
    "from carbonplan_trace.v1.inference import predict, predict_delayed \n",
    "from carbonplan_trace.v1 import utils, postprocess, load\n",
    "from carbonplan_trace.tiles import tiles\n",
    "from carbonplan_trace.v1.landsat_preprocess import access_credentials, test_credentials\n",
    "import prefect\n",
    "from prefect import task, Flow, Parameter\n",
    "from prefect.engine.executors import DaskExecutor\n",
    "from prefect.utilities.debug import raise_on_exception\n",
    "from datetime import datetime as time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5e439a-4b13-40b0-b7a6-ed0eaf812057",
   "metadata": {},
   "outputs": [],
   "source": [
    "from carbonplan_trace import version\n",
    "%reload_ext watermark\n",
    "print(version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651f96e4-26bf-428d-83b8-af76ae40fca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "watermark -d -n -t -u -v -p carbonplan_trace -h -m -g -r -b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075df2af-bfda-43fe-859c-d09d570a5488",
   "metadata": {},
   "outputs": [],
   "source": [
    "kind_of_cluster = \"remote\"\n",
    "if kind_of_cluster == \"local\":\n",
    "    # spin up local cluster. must be on big enough machine\n",
    "    from dask.distributed import Client\n",
    "\n",
    "    local_cluster_client = Client(n_workers=1, threads_per_worker=4)\n",
    "    local_cluster_client\n",
    "elif kind_of_cluster == \"remote\":\n",
    "    gateway = Gateway()\n",
    "    options = gateway.cluster_options()\n",
    "    options.environment = {\n",
    "        \"AWS_REQUEST_PAYER\": \"requester\",\n",
    "        \"AWS_REGION_NAME\": \"us-west-2\",\n",
    "        \"DASK_DISTRIBUTED__WORKER__RESOURCES__WORKERTOKEN\": \"1\",\n",
    "    }\n",
    "    options.worker_cores = 1\n",
    "    options.worker_memory = 31\n",
    "\n",
    "    options.image = \"carbonplan/trace-python-notebook:latest\"\n",
    "    cluster = gateway.new_cluster(cluster_options=options)\n",
    "    #     cluster.adapt(minimum=0,maximum=2)\n",
    "    cluster.scale(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377caf31-81a8-4f30-b746-74e56e90e98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster.shutdown()\n",
    "# local_cluster_client.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574c210a-4d04-4c21-a0a8-c14ea9d250ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clusters = gateway.list_clusters()\n",
    "cluster = gateway.connect(clusters[0].name)\n",
    "cluster.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66808189-0cda-4094-8af3-256f49c85e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3531be33-db2d-40a4-9e30-b0153a33e466",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ffccec-fa70-4ca3-89e8-79efb80eb732",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = cluster.get_client()\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de448c7-7840-4d72-b7f8-de452b6f2896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find existing output and skip those, something like this\n",
    "\n",
    "# processed_scenes = []\n",
    "# for year in np.arange(2015, 2021):\n",
    "#     processed_scenes.extend(\n",
    "#         fs.ls(f\"{bucket}/inference/rf/{year}\", recursive=True)\n",
    "#     )\n",
    "\n",
    "# processed_scenes = [scene[-19:-8] for scene in processed_scenes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9442d72c-59c0-4cf7-87d0-6d374b4499c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "access_key_id, secret_access_key = access_credentials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2900c8ab-f05e-47f7-95f9-f6eb015f69cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = []\n",
    "# define starting and ending years (will want to go back to 2014 but that might not be ready right now)\n",
    "year0, year1 = 2014, 2021\n",
    "# define the size of subtile you want to work in (2 degrees recommended)\n",
    "tile_degree_size = 2\n",
    "# if you want to write the metadata for the zarr store\n",
    "write_tile_metadata = True\n",
    "chunks_dict = {\"x\": 1000, \"y\": 1000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f49be9-788c-428c-8f93-afa50827417b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parameters_list = []\n",
    "for tile in tiles[178:179]:  # [tiles[178],tiles[180]]:\n",
    "    #     if tile not in already_processed:\n",
    "    lat_tag, lon_tag = utils.get_lat_lon_tags_from_tile_path(tile)\n",
    "    lat_lon_box = utils.parse_bounding_box_from_lat_lon_tags(lat_tag, lon_tag)\n",
    "    # find the lat_lon_box for that tile\n",
    "    min_lat, max_lat, min_lon, max_lon = lat_lon_box\n",
    "\n",
    "    # initialize empty dataset. only need to do this once, and not if the tile has already been processed\n",
    "    data_path = postprocess.initialize_empty_dataset(\n",
    "        lat_tag, lon_tag, year0, year1, write_tile_metadata=write_tile_metadata\n",
    "    )\n",
    "    # now we'll split up each of those tiles into smaller subtiles of length `tile_degree_size`\n",
    "    # and run through those. In this case since we've specified 2, we'll have 25 in each box\n",
    "\n",
    "    prefect_parameters = {\n",
    "        \"MIN_LAT\": min_lat,\n",
    "        \"MIN_LON\": min_lon,\n",
    "        \"YEAR_0\": year0,\n",
    "        \"YEAR_1\": year1,\n",
    "        \"TILE_DEGREE_SIZE\": tile_degree_size,\n",
    "        \"DATA_PATH\": data_path,\n",
    "        \"ACCESS_KEY_ID\": access_key_id,\n",
    "        \"SECRET_ACCESS_KEY\": secret_access_key,\n",
    "        \"CHUNKS_DICT\": chunks_dict,\n",
    "    }\n",
    "\n",
    "    for lat_increment in np.arange(0, 10, tile_degree_size)[0:1]:\n",
    "        for lon_increment in np.arange(0, 10, tile_degree_size)[0:1]:\n",
    "            increment_parameters = prefect_parameters.copy()\n",
    "            increment_parameters[\"LAT_INCREMENT\"] = lat_increment\n",
    "            increment_parameters[\"LON_INCREMENT\"] = lon_increment\n",
    "            parameters_list.append(increment_parameters)\n",
    "\n",
    "#         tasks.append(client.compute(postprocess_delayed(subtile_ul_lat, subtile_ul_lon, year0, year1, tile_degree_size, mapper)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6245ea6-00f4-4758-9423-45da687d38c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if kind_of_cluster == \"local\":\n",
    "    executor = DaskExecutor(address=local_cluster_client.scheduler.address)\n",
    "elif kind_of_cluster == \"remote\":\n",
    "    executor = DaskExecutor(\n",
    "        address=client.scheduler.address,\n",
    "        client_kwargs={\"security\": cluster.security},\n",
    "        debug=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a01d565-d595-4391-9178-577727043f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fail_nicely(task, old_state, new_state):\n",
    "    print(old_state, new_state)\n",
    "    if new_state.is_running():\n",
    "        print(\"running!\")\n",
    "    if new_state.is_failed():\n",
    "        print(\"this task {} failed\".format(task))\n",
    "        raise ValueError(\"OH NO\")  # function that sends a notification\n",
    "    return new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398f7469-5242-40ae-a83e-4f48c414f0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prefect.engine.signals.state.Skipped()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e2ff9a-4ad3-43e7-bb89-7504a213bcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "postprocess_task = task(\n",
    "    postprocess.postprocess_subtile,\n",
    "    tags=[\"dask-resource:workertoken=1\"],\n",
    "    state_handlers=[fail_nicely],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a47e4f-e034-4a32-b040-8870fc048967",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Flow(\"Postprocessing\") as flow:\n",
    "    # Run postprocess\n",
    "    postprocess_task.map(parameters_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62859d4a-cbda-4d8f-8321-8d0bf0362774",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with raise_on_exception():\n",
    "    # if running locally (no cluster)\n",
    "    flow.run()\n",
    "    # if running on cluster\n",
    "#     flow.run(executor=executor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:notebook] *",
   "language": "python",
   "name": "conda-env-notebook-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
